document_id,category,original_text,stemmed_text,word_count,keyword_percentage
8319,Computers and IT,"This paper establishes an innovative and efficient multiresolution adaptive approach combined with high-resolution methods, for the numerical solution of a single or a system of partial differential equations. The proposed methodology is unconditionally bounded (even for hyperbolic equations) and dynamically adapts the grid so that higher spatial resolution is automatically allocated to domain regions where strong gradients are observed, thus possessing the two desired properties of a numerical approach: stability and accuracy. Numerical results for five test problems are presented which clearly show the robustness and cost effectiveness of the proposed method","this paper establishes an innovative and efficient multiresolution adaptiveapproach combined with high-resolution methods, for the numerical solution of a single or a system of partial differential equations. the proposed methodology is unconditionally bounded (even for hyperbolic equations) and dynamically adapts the grid so that higher spatial resolution is automatically allocated to domain regions where strong gradients are observed, thus possessing the two desired properties of a numerical approach: stability and accuracy. numerical results for five test problems are presented which clearly show the robustness and cost effectiveness of the proposed method",91,0.666666667
8320,Computers and IT,"A formulation for a porous medium saturated with a compressible fluid undergoing large elastic and plastic deformations is presented. A consistent thermodynamic formulation is proposed for the two-phase mixture problem; thus preserving a straightforward and robust numerical scheme. A novel feature is the specification of the fluid compressibility in terms of a volumetric logarithmic strain, which is energy conjugated to the fluid pressure in the entropy inequality. As a result, the entropy inequality is used to separate three different mechanisms representing the response: effective stress response according to Terzaghi in the solid skeleton, fluid pressure response to compressibility of the fluid, and dissipative Darcy flow representing the interaction between the two phases. The paper is concluded with a couple of numerical examples that display the predictive capabilities of the proposed formulation. In particular, we consider results for the kinematically linear theory as compared to the kinematically non-linear theory","a formulation for a porous medium saturate with a compressible fluid undergo large elastic and plastic deformation be present . a consistent thermodynamic formulation be propose for the two-phase mixture problem ; thus preserve a straightforward and robust numerical scheme . a novel feature be the specification of the fluid compressibility in term of a volumetric logarithmic strain , which be energy conjugate to the fluid pressure in the entropy inequality . as a result , the entropy inequality be use to separate three different mechanism represent the response : effective stress response accord to terzaghi in the solid skeleton , fluid pressure response to compressibility of the fluid , and dissipative darcy flow represent the interaction between the two phase . the paper be conclude with a couple of numerical example that display the predictive capability of the propose formulation . in particular , we consider result for the kinematically linear theory as compare to the kinematically non-linear theory",148,0.736842105
8321,Computers and IT,"Recent models of natural language processing employ statistical reasoning for dealing with the ambiguity of formal grammars. In this approach, statistics, concerning the various linguistic phenomena of interest, are gathered from actual linguistic data and used to estimate the probabilities of the various entities that are generated by a given grammar, e. g. , derivations, parse-trees and sentences. The extension of grammars with probabilities makes it possible to state ambiguity resolution as a constrained optimization formula, which aims at maximizing the probability of some entity that the grammar generates given the input (e. g. , maximum probability parse-tree given some input sentence). The implementation of these optimization formulae in efficient algorithms, however, does not always proceed smoothly. In this paper, we address the computational complexity of ambiguity resolution under various kinds of probabilistic models. We provide proofs that some, frequently occurring problems of ambiguity resolution are NP-complete. These problems are encountered in various applications, e. g. , language understanding for textand speech-based applications. Assuming the common model of computation, this result implies that, for many existing probabilistic models it is not possible to devise tractable algorithms for solving these optimization problems","recent models of natural language processing employ statistical reasoning fordealing with the ambiguity of formal grammars. in this approach, statistics, concerning the various linguistic phenomena of interest, are gathered from actual linguistic data and used to estimate the probabilities of the various entities that are generated by a given grammar, e.g., derivations, parse-trees and sentences. the extension of grammars with probabilities makes it possible to state ambiguity resolution as a constrained optimization formula, which aims at maximizing the probability of some entity that the grammar generates given the input (e.g., maximum probability parse-tree given some input sentence). the implementation of these optimization formulae in efficient algorithms, however, does not always proceed smoothly. in this paper, we address the computational complexity of ambiguity resolution under various kinds of probabilistic models. we provide proofs that some, frequently occurring problems of ambiguity resolution are np-complete. these problems are encountered in various applications, e.g., language understanding for textand speech-based applications. assuming the common model of computation, this result implies that, for many existing probabilistic models it is not possible to devise tractable algorithms for solving these optimization problems",184,0.615384615
8322,Computers and IT,"The generalized LR parsing algorithm for context-free grammars, introduced by Tomita in 1986, is a polynomial-time implementation of nondeterministic LR parsing that uses graph-structured stack to represent the contents of the nondeterministic parser's pushdown for all possible branches of computation at a single computation step. It has been specifically developed as a solution for practical parsing tasks arising in computational linguistics, and indeed has proved itself to be very suitable for natural language processing. Conjunctive grammars extend context-free grammars by allowing the use of an explicit intersection operation within grammar rules. This paper develops a new LR-style parsing algorithm for these grammars, which is based on the very same idea of a graph-structured pushdown, where the simultaneous existence of several paths in the graph is used to perform the mentioned intersection operation. The underlying finite automata are treated in the most general way: instead of showing the algorithm's correctness for some particular way of constructing automata, the paper defines a wide class of automata usable with a given grammar, which includes not only the traditional LR(k) automata, but also, for instance, a trivial automaton with a single reachable state. A modification of the SLR(k) table construction method that makes use of specific properties of conjunctive grammars is provided as one possible way of making finite automata to use with the algorithm","the generalized lr parsing algorithm for context-free grammars, introduced bytomita in 1986, is a polynomial-time implementation of nondeterministic lr parsing that uses graph-structured stack to represent the contents of the nondeterministic parser's pushdown for all possible branches of computation at a single computation step. it has been specifically developed as a solution for practical parsing tasks arising in computational linguistics, and indeed has proved itself to be very suitable for natural language processing. conjunctive grammars extend context-free grammars by allowing the use of an explicit intersection operation within grammar rules. this paper develops a new lr-style parsing algorithm for these grammars, which is based on the very same idea of a graph-structured pushdown, where the simultaneous existence of several paths in the graph is used to perform the mentioned intersection operation. the underlying finite automata are treated in the most general way: instead of showing the algorithm's correctness for some particular way of constructing automata, the paper defines a wide class of automata usable with a given grammar, which includes not only the traditional lr(k) automata, but also, for instance, a trivial automaton with a single reachable state. a modification of the slr(k) table construction method that makes use of specific properties of conjunctive grammars is provided as one possible way of making finite automata to use with the algorithm",220,0.666666667
8323,Computers and IT,"We continue the study of those P systems where the computation is performed by the communication of objects, that is, systems with symport and antiport rules. Instead of the (number of) objects collected in a specified membrane, as the result of a computation we consider the itineraries of a certain object through membranes, during a halting computation, written as a coding of the string of labels of the visited membranes. The family of languages generated in this way is investigated with respect to its place in the Chomsky hierarchy. When the (symport and antiport) rules are applied in a conditional manner, promoted or inhibited by certain objects which should be present in the membrane where a rule is applied, then a characterization of recursively enumerable languages is obtained; the power of systems with the rules applied freely is only partially described","we continue the study of those p systems where the computation is performed bythe communication of objects, that is, systems with symport and antiport rules. instead of the (number of) objects collected in a specified membrane, as the result of a computation we consider the itineraries of a certain object through membranes, during a halting computation, written as a coding of the string of labels of the visited membranes. the family of languages generated in this way is investigated with respect to its place in the chomsky hierarchy. when the (symport and antiport) rules are applied in a conditional manner, promoted or inhibited by certain objects which should be present in the membrane where a rule is applied, then a characterization of recursively enumerable languages is obtained; the power of systems with the rules applied freely is only partially described",140,0.454545455
8324,Computers and IT,"The cone-beam approach for image reconstruction attracts increasing attention in various applications, especially medical imaging. Previously, the traditional practical cone-beam reconstruction method, the Feldkamp algorithm, was generalized into the case of spiral/helical scanning loci with equispatial cone-beam projection data. In this paper, we formulated the generalized Feldkamp algorithm in the case of equiangular cone-beam projection data, and performed numerical simulation to evaluate the image quality. Because medical multi-slice/cone-beam CT scanners typically use equiangular projection data, our new formula may be useful in this area as a framework for further refinement and a benchmark for comparison","the cone-beam approach for image reconstruction attract increase attentionin various application , especially medical imaging . previously , the traditional practical cone-beam reconstruction method , the feldkamp algorithm , be generalize into the case of spiral/helical scan locus with equispatial cone-beam projection data . in this paper , we formulate the generalize feldkamp algorithm in the case of equiangular cone-beam projection data , and perform numerical simulation to evaluate the image quality . because medical multi-slice/cone-beam ct scanner typically use equiangular projection data , our new formula may be useful in this area as a framework for further refinement and a benchmark for comparison",94,0.833333333
8325,Computers and IT,"This article reports on a pilot study conducted by the Academic Libraries of the 21st Century project team to determine whether the benefits of the case study method as a training framework for change initiatives could successfully transfer from the traditional face-to-face format to a virtual format. Methods of developing the training framework, as well as the benefits, challenges, and recommendations for future strategies gained from participant feedback are outlined. The results of a survey administered to chat session registrants are presented in three sections: (1) evaluation of the training framework; (2) evaluation of participants' experiences in the virtual environment; and (3) a comparison of participants' preference of format. The overall participant feedback regarding the utilization of the case study method in a virtual environment for professional development and collaborative problem solving is very positive","this article reports on a pilot study conducted by the academic libraries ofthe 21st century project team to determine whether the benefits of the case study method as a training framework for change initiatives could successfully transfer from the traditional face-to-face format to a virtual format. methods of developing the training framework, as well as the benefits, challenges, and recommendations for future strategies gained from participant feedback are outlined. the results of a survey administered to chat session registrants are presented in three sections: (1) evaluation of the training framework; (2) evaluation of participants' experiences in the virtual environment; and (3) a comparison of participants' preference of format. the overall participant feedback regarding the utilization of the case study method in a virtual environment for professional development and collaborative problem solving is very positive",134,0.5
8326,Computers and IT,"Learning is likely to take a new form in the twenty-first century, and a transformation is already in process. Under the framework of information fluency, efforts are being made at Rollins College to develop a Web-enhanced course that encompasses information literacy, basic computer literacy, and critical thinking skills. Computer-based education can be successful when librarians use technology effectively to enhance their integrated library teaching. In an online learning environment, students choose a time for learning that best suits their needs and motivational levels. They can learn at their own pace, take a nonlinear approach to the subject, and maintain constant communication with instructors and other students. The quality of a technology-facilitated course can be upheld if the educational objectives and methods for achieving those objectives are carefully planned and explored","learn be likely to take a new form in the twenty-first century , and a transformation be already in process . under the framework of information fluency , effort be be make at rollin college to develop a web-enhanced course that encompass information literacy , basic computer literacy , and critical thinking skill . computer-based education can be successful when librarian use technology effectively to enhance their integrate library teaching . in an online learning environment , student choose a time for learn that best suit their need and motivational level . they can learn at their own pace , take a nonlinear approach to the subject , and maintain constant communication with instructor and other student . the quality of a technology-facilitated course can be uphold if the educational objective and method for achieve those objective be carefully plan and explore",130,0.8
8327,Computers and IT,"Current discussions and trends in digital reference have emphasized the use of real-time digital reference services. Recent articles have questioned both the utility and use of asynchronous services such as e-mail. This article uses data from the AskERIC digital reference service to demonstrate that asynchronous services are not only useful and used, but may have greater utility than real-time systems","current discussion and trend in digital reference have emphasize the use ofreal-time digital reference service . recent article have question both the utility and use of asynchronous service such as e-mail . this article use data from the askeric digital reference service to demonstrate that asynchronous service be not only useful and used , but may have great utility than real-time system",59,0.666666667
8328,Computers and IT,The author considers the history of the Guide to Reference Books (GRB) and its importance in librarianship. He discusses the ways in which the new edition is taking advantage of changing times. GRB has become a cornerstone of the literature of U. S. librarianship. The biggest change GRB will undergo to become GRS (Guide to Reference Sources) will be designing it primarily as a Web product,the author considers the history of the guide to reference books (grb) and its importance in librarianship. he discusses the ways in which the new edition is taking advantage of changing times. grb has become a cornerstone of the literature of u.s. librarianship. the biggest change grb will undergo to become grs (guide to reference sources) will be designing it primarily as a web product,65,0.777777778
8329,Computers and IT,"There are many different types of e-commerce depending upon who or what is selling and who or what is buying. In addition, e-commerce is more than an exchange of funds and goods or services, it encompasses an entire infrastructure of services, computer hardware and software products, technologies, and communications formats. The paper discusses e-commerce terminology, types and information resources, including books and Web sites","there be many different type of e-commerce depend upon who or what isselling and who or what be buy . in addition , e-commerce be more than an exchange of fund and good or service , it encompass an entire infrastructure of service , computer hardware and software product , technology , and communication format . the paper discuss e-commerce terminology , type and information resource , include book and web site",63,0.714285714
8330,Computers and IT,"This research develops design recommendations for surface textures (patterns of color on object surfaces) rendered with stereoscopic displays. In 3 method-of-adjustment procedure experiments, 8 participants matched the disparity of a circular probe and a planar stimulus rendered using a single visible edge. The experiments varied stimulus orientation and surface texture. Participants more accurately matched the depth of vertical stimuli than that of horizontal stimuli, consistent with previous studies and existing theory. Participants matched the depth of surfaces with large pixel-to-pixel luminance variations more accurately than they did surfaces with a small pixel-to-pixel luminance variation. Finally, they matched the depth of surfaces with vertical line patterns more accurately than they did surfaces with horizontal-striped texture patterns. These results suggest that designers can enhance depth perception in stereoscopic displays, and also reduce undesirable sensitivity to orientation, by rendering objects with surface textures using large pixel-to-pixel luminance variations","this research develops design recommendations for surface textures (patterns ofcolor on object surfaces) rendered with stereoscopic displays. in 3 method-of-adjustment procedure experiments, 8 participants matched the disparity of a circular probe and a planar stimulus rendered using a single visible edge. the experiments varied stimulus orientation and surface texture. participants more accurately matched the depth of vertical stimuli than that of horizontal stimuli, consistent with previous studies and existing theory. participants matched the depth of surfaces with large pixel-to-pixel luminance variations more accurately than they did surfaces with a small pixel-to-pixel luminance variation. finally, they matched the depth of surfaces with vertical line patterns more accurately than they did surfaces with horizontal-striped texture patterns. these results suggest that designers can enhance depth perception in stereoscopic displays, and also reduce undesirable sensitivity to orientation, by rendering objects with surface textures using large pixel-to-pixel luminance variations",144,0.6
8331,Computers and IT,"For multiple degree-of-freedom (DOF) systems, it is important to determine how accurately operators can control each DOF and what influence perceptual, information processing, and psychomotor components have on performance. Sixteen right-handed male students participated in 2 experiments: 1 involving positioning and 1 involving tracking with 3 translational DOFs. To separate perceptual and psychomotor effects, we used 2 control-display mappings that differed in the coupling of vertical and depth dimensions to the up-down and fore-aft control axes. We observed information processing effects in the positioning task: Initial error correction on the vertical dimension lagged in time behind the horizontal dimension. The depth dimension error correction lagged behind both, which was ascribed to the poorer perceptual information. We observed this perceptual effect also in the tracking experiment. Motor effects were also present, with tracking errors along the up-down axis of the hand controller being 1. 1 times larger than along the fore-aft axis. These results indicate that all 3 components contribute to control performance. Actual applications of this research include interface design","for multiple degree-of-freedom (dof) systems, it is important to determine howaccurately operators can control each dof and what influence perceptual, information processing, and psychomotor components have on performance. sixteen right-handed male students participated in 2 experiments: 1 involving positioning and 1 involving tracking with 3 translational dofs. to separate perceptual and psychomotor effects, we used 2 control-display mappings that differed in the coupling of vertical and depth dimensions to the up-down and fore-aft control axes. we observed information processing effects in the positioning task: initial error correction on the vertical dimension lagged in time behind the horizontal dimension. the depth dimension error correction lagged behind both, which was ascribed to the poorer perceptual information. we observed this perceptual effect also in the tracking experiment. motor effects were also present, with tracking errors along the up-down axis of the hand controller being 1.1 times larger than along the fore-aft axis. these results indicate that all 3 components contribute to control performance. actual applications of this research include interface design",168,0.727272727
8332,Computers and IT,"Contrast sensitivity was determined as a function of target velocity (0 degrees -120 degrees /s) over a variety of viewing conditions. In Experiment 1, measurements of dynamic contrast sensitivity were determined for observers as a function of target velocity for letter stimuli. Significant main effects were found for target velocity, target size, and target duration, but significant interactions among the variables indicated especially pronounced adverse effects of increasing target velocity for small targets and brief durations. In Experiment 2, the effects of simulated cataracts were determined. Although the simulated impairment had no effect on traditional acuity scores, dynamic contrast sensitivity was markedly reduced. Results are discussed in terms of dynamic contrast sensitivity as a useful composite measure of visual functioning that may provide a better overall picture of an individual's visual functioning than does traditional static acuity, dynamic acuity, or contrast sensitivity alone. The measure of dynamic contrast sensitivity may increase understanding of the practical effects of various conditions, such as aging or disease, on the visual system, or it may allow improved prediction of individuals' performance in visually dynamic situations","contrast sensitivity was determined as a function of target velocity (0 degrees -120 degrees /s) over a variety of viewing conditions. in experiment 1, measurements of dynamic contrast sensitivity were determined for observers as a function of target velocity for letter stimuli. significant main effects were found for target velocity, target size, and target duration, but significant interactions among the variables indicated especially pronounced adverse effects of increasing target velocity for small targets and brief durations. in experiment 2, the effects of simulated cataracts were determined. although the simulated impairment had no effect on traditional acuity scores, dynamic contrast sensitivity was markedly reduced. results are discussed in terms of dynamic contrast sensitivity as a useful composite measure of visual functioning that may provide a better overall picture of an individual's visual functioning than does traditional static acuity, dynamic acuity, or contrast sensitivity alone. the measure of dynamic contrast sensitivity may increase understanding of the practical effects of various conditions, such as aging or disease, on the visual system, or it may allow improved prediction of individuals' performance in visually dynamic situations",181,0.636363636
8333,Computers and IT,"The present study investigated the effects of discomfort glare on driving behavior. Participants (old and young; US and Europeans) were exposed to a simulated low- beam light source mounted on the hood of an instrumented vehicle. Participants drove at night in actual traffic along a track consisting of urban, rural, and highway stretches. The results show that the relatively low glare source caused a significant drop in detecting simulated pedestrians along the roadside and made participants drive significantly slower on dark and winding roads. Older participants showed the largest drop in pedestrian detection performance and reduced their driving speed the most. The results indicate that the de Boer rating scale, the most commonly used rating scale for discomfort glare, is practically useless as a predictor of driving performance. Furthermore, the maximum US headlamp intensity (1380 cd per headlamp) appears to be an acceptable upper limit","the present study investigated the effects of discomfort glare on drivingbehavior. participants (old and young; us and europeans) were exposed to a simulated low- beam light source mounted on the hood of an instrumented vehicle. participants drove at night in actual traffic along a track consisting of urban, rural, and highway stretches. the results show that the relatively low glare source caused a significant drop in detecting simulated pedestrians along the roadside and made participants drive significantly slower on dark and winding roads. older participants showed the largest drop in pedestrian detection performance and reduced their driving speed the most. the results indicate that the de boer rating scale, the most commonly used rating scale for discomfort glare, is practically useless as a predictor of driving performance. furthermore, the maximum us headlamp intensity (1380 cd per headlamp) appears to be an acceptable upper limit",144,0.333333333
8334,Computers and IT,"Although increases in the use of automation have occurred across society, research has found that human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman-Riley (1997)). Nearly 275 Cameron University students participated in 1 of 3 experiments performed to examine the effects of perceived utility (Dzindolet et al. (2001)) on automation use in a visual detection task and to compare reliance on automated aids with reliance on humans. Results revealed a bias for human operators to rely on themselves. Although self-report data indicate a bias toward automated aids over human aids, performance data revealed that participants were more likely to disuse automated aids than to disuse human aids. This discrepancy was accounted for by assuming human operators have a ""perfect automation"" schema. Actual or potential applications of this research include the design of future automated decision aids and training procedures for operators relying on such aids","although increases in the use of automation have occurred across society,research has found that human operators often underutilize (disuse) and overly rely on (misuse) automated aids (parasuraman-riley (1997)). nearly 275 cameron university students participated in 1 of 3 experiments performed to examine the effects of perceived utility (dzindolet et al. (2001)) on automation use in a visual detection task and to compare reliance on automated aids with reliance on humans. results revealed a bias for human operators to rely on themselves. although self-report data indicate a bias toward automated aids over human aids, performance data revealed that participants were more likely to disuse automated aids than to disuse human aids. this discrepancy was accounted for by assuming human operators have a ""perfect automation"" schema. actual or potential applications of this research include the design of future automated decision aids and training procedures for operators relying on such aids",148,0.5
8335,Computers and IT,"Ecological interface design (EID) is a theoretical framework for designing human-computer interfaces for complex socio-technical systems. Its primary aim is to support knowledge workers in adapting to change and novelty. This literature review shows that in situations requiring problem solving, EID improves performance when compared with current design approaches in industry. EID has been applied to industry-scale problems in a broad variety of application domains (e. g. , process control, aviation, computer network management, software engineering, medicine, command and control, and information retrieval) and has consistently led to the identification of new information requirements. An experimental evaluation of EID using a full-fidelity simulator with professional workers has yet to be conducted, although some are planned. Several significant challenges remain as obstacles to the confident use of EID in industry. Promising paths for addressing these outstanding issues are identified. Actual or potential applications of this research include improving the safety and productivity of complex socio-technical systems","ecological interface design (eid) is a theoretical framework for designinghuman-computer interfaces for complex socio-technical systems. its primary aim is to support knowledge workers in adapting to change and novelty. this literature review shows that in situations requiring problem solving, eid improves performance when compared with current design approaches in industry. eid has been applied to industry-scale problems in a broad variety of application domains (e.g., process control, aviation, computer network management, software engineering, medicine, command and control, and information retrieval) and has consistently led to the identification of new information requirements. an experimental evaluation of eid using a full-fidelity simulator with professional workers has yet to be conducted, although some are planned. several significant challenges remain as obstacles to the confident use of eid in industry. promising paths for addressing these outstanding issues are identified. actual or potential applications of this research include improving the safety and productivity of complex socio-technical systems",152,0.428571429
8336,Computers and IT,"A laboratory study was conducted to determine the effects of work pace on typing force, electromyographic (EMG) activity, and subjective discomfort. We found that as participants typed faster, their typing force and finger flexor and extensor EMG activity increased linearly. There was also an increase in subjective discomfort, with a sharp threshold between participants' self-selected pace and their maximum typing speed. The results suggest that participants self-select a typing pace that maximizes typing speed and minimizes discomfort. The fastest typists did not produce significantly more finger flexor EMG activity but did produce proportionately less finger extensor EMG activity compared with the slower typists. We hypothesize that fast typists may use different muscle recruitment patterns that allow them to be more efficient than slower typists at striking the keys. In addition, faster typists do not experience more discomfort than slow typists. These findings show that the relative pace of typing is more important than actual typing speed with regard to discomfort and muscle activity. These results suggest that typists may benefit from skill training to increase maximum typing speed. Potential applications of this research includes skill training for typists","a laboratory study was conducted to determine the effects of work pace on typing force, electromyographic (emg) activity, and subjective discomfort. we found that as participants typed faster, their typing force and finger flexor and extensor emg activity increased linearly. there was also an increase in subjective discomfort, with a sharp threshold between participants' self-selected pace and their maximum typing speed. the results suggest that participants self-select a typing pace that maximizes typing speed and minimizes discomfort. the fastest typists did not produce significantly more finger flexor emg activity but did produce proportionately less finger extensor emg activity compared with the slower typists. we hypothesize that fast typists may use different muscle recruitment patterns that allow them to be more efficient than slower typists at striking the keys. in addition, faster typists do not experience more discomfort than slow typists. these findings show that the relative pace of typing is more important than actual typing speed with regard to discomfort and muscle activity. these results suggest that typists may benefit from skill training to increase maximum typing speed. potential applications of this research includes skill training for typists",188,0.7
8337,Computers and IT,"Automated diagnostic aids that are less than perfectly reliable often produce unwarranted levels of disuse by operators. In the present study, users' tendencies to either agree or disagree with automated diagnostic aids were examined under conditions in which: (1) the aids were less than perfectly reliable but aided-diagnosis was still more accurate that unaided diagnosis; and (2) the system was completely opaque, affording users no additional information upon which to base a diagnosis. The results revealed that some users adopted a strategy of always agreeing with the aids, thereby maximizing the number of correct diagnoses made over several trials. Other users, however, adopted a probability-matching strategy in which agreement and disagreement rates matched the rate of correct and incorrect diagnoses of the aids. The probability-matching strategy, therefore, resulted in diagnostic accuracy scores that were lower than was maximally possible. Users who adopted the maximization strategy had higher self-ratings of problem-solving and decision-making skills, were more accurate in estimating aid reliabilities, and were more confident in their diagnosis on trials in which they agreed with the aids. The potential applications of these findings include the design of interface and training solutions that facilitate the adoption of the most effective concurrence strategies by users of automated diagnostic aids","automated diagnostic aids that are less than perfectly reliable often produce unwarranted levels of disuse by operators. in the present study, users' tendencies to either agree or disagree with automated diagnostic aids were examined under conditions in which: (1) the aids were less than perfectly reliable but aided-diagnosis was still more accurate that unaided diagnosis; and (2) the system was completely opaque, affording users no additional information upon which to base a diagnosis. the results revealed that some users adopted a strategy of always agreeing with the aids, thereby maximizing the number of correct diagnoses made over several trials. other users, however, adopted a probability-matching strategy in which agreement and disagreement rates matched the rate of correct and incorrect diagnoses of the aids. the probability-matching strategy, therefore, resulted in diagnostic accuracy scores that were lower than was maximally possible. users who adopted the maximization strategy had higher self-ratings of problem-solving and decision-making skills, were more accurate in estimating aid reliabilities, and were more confident in their diagnosis on trials in which they agreed with the aids. the potential applications of these findings include the design of interface and training solutions that facilitate the adoption of the most effective concurrence strategies by users of automated diagnostic aids",206,0.444444444
8338,Computers and IT,"The rising popularity of Linux, combined with perceived cost savings, has spurred many embedded developers to consider a real-time Linux variant as an alternative to a traditional RTOS. The paper presents the legal implications for the proprietary parts of firmware","the rise popularity of linux , combine with perceive cost saving , hasspurr many embed developer to consider a real-time linux variant as an alternative to a traditional rto . the paper present the legal implication for the proprietary part of firmware",39,0.5
8339,Computers and IT,"This paper discusses a formal and rigorous approach to the analysis of operator interaction with machines. It addresses the acute problem of detecting design errors in human-machine interaction and focuses on verifying the correctness of the interaction in complex and automated control systems. The paper describes a systematic methodology for evaluating whether the interface provides the necessary information about the machine to enable the operator to perform a specified task successfully and unambiguously. It also addresses the adequacy of information provided to the user via training materials (e. g. , user manual) about the machine's behavior. The essentials of the methodology, which can be automated and applied to the verification of large systems, are illustrated by several examples and through a case study of pilot interaction with an autopilot aboard a modern commercial aircraft. The expected application of this methodology is an augmentation and enhancement, by formal verification, of human-automation interfaces","this paper discusses a formal and rigorous approach to the analysis of operatorinteraction with machines. it addresses the acute problem of detecting design errors in human-machine interaction and focuses on verifying the correctness of the interaction in complex and automated control systems. the paper describes a systematic methodology for evaluating whether the interface provides the necessary information about the machine to enable the operator to perform a specified task successfully and unambiguously. it also addresses the adequacy of information provided to the user via training materials (e.g., user manual) about the machine's behavior. the essentials of the methodology, which can be automated and applied to the verification of large systems, are illustrated by several examples and through a case study of pilot interaction with an autopilot aboard a modern commercial aircraft. the expected application of this methodology is an augmentation and enhancement, by formal verification, of human-automation interfaces",148,0.571428571
8340,Computers and IT,"Thirty-six pilots (31 men, 5 women) were tested in a flight simulator on their ability to intercept a pathway depicted on a highway-in-the-sky (HITS) display. While intercepting and flying the pathway, pilots were required to watch for traffic outside the cockpit. Additionally, pilots were tested on their awareness of speed, altitude, and heading during the flight. Results indicated that the presence of a flight guidance cue significantly improved flight path awareness while intercepting the pathway, but significant practice effects suggest that a guidance cue might be unnecessary if pilots are given proper training. The amount of time spent looking outside the cockpit while using the HITS display was significantly less than when using conventional aircraft instruments. Additionally, awareness of flight information present on the HITS display was poor. Actual or potential applications of this research include guidance for the development of perspective flight display standards and as a basis for flight training requirements","thirty-six pilots (31 men, 5 women) were tested in a flight simulator on theirability to intercept a pathway depicted on a highway-in-the-sky (hits) display. while intercepting and flying the pathway, pilots were required to watch for traffic outside the cockpit. additionally, pilots were tested on their awareness of speed, altitude, and heading during the flight. results indicated that the presence of a flight guidance cue significantly improved flight path awareness while intercepting the pathway, but significant practice effects suggest that a guidance cue might be unnecessary if pilots are given proper training. the amount of time spent looking outside the cockpit while using the hits display was significantly less than when using conventional aircraft instruments. additionally, awareness of flight information present on the hits display was poor. actual or potential applications of this research include guidance for the development of perspective flight display standards and as a basis for flight training requirements",152,0.555555556
8341,Computers and IT,We have proposed in a previous note a time discretization for partial differential evolution equation that allows for parallel implementations. This scheme is here reinterpreted as a preconditioning procedure on an algebraic setting of the time discretization. This allows for extending the parallel methodology to the problem of optimal control for partial differential equations. We report a first numerical implementation that reveals a large interest,we have propose in a previous note a time discretization for partialdifferential evolution equation that allow for parallel implementation . this scheme be here reinterpret as a precondition procedure on an algebraic setting of the time discretization . this allow for extend the parallel methodology to the problem of optimal control for partial differential equation . we report a first numerical implementation that reveal a large interest,64,0.625
8342,Computers and IT,"Software maintenance accounts for a significant proportion of the lifetime cost of a software system. Software comprehension is required in many parts of the maintenance process and is one of the most expensive activities. Many tools have been developed to help the maintainer reduce the time and cost of this task, but of the numerous tools and methods available one group has received relatively little attention: those using plausible reasoning to address the concept assignment problem. We present a concept assignment method for COBOL II: hypothesis-based concept assignment (HB-CA). An implementation of a prototype tool is described, and results from a comprehensive evaluation using commercial COBOL II sources are summarised. In particular, we identify areas of a standard maintenance process where such methods would be appropriate, and discuss the potential cost savings that may result","software maintenance accounts for a significant proportion of the lifetime costof a software system. software comprehension is required in many parts of the maintenance process and is one of the most expensive activities. many tools have been developed to help the maintainer reduce the time and cost of this task, but of the numerous tools and methods available one group has received relatively little attention: those using plausible reasoning to address the concept assignment problem. we present a concept assignment method for cobol ii: hypothesis-based concept assignment (hb-ca). an implementation of a prototype tool is described, and results from a comprehensive evaluation using commercial cobol ii sources are summarised. in particular, we identify areas of a standard maintenance process where such methods would be appropriate, and discuss the potential cost savings that may result",134,0.8
8343,Computers and IT,"Diversification of an investment into independently fluctuating assets reduces its risk. In reality, movements of assets are mutually correlated and therefore knowledge of cross-correlations among asset price movements are of great importance. Our results support the possibility that the problem of finding an investment in stocks which exposes invested funds to a minimum level of risk is analogous to the problem of finding the magnetization of a random magnet. The interactions for this ""random magnet problem"" are given by the cross-correlation matrix C of stock returns. We find that random matrix theory allows us to make an estimate for C which outperforms the standard estimate in terms of constructing an investment which carries a minimum level of risk","diversification of an investment into independently fluctuate asset reducesit risk . in reality , movement of asset be mutually correlated and therefore knowledge of cross-correlation among asset price movement be of great importance . our result support the possibility that the problem of find an investment in stock which expose invest fund to a minimum level of risk be analogous to the problem of find the magnetization of a random magnet . the interaction for this `` random magnet problem '' be give by the cross-correlation matrix c of stock return . we find that random matrix theory allow us to make an estimate for c which outperform the standard estimate in term of construct an investment which carry a minimum level of risk",117,0.75
8344,Computers and IT,"Neurosurgical teams, who are normally located in specialist centres, frequently use teleradiology to make a decision about the transfer of a patient to the nearest neurosurgical department. This decision depends on the type of pathology, the clinical status of the patient and the prognosis. If the transfer of the patient is not possible, for example because of an unstable clinical status, a mobile neurosurgical team may be used. We report a case which was dealt with in a remote French military airborne surgical unit, in the Republic of Chad. The unit, which provides health-care to the French military personnel stationed there, also provides free medical care for the local population. It conducts about 100 operations each month. The unit comprises two surgeons (an orthopaedic and a general surgeon), one anaesthetist, two anaesthetic nurses, one operating room nurse, two nurses, three paramedics and a secretary. The civilian patient presented with unstable cervical trauma. A mobile neurosurgeon operated on her, and used telemedicine before, during and after surgery","neurosurgical teams, who are normally located in specialist centres, frequently use teleradiology to make a decision about the transfer of a patient to the nearest neurosurgical department. this decision depends on the type of pathology, the clinical status of the patient and the prognosis. if the transfer of the patient is not possible, for example because of an unstable clinical status, a mobile neurosurgical team may be used. we report a case which was dealt with in a remote french military airborne surgical unit, in the republic of chad. the unit, which provides health-care to the french military personnel stationed there, also provides free medical care for the local population. it conducts about 100 operations each month. the unit comprises two surgeons (an orthopaedic and a general surgeon), one anaesthetist, two anaesthetic nurses, one operating room nurse, two nurses, three paramedics and a secretary. the civilian patient presented with unstable cervical trauma. a mobile neurosurgeon operated on her, and used telemedicine before, during and after surgery",166,0.818181818
8345,Computers and IT,"We developed a hand-held digital colour video-camera for eye examination in primary care. The device weighed 550 g. It featured a charge-coupled device (CCD) and corrective optics. Both colour video and digital still images could be taken. The video-camera was connected to a PC with software for database storage, image processing and telecommunication. We studied 88 normal subjects (38 male, 50 female), aged 7-62 years. It was not necessary to use mydriatic eye drops for pupillary dilation. Satisfactory digital images of the whole face and the anterior eye were obtained. The optic disc and the central part of the ocular fundus could also be recorded. Image quality of the face and the anterior eye were excellent; image quality of the optic disc and macula were good enough for tele-ophthalmology. Further studies are needed to evaluate the usefulness of the equipment in different clinical conditions","we developed a hand-held digital colour video-camera for eye examination inprimary care. the device weighed 550 g. it featured a charge-coupled device (ccd) and corrective optics. both colour video and digital still images could be taken. the video-camera was connected to a pc with software for database storage, image processing and telecommunication. we studied 88 normal subjects (38 male, 50 female), aged 7-62 years. it was not necessary to use mydriatic eye drops for pupillary dilation. satisfactory digital images of the whole face and the anterior eye were obtained. the optic disc and the central part of the ocular fundus could also be recorded. image quality of the face and the anterior eye were excellent; image quality of the optic disc and macula were good enough for tele-ophthalmology. further studies are needed to evaluate the usefulness of the equipment in different clinical conditions",143,0.857142857
8346,Computers and IT,"Two hundred general practitioners were equipped with a portable electrocardiograph which could transmit a 12-lead electrocardiogram (ECG) via a telephone line. A cardiologist was available 24 h a day for an interactive teleconsultation. In a 13 month period there were 5073 calls to the telecardiology service and 952 subjects with chest pain were identified. The telecardiology service allowed the general practitioners to manage 700 cases (74%) themselves; further diagnostic tests were requested for 162 patients (17%) and 83 patients (9%) were sent to the hospital emergency department. In the last group a cardiological diagnosis was confirmed in 60 patients and refuted in 23. Seven patients in whom the telecardiology service failed to detect a cardiac problem were hospitalized in the subsequent 48 h. The telecardiology service showed a sensitivity of 97. 4%, a specificity of 89. 5% and a diagnostic accuracy of 86. 9% for chest pain. Telemedicine could be a useful tool in the diagnosis of chest pain in primary care","two hundred general practitioners were equipped with a portableelectrocardiograph which could transmit a 12-lead electrocardiogram (ecg) via a telephone line. a cardiologist was available 24 h a day for an interactive teleconsultation. in a 13 month period there were 5073 calls to the telecardiology service and 952 subjects with chest pain were identified. the telecardiology service allowed the general practitioners to manage 700 cases (74%) themselves; further diagnostic tests were requested for 162 patients (17%) and 83 patients (9%) were sent to the hospital emergency department. in the last group a cardiological diagnosis was confirmed in 60 patients and refuted in 23. seven patients in whom the telecardiology service failed to detect a cardiac problem were hospitalized in the subsequent 48 h. the telecardiology service showed a sensitivity of 97.4%, a specificity of 89.5% and a diagnostic accuracy of 86.9% for chest pain. telemedicine could be a useful tool in the diagnosis of chest pain in primary care",158,0.8
8347,Computers and IT,"We studied consultations between a doctor, emergency nurse practitioners (ENPs) and their patients in a minor accident and treatment service (MATS). In the conventional consultations, all three people were located at the main hospital. In the teleconsultations, the doctor was located in a hospital 6 km away from the MATS and used a videoconferencing link connected at 384 kbit/s. There were 30 patients in the conventional group and 30 in the telemedical group. The presenting problems were similar in the two groups. The mean duration of teleconsultations was 951 s and the mean duration of face-to-face consultations was 247 s. In doctor-nurse communication there was a higher rate of turn taking in teleconsultations than in face-to-face consultations; there were also more interruptions, more words and more `backchannels' (e. g. `mhm', `uh-huh') per teleconsultation. In doctor-patient communication there was a higher rate of turn taking, more words, more interruptions and more backchannels per teleconsultation. In patient-nurse communication there was. relatively little difference between the two modes of consulting the doctor. Telemedicine appeared to empower the patient to ask more questions of the doctor. It also seemed that the doctor took greater care in a teleconsultation to achieve coordination of beliefs with the patient than in a face-to-face consultation","we studied consultations between a doctor, emergency nurse practitioners (enps)and their patients in a minor accident and treatment service (mats). in the conventional consultations, all three people were located at the main hospital. in the teleconsultations, the doctor was located in a hospital 6 km away from the mats and used a videoconferencing link connected at 384 kbit/s. there were 30 patients in the conventional group and 30 in the telemedical group. the presenting problems were similar in the two groups. the mean duration of teleconsultations was 951 s and the mean duration of face-to-face consultations was 247 s. in doctor-nurse communication there was a higher rate of turn taking in teleconsultations than in face-to-face consultations; there were also more interruptions, more words and more `backchannels' (e.g. `mhm', `uh-huh') per teleconsultation. in doctor-patient communication there was a higher rate of turn taking, more words, more interruptions and more backchannels per teleconsultation. in patient-nurse communication there was. relatively little difference between the two modes of consulting the doctor. telemedicine appeared to empower the patient to ask more questions of the doctor. it also seemed that the doctor took greater care in a teleconsultation to achieve coordination of beliefs with the patient than in a face-to-face consultation",205,0.842105263
8348,Computers and IT,"We carried out a prospective study of an Internet-based remote counselling service. A total of 15, 456 Internet users visited the Web site over eight years. From these, 1500 users were randomly selected for analysis. Medical counselling had been granted to 901 of the people requesting it (60%). One hundred and sixty-four physicians formed project groups to process the requests and responded using email. The distribution of patients using the service was similar to the availability of the Internet: 78% were from the European Union, North America and Australia. Sixty-seven per cent of the patients lived in urban areas and the remainder were residents of remote rural areas with limited local medical coverage. Sixty-five per cent of the requests were about problems of internal medicine and 30% of the requests concerned surgical issues. The remaining 5% of the patients sought information about recent developments, such as molecular medicine or aviation medicine. During the project, our portal became inaccessible five times, and counselling was not possible on 44 days. There was no hacking of the Web site. Internet-based medical counselling is a helpful addition to conventional practice","we carried out a prospective study of an internet-based remote counsellingservice. a total of 15,456 internet users visited the web site over eight years. from these, 1500 users were randomly selected for analysis. medical counselling had been granted to 901 of the people requesting it (60%). one hundred and sixty-four physicians formed project groups to process the requests and responded using email. the distribution of patients using the service was similar to the availability of the internet: 78% were from the european union, north america and australia. sixty-seven per cent of the patients lived in urban areas and the remainder were residents of remote rural areas with limited local medical coverage. sixty-five per cent of the requests were about problems of internal medicine and 30% of the requests concerned surgical issues. the remaining 5% of the patients sought information about recent developments, such as molecular medicine or aviation medicine. during the project, our portal became inaccessible five times, and counselling was not possible on 44 days. there was no hacking of the web site. internet-based medical counselling is a helpful addition to conventional practice",184,0.7
8349,Computers and IT,"Logics of general branching time, or historical necessity, have long been studied but important axiomatization questions remain open. Here the difficulties of finding axioms for such logics are considered and ideas for solving some of the main open problems are presented. A new, more expressive logical account is also given to support Peirce's prohibition on truth values being attached to the contingent future","logic of general branch time , or historical necessity , have long beenstudi but important axiomatization question remain open . here the difficulty of find axiom for such logic be consider and idea for solve some of the main open problem be present . a new , more expressive logical account be also give to support peirce 's prohibition on truth value be attach to the contingent future",62,0.75
8350,Computers and IT,"We carried out a pilot study comparing satisfaction levels between psychiatric patients seen face to face (FTF) and those seen via videoconference. Patients who consented were randomly assigned to one of two groups. One group received services in person (FTF from the visiting psychiatrist) while the other was seen using videoconferencing at 128 kbit/s. One psychiatrist provided all the FTF and videoconferencing assessment and follow-up visits. A total of 24 subjects were recruited. Three of the subjects (13%) did not attend their appointments and two subjects in each group were lost to follow-up. Thus there were nine in the FTF group and eight in the videoconferencing group. The two groups were similar in most respects. Patient satisfaction with the services was assessed using the Client Satisfaction Questionnaire (CSQ-8), completed four months after the initial consultation. The mean scores were 25. 3 in the FTF group and 21. 6 in the videoconferencing group. Although there was a trend in favour of the FTF service, the difference was not significant. Patient satisfaction is only one component of evaluation. The efficacy of telepsychiatry must also be measured relative to that of conventional, FTF care before policy makers can decide how extensively telepsychiatry should be implemented","we carried out a pilot study comparing satisfaction levels between psychiatric patients seen face to face (ftf) and those seen via videoconference. patients who consented were randomly assigned to one of two groups. one group received services in person (ftf from the visiting psychiatrist) while the other was seen using videoconferencing at 128 kbit/s. one psychiatrist provided all the ftf and videoconferencing assessment and follow-up visits. a total of 24 subjects were recruited. three of the subjects (13%) did not attend their appointments and two subjects in each group were lost to follow-up. thus there were nine in the ftf group and eight in the videoconferencing group. the two groups were similar in most respects. patient satisfaction with the services was assessed using the client satisfaction questionnaire (csq-8), completed four months after the initial consultation. the mean scores were 25.3 in the ftf group and 21.6 in the videoconferencing group. although there was a trend in favour of the ftf service, the difference was not significant. patient satisfaction is only one component of evaluation. the efficacy of telepsychiatry must also be measured relative to that of conventional, ftf care before policy makers can decide how extensively telepsychiatry should be implemented",200,0.444444444
8351,Computers and IT,"We evaluated various aspects of grand rounds videoconferenced from a tertiary care hospital to a regional hospital in Nova Scotia. During a five-month study period, 29 rounds were broadcast (19 in medicine and 10 in cardiology). The total recorded attendance at the remote site was 103, comprising 70 specialists, nine family physicians and 24 other health-care professionals. We received 55 evaluations, a response rate of 53%. On a five-point Likert scale (on which higher scores indicated better quality), mean ratings by remote-site participants of the technical quality of the videoconference were 3. 0-3. 5, with the lowest ratings being for ability to hear the discussion (3. 0) and to see visual aids (3. 1). Mean ratings for content, presentation, discussion and educational value were 3. 8 or higher. Of the 49 physicians who presented the rounds, we received evaluations from 41, a response rate of 84%. The presenters rated all aspects of the videoconference and interaction with remote sites at 3. 8 or lower. The lowest ratings were for ability to see the remote sites (3. 0) and the usefulness of the discussion (3. 4). We received 278 evaluations from participants at the presenting site, an estimated response rate of about 55%. The results indicated no adverse opinions of the effect of videoconferencing (mean scores 3. 1-3. 3). The estimated costs of videoconferencing one grand round to one site and four sites were C$723 and C$1515, respectively. The study confirmed that videoconferenced rounds can provide satisfactory continuing medical education to community specialists, which is an especially important consideration as maintenance of certification becomes mandatory","we evaluated various aspects of grand rounds videoconferenced from a tertiarycare hospital to a regional hospital in nova scotia. during a five-month study period, 29 rounds were broadcast (19 in medicine and 10 in cardiology). the total recorded attendance at the remote site was 103, comprising 70 specialists, nine family physicians and 24 other health-care professionals. we received 55 evaluations, a response rate of 53%. on a five-point likert scale (on which higher scores indicated better quality), mean ratings by remote-site participants of the technical quality of the videoconference were 3.0-3.5, with the lowest ratings being for ability to hear the discussion (3.0) and to see visual aids (3.1). mean ratings for content, presentation, discussion and educational value were 3.8 or higher. of the 49 physicians who presented the rounds, we received evaluations from 41, a response rate of 84%. the presenters rated all aspects of the videoconference and interaction with remote sites at 3.8 or lower. the lowest ratings were for ability to see the remote sites (3.0) and the usefulness of the discussion (3.4). we received 278 evaluations from participants at the presenting site, an estimated response rate of about 55%. the results indicated no adverse opinions of the effect of videoconferencing (mean scores 3.1-3.3). the estimated costs of videoconferencing one grand round to one site and four sites were c$723 and c$1515, respectively. the study confirmed that videoconferenced rounds can provide satisfactory continuing medical education to community specialists, which is an especially important consideration as maintenance of certification becomes mandatory",253,0.6
8352,Computers and IT,"We conducted a systematic review of the literature to evaluate the efficacy of telemedicine for making diagnostic and management decisions in three classes of application: office/hospital-based, store-and-forward, and home-based telemedicine. We searched the MEDLINE, EMBASE, CINAHL and HealthSTAR databases and printed resources, and interviewed investigators in the field. We excluded studies where the service did not historically require face-to-face encounters (e. g. radiology or pathology diagnosis). A total of 58 articles met the inclusion criteria. The articles were summarized and graded for the quality and direction of the evidence. There were very few high-quality studies. The strongest evidence for the efficacy of telemedicine for diagnostic and management decisions came from the specialties of psychiatry and dermatology. There was also reasonable evidence that general medical history and physical examinations performed via telemedicine had relatively good sensitivity and specificity. Other specialties in which some evidence for efficacy existed were cardiology and certain areas of ophthalmology. Despite the widespread use of telemedicine in most major medical specialties, there is strong evidence in only a few of them that the diagnostic and management decisions provided by telemedicine are comparable to face-to-face care","we conducted a systematic review of the literature to evaluate the efficacy of telemedicine for making diagnostic and management decisions in three classes of application: office/hospital-based, store-and-forward, and home-based telemedicine. we searched the medline, embase, cinahl and healthstar databases and printed resources, and interviewed investigators in the field. we excluded studies where the service did not historically require face-to-face encounters (e.g. radiology or pathology diagnosis). a total of 58 articles met the inclusion criteria. the articles were summarized and graded for the quality and direction of the evidence. there were very few high-quality studies. the strongest evidence for the efficacy of telemedicine for diagnostic and management decisions came from the specialties of psychiatry and dermatology. there was also reasonable evidence that general medical history and physical examinations performed via telemedicine had relatively good sensitivity and specificity. other specialties in which some evidence for efficacy existed were cardiology and certain areas of ophthalmology. despite the widespread use of telemedicine in most major medical specialties, there is strong evidence in only a few of them that the diagnostic and management decisions provided by telemedicine are comparable to face-to-face care",187,0.75
8353,Computers and IT,"A literature review was conducted to investigate the extent to which telehealth has been researched within the domain of speech-language pathology and the outcomes of this research. A total of 13 studies were identified. Three early studies demonstrated that telehealth was feasible, although there was no discussion of the cost-effectiveness of this process in terms of patient outcomes. The majority of the subsequent studies indicated positive or encouraging outcomes resulting from telehealth. However, there were a number of shortcomings in the research, including a lack of cost-benefit information, failure to evaluate the technology itself, an absence of studies of the educational and informational aspects of telehealth in relation to speech-language pathology, and the use of telehealth in a limited range of communication disorders. Future research into the application of telehealth to speech-language pathology services must adopt a scientific approach, and have a well defined development and evaluation framework that addresses the effectiveness of the technique, patient outcomes and satisfaction, and the cost-benefit relationship","a literature review be conduct to investigate the extent to which telehealthha be research within the domain of speech-language pathology and the outcome of this research . a total of 13 study be identify . three early study demonstrate that telehealth be feasible , although there be no discussion of the cost-effectivenes of this process in term of patient outcome . the majority of the subsequent study indicate positive or encouraging outcome result from telehealth . however , there be a number of shortcoming in the research , include a lack of cost-benefit information , failure to evaluate the technology itself , an absence of study of the educational and informational aspect of telehealth in relation to speech-language pathology , and the use of telehealth in a limited range of communication disorder . future research into the application of telehealth to speech-language pathology service must adopt a scientific approach , and have a well define development and evaluation framework that address the effectiveness of the technique , patient outcome and satisfaction , and the cost-benefit relationship",162,0.555555556
8354,Computers and IT,"PC makers are dwindling. If you are planning to make a PC purchase soon, here are a few things to look out for before you buy","pc maker be dwindle . if you be plan to make a pc purchase soon , hereare a few thing to look out for before you buy",25,1
8355,Computers and IT,"Intel's Pentium 4-M processor has reached the coveted 2-GHz mark, and speed-hungry mobile users will be tempted to buy a laptop with the chip. However, while our exclusive tests found 2-GHz P4-M notebooks among the fastest units we've tested, the new models failed to make dramatic gains compared with those based on Intel's 1. 8-GHz mobile chip. Since 2-GHz notebooks carry a hefty price premium, buyers seeking both good performance and a good price might prefer a 1. 8-GHz unit instead","intel 's pentium 4-m processor have reach the coveted 2-ghz mark , andspeed-hungry mobile user will be tempt to buy a laptop with the chip . however , while our exclusive test find 2-ghz p4-m notebook among the fast unit we 've test , the new model fail to make dramatic gain compare with those base on intel 's 1.8-ghz mobile chip . since 2-ghz notebook carry a hefty price premium , buyer seek both good performance and a good price might prefer a 1.8-ghz unit instead",78,0.6
8356,Computers and IT,"Financed by advertising dollars from big names, online marketers are embracing more aggressive tactics","finance by advertise dollar from big name , online marketer be embracingmore aggressive tactic",13,0.5
8357,Computers and IT,"Accessing documents and services on today's Web requires human intelligence. The interface to these documents and services is the Web page, written in natural language, which humans must understand and act upon. The paper discusses the Semantic Web which will augment the current Web with formalized knowledge and data that computers can process. In the future, some services will mix human-readable and structured data so that both humans and computers can use them. Others will support formalized knowledge that only machines will use","access document and service on today 's web require human intelligence . the interface to these document and service be the web page , write in natural language , which human must understand and act upon . the paper discuss the semantic web which will augment the current web with formalize knowledge and data that computer can process . in the future , some service will mix human-readable and structured data so that both human and computer can use them . other will support formalize knowledge that only machine will use",82,0.571428571
8358,Computers and IT,"If John McCarthy, the father of AI, were to coin a new phrase for ""artificial intelligence"" today, he would probably use ""computational intelligence. "" McCarthy is not just the father of AI, he is also the inventor of the Lisp (list processing) language. The author considers McCarthy's conception of Lisp and discusses McCarthy's recent research that involves elaboration tolerance, creativity by machines, free will of machines, and some improved ways of doing situation calculus","if john mccarthy, the father of ai, were to coin a new phrase for ""artificialintelligence"" today, he would probably use ""computational intelligence."" mccarthy is not just the father of ai, he is also the inventor of the lisp (list processing) language. the author considers mccarthy's conception of lisp and discusses mccarthy's recent research that involves elaboration tolerance, creativity by machines, free will of machines, and some improved ways of doing situation calculus",72,0.9
8359,Computers and IT,"In the wake of the computer and information technology revolutions, vehicles are undergoing dramatic changes in their capabilities and how they interact with drivers. Although some vehicles can decide to either generate warnings for the human driver or control the vehicle autonomously, they must usually make these decisions in real time with only incomplete information. So, human drivers must still maintain control over the vehicle. I sketch a digital driving behavior model. By simulating and analyzing driver behavior during different maneuvers such as lane changing, lane following, and traffic avoidance, researchers participating in the Beijing Institute of Technology's digital-driving project will be able to examine the possible correlations or causal relations between the smart vehicle, IVISs, the intelligent road-traffic-information network, and the driver. We aim to successfully demonstrate that a digital-driving system can provide a direction for developing human-centered smart vehicles","in the wake of the computer and information technology revolution , vehiclesare undergo dramatic change in their capability and how they interact with driver . although some vehicle can decide to either generate warning for the human driver or control the vehicle autonomously , they must usually make these decision in real time with only incomplete information . so , human driver must still maintain control over the vehicle . i sketch a digital driving behavior model . by simulate and analyze driver behavior during different maneuver such as lane change , lane following , and traffic avoidance , researcher participate in the beij institute of technology 's digital-driving project will be able to examine the possible correlation or causal relation between the smart vehicle , ivis , the intelligent road-traffic-information network , and the driver . we aim to successfully demonstrate that a digital-driving system can provide a direction for develop human-centered smart vehicle",140,0.357142857
8360,Computers and IT,"In this paper, a generalization of Kamp's theorem relative to the functional completeness of the until operator is proved. Such a generalization consists in showing the functional completeness of more expressive temporal operators with respect to the extension of the first-order theory of linear orders MFO[] with an extra binary relational symbol. The result is motivated by the search of a modal language capable of expressing properties and operators suitable to model time granularity in omega -layered temporal structures","in this paper, a generalization of kamp's theorem relative to the functionalcompleteness of the until operator is proved. such a generalization consists in showing the functional completeness of more expressive temporal operators with respect to the extension of the first-order theory of linear orders mfo[<] with an extra binary relational symbol. the result is motivated by the search of a modal language capable of expressing properties and operators suitable to model time granularity in omega -layered temporal structures",78,0.777777778
8361,Computers and IT,"By all measures, the Web is enormous and growing at a staggering rate, which has made it increasingly difficult-and important-for both people and programs to have quick and accurate access to Web information and services. The Semantic Web offers a solution, capturing and exploiting the meaning of terms to transform the Web from a platform that focuses on presenting information, to a platform that focuses on understanding and reasoning with information. To support Semantic Web development, the US Defense Advanced Research Projects Agency launched the DARPA Agent Markup Language (DAML) initiative to fund research in languages, tools, infrastructure, and applications that make Web content more accessible and understandable. Although the US government funds DAML, several organizations-including US and European businesses and universities, and international consortia such as the World Wide Web Consortium-have contributed to work on issues related to DAML's development and deployment. We focus on DAML's current markup language, DAML+OIL, which is a proposed starting point for the W3C's Semantic Web Activity's Ontology Web Language (OWL). We introduce DAML+OIL syntax and usage through a set of examples, drawn from a wine knowledge base used to teach novices how to build ontologies","by all measures, the web is enormous and growing at a staggering rate, whichhas made it increasingly difficult-and important-for both people and programs to have quick and accurate access to web information and services. the semantic web offers a solution, capturing and exploiting the meaning of terms to transform the web from a platform that focuses on presenting information, to a platform that focuses on understanding and reasoning with information. to support semantic web development, the us defense advanced research projects agency launched the darpa agent markup language (daml) initiative to fund research in languages, tools, infrastructure, and applications that make web content more accessible and understandable. although the us government funds daml, several organizations-including us and european businesses and universities, and international consortia such as the world wide web consortium-have contributed to work on issues related to daml's development and deployment. we focus on daml's current markup language, daml+oil, which is a proposed starting point for the w3c's semantic web activity's ontology web language (owl). we introduce daml+oil syntax and usage through a set of examples, drawn from a wine knowledge base used to teach novices how to build ontologies",191,1
8362,Computers and IT,"Molecular biology's advent in the 20th century has exponentially increased our knowledge about the inner workings of life. We have dozens of completed genomes and an array of high-throughput methods to characterize gene encodings and gene product operation. The question now is how we will assemble the various pieces. In other words, given sufficient information about a living cell's molecular components, can we predict its behavior? We introduce the major classes of cellular processes relevant to modeling, discuss software engineering's role in cell simulation, and identify cell simulation requirements. Our E-Cell project aims to develop the theories, techniques, and software platforms necessary for whole-cell-scale modeling, simulation, and analysis. Since the project's launch in 1996, we have built a variety of cell models, and we are currently developing new models that vary with respect to species, target subsystem, and overall scale","molecular biology 's advent in the 20th century have exponentially increase ourknowledge about the inner working of life . we have dozen of complete genome and an array of high-throughput method to characterize gene encoding and gene product operation . the question now be how we will assemble the various piece . in other word , give sufficient information about a living cell 's molecular component , can we predict its behavior ? we introduce the major class of cellular process relevant to modeling , discuss software engineering 's role in cell simulation , and identify cell simulation requirement . our e-cell project aim to develop the theory , technique , and software platform necessary for whole-cell-scale modeling , simulation , and analysis . since the project 's launch in 1996 , we have build a variety of cell model , and we be currently develop new model that vary with respect to specie , target subsystem , and overall scale",139,0.833333333
8363,Computers and IT,"Surveys are important tools for marketing and for managing customer relationships; the answers to open-ended questions, in particular, often contain valuable information and provide an important basis for business decisions. The summaries that human analysts make of these open answers, however, tend to rely too much on intuition and so aren't satisfactorily reliable. Moreover, because the Web makes it so easy to take surveys and solicit comments, companies are finding themselves inundated with data from questionnaires and other sources. Handling it all manually would be not only cumbersome but also costly. Thus, devising a computer system that can automatically mine useful information from open answers has become an important issue. We have developed a survey analysis system that works on these principles. The system mines open answers through two statistical learning techniques: rule learning (which we call rule analysis) and correspondence analysis","surveys are important tools for marketing and for managing customerrelationships; the answers to open-ended questions, in particular, often contain valuable information and provide an important basis for business decisions. the summaries that human analysts make of these open answers, however, tend to rely too much on intuition and so aren't satisfactorily reliable. moreover, because the web makes it so easy to take surveys and solicit comments, companies are finding themselves inundated with data from questionnaires and other sources. handling it all manually would be not only cumbersome but also costly. thus, devising a computer system that can automatically mine useful information from open answers has become an important issue. we have developed a survey analysis system that works on these principles. the system mines open answers through two statistical learning techniques: rule learning (which we call rule analysis) and correspondence analysis",141,0.375
8364,Computers and IT,"With the proliferation of harmful Internet content such as pornography, violence, and hate messages, effective content-filtering systems are essential. Many Web-filtering systems are commercially available, and potential users can download trial versions from the Internet. However, the techniques these systems use are insufficiently accurate and do not adapt well to the ever-changing Web. To solve this problem, we propose using artificial neural networks to classify Web pages during content filtering. We focus on blocking pornography because it is among the most prolific and harmful Web content. However, our general framework is adaptable for filtering other objectionable Web material","with the proliferation of harmful internet content such as pornography , violence , and hate message , effective content-filtering system be essential . many web-filtering system be commercially available , and potential user can download trial version from the internet . however , the technique these system use be insufficiently accurate and do not adapt well to the ever-changing web . to solve this problem , we propose use artificial neural network to classify web page during content filter . we focus on block pornography because it be among the most prolific and harmful web content . however , our general framework be adaptable for filter other objectionable web material",97,0.375
8365,Computers and IT,"Many computer systems are designed according to engineering and technology principles and are typically difficult to learn and use. The fields of human-computer interaction, interface design, and human factors have made significant contributions to ease of use and are primarily concerned with the interfaces between systems and users, not with the structures that are often more fundamental for designing truly human-centered systems. The emerging paradigm of human-centered computing (HCC)-which has taken many forms-offers a new look at system design. HCC requires more than merely designing an artificial agent to supplement a human agent. The dynamic interactions in a distributed system composed of human and artificial agents-and the context in which the system is situated-are indispensable factors. While we have successfully applied our methodology in designing a prototype of a human-centered intelligent flight-surgeon console at NASA Johnson Space Center, this article presents a methodology for designing human-centered computing systems using electronic medical records (EMR) systems","many computer systems are designed according to engineering and technologyprinciples and are typically difficult to learn and use. the fields of human-computer interaction, interface design, and human factors have made significant contributions to ease of use and are primarily concerned with the interfaces between systems and users, not with the structures that are often more fundamental for designing truly human-centered systems. the emerging paradigm of human-centered computing (hcc)-which has taken many forms-offers a new look at system design. hcc requires more than merely designing an artificial agent to supplement a human agent. the dynamic interactions in a distributed system composed of human and artificial agents-and the context in which the system is situated-are indispensable factors. while we have successfully applied our methodology in designing a prototype of a human-centered intelligent flight-surgeon console at nasa johnson space center, this article presents a methodology for designing human-centered computing systems using electronic medical records (emr) systems",153,0.5
8366,Computers and IT,"Work systems involve people engaging in activities over time-not just with each other, but also with machines, tools, documents, and other artifacts. These activities often produce goods, services, or-as is the case in the work system described in this article-scientific data. Work systems and work practice evolve slowly over time. The integration and use of technology, the distribution and collocation of people, organizational roles and procedures, and the facilities where the work occurs largely determine this evolution","work system involve people engage in activity over time-not just with eachother , but also with machine , tool , document , and other artifact . these activity often produce good , service , or-a be the case in the work system describe in this article-scientific data . work system and work practice evolve slowly over time . the integration and use of technology , the distribution and collocation of people , organizational role and procedure , and the facility where the work occur largely determine this evolution",76,0
8367,Computers and IT,"Future manned space operations will include a greater use of automation than we currently see. For example, semiautonomous robots and software agents will perform difficult tasks while operating unattended most of the time. As these automated agents become more prevalent, human contact with them will occur more often and become more routine, so designing these automated agents according to the principles of human-centered computing is important. We describe two cases of semiautonomous control software developed and fielded in test environments at the NASA Johnson Space Center. This software operated continuously at the JSC and interacted closely with humans for months at a time","future man space operation will include a great use of automation than wecurrently see . for example , semiautonomous robot and software agent will perform difficult task while operate unattended most of the time . as these automate agent become more prevalent , human contact with them will occur more often and become more routine , so design these automate agent accord to the principle of human-centered computing be important . we describe two case of semiautonomous control software develop and field in test environment at the nasa johnson space center . this software operate continuously at the jsc and interacted closely with human for month at a time",102,0.538461538
8368,Computers and IT,"In the late 1990s, tightly coordinated airline schedules unraveled owing to massive delays resulting from inclement weather, overbooked flights, and airline operational difficulties. As schedules slipped, the delayed departures and late arrivals led to systemwide breakdowns, customers missed their connections, and airline work activities fell further out of sync. In offering possible answers, we emphasize the need to consider the customer as participant, following the human-centered computing model. Our study applied ethnographic methods to understand the airline system domain and the nature of airline delays, and it revealed the deficiencies of the airline production system model of operations. The research insights that led us to shift from a production and marketing system perspective to a customer-as-participant view might appear obvious to some readers. However, we do not know of any airline that designs its operations and technologies around any other model than the production and marketing system view. Our human-centered analysis used ethnographic methods to gather information, offering new insight into airline delays and suggesting effective ways to improve operations reliability","in the late 1990s , tightly coordinate airline schedule unravel owing tomassive delay result from inclement weather , overbooked flight , and airline operational difficulty . as schedule slip , the delay departure and late arrival lead to systemwide breakdown , customer miss their connection , and airline work activity fall far out of sync . in offer possible answer , we emphasize the need to consider the customer as participant , follow the human-centered computing model . our study apply ethnographic method to understand the airline system domain and the nature of airline delay , and it reveal the deficiency of the airline production system model of operation . the research insight that lead us to shift from a production and marketing system perspective to a customer-as-participant view might appear obvious to some reader . however , we do not know of any airline that design its operation and technology around any other model than the production and marketing system view . our human-centered analysis use ethnographic method to gather information , offer new insight into airline delay and suggest effective way to improve operation reliability",170,0.4
8369,Computers and IT,"In early May, media inquiries started arriving at my office at the Center for Robot-Assisted Search and Rescue (www. crasar. org). Because I'm CRASAR's director, I thought the press was calling to follow up on the recent humanitarian award given to the center's founder, John Blitch, for successfully using small, backpackable robots at the World Trade Center disaster. Instead, I found they were asking me to comment on the ""roborats"" study in the 2 May 2002 Nature. In this study, rats with medial force brain implants underwent operant conditioning to force them into a form of guided behavior, one aspect of which was thought useful for search and rescue. The article's closing comment suggested that a guided rat could serve as both a mobile robot and a biological sensor. Although a roboticist by training, I'm committed to any technology that will help save lives while reducing the risk to rescuers. But rats?","in early may, media inquiries started arriving at my office at the center forrobot-assisted search and rescue (www.crasar.org). because i'm crasar's director, i thought the press was calling to follow up on the recent humanitarian award given to the center's founder, john blitch, for successfully using small, backpackable robots at the world trade center disaster. instead, i found they were asking me to comment on the ""roborats"" study in the 2 may 2002 nature. in this study, rats with medial force brain implants underwent operant conditioning to force them into a form of guided behavior, one aspect of which was thought useful for search and rescue. the article's closing comment suggested that a guided rat could serve as both a mobile robot and a biological sensor. although a roboticist by training, i'm committed to any technology that will help save lives while reducing the risk to rescuers. but rats?",149,0.75
8370,Computers and IT,"interface We construct and study a two-dimensional model of the work of the system of wells in a layer with the mobile boundary between liquids of various viscosity. We use a 'plunger' displacement model of liquids. The boundaries of the filtration region of these liquids are modelled by curves of the Lyapunov class. Unlike familiar work, we solve two-dimensonal problems in an inhomogeneous layer when the mobile boundary and the boundaries of the filtration region are modelled by curves of the Lyapunov class. We show the practical convergence of the numerical solution of the problems studied","interface we construct and study a two-dimensional model of the work of the system of well in a layer with the mobile boundary between liquid of various viscosity . we use a ` plunger ' displacement model of liquid . the boundary of the filtration region of these liquid be model by curve of the lyapunov class . unlike familiar work , we solve two-dimensonal problem in an inhomogeneous layer when the mobile boundary and the boundary of the filtration region be model by curve of the lyapunov class . we show the practical convergence of the numerical solution of the problem study",96,0.4
8371,Computers and IT,"This paper defines and examines model checking games for the branching time temporal logic CTL*. The games employ a technique called focus which enriches sets by picking out one distinguished element. This is necessary to avoid ambiguities in the regeneration of temporal operators. The correctness of these games is proved, and optimizations are considered to obtain model checking games for important fragments of CTL*. A game based model checking algorithm that matches the known lower and upper complexity bounds is sketched","this paper define and examine model checking game for the branch timetemporal logic ctl * . the game employ a technique call focus which enrich set by pick out one distinguished element . this be necessary to avoid ambiguity in the regeneration of temporal operator . the correctness of these game be prove , and optimization be consider to obtain model check game for important fragment of ctl * . a game base model check algorithm that match the know low and upper complexity bound be sketch",80,0.8
8372,Computers and IT,"For the case of isotropic diffusion we consider the representation of the weighted concentration of trajectories and its space derivatives in the form of integrals (with some weights) of the solution to the corresponding boundary value problem and its directional derivative of a convective velocity. If the convective velocity at the domain boundary is degenerate and some other additional conditions are imposed this representation allows us to construct an efficient 'random walk by spheres and balls' algorithm. When these conditions are violated, transition to modelling the diffusion trajectories by the Euler scheme is realized, and the directional derivative of velocity is estimated by the dependent testing method, using the parallel modelling of two closely-spaced diffusion trajectories. We succeeded in justifying this method by statistically equivalent transition to modelling a single trajectory after the first step in the Euler scheme, using a suitable weight. This weight also admits direct differentiation with respect to the initial coordinate along a given direction. The resulting weight algorithm for calculating concentration derivatives is especially efficient if the initial point is in the subdomain in which the coefficients of the diffusion equation are constant","for the case of isotropic diffusion we consider the representation of the weighted concentration of trajectories and its space derivatives in the form of integrals (with some weights) of the solution to the corresponding boundary value problem and its directional derivative of a convective velocity. if the convective velocity at the domain boundary is degenerate and some other additional conditions are imposed this representation allows us to construct an efficient 'random walk by spheres and balls' algorithm. when these conditions are violated, transition to modelling the diffusion trajectories by the euler scheme is realized, and the directional derivative of velocity is estimated by the dependent testing method, using the parallel modelling of two closely-spaced diffusion trajectories. we succeeded in justifying this method by statistically equivalent transition to modelling a single trajectory after the first step in the euler scheme, using a suitable weight. this weight also admits direct differentiation with respect to the initial coordinate along a given direction. the resulting weight algorithm for calculating concentration derivatives is especially efficient if the initial point is in the subdomain in which the coefficients of the diffusion equation are constant",188,0.681818182
8373,Computers and IT,We consider a finite element approximation and iteration algorithms for solving stiff elliptic boundary value problems with large parameters in front of a higher derivative. The convergence rate of the algorithms is independent of the spread in coefficients and a discretization parameter,we consider a finite element approximation and iteration algorithm for solvingstiff elliptic boundary value problem with large parameter in front of a high derivative . the convergence rate of the algorithm be independent of the spread in coefficient and a discretization parameter,41,0.857142857
8374,Computers and IT,We consider problems of statistical analysis of share prices and propose probabilistic characteristics to describe the price series. We discuss three methods of mathematical modelling of price series with given probabilistic characteristics,we consider problem of statistical analysis of share price and proposeprobabilistic characteristic to describe the price series . we discuss three method of mathematical modelling of price series with give probabilistic characteristic,31,0.833333333
8375,Computers and IT,"We study the problem of reconstructing a source function in the kinetic coagulation-fragmentation equation. The study is based on optimal control methods, the solvability theory of operator equations, and the use of iteration algorithms","we study the problem of reconstruct a source function in the kinetic coagulation-fragmentation equation . the study be base on optimal control method , the solvability theory of operator equation , and the use of iteration algorithm",34,0.833333333
8376,Computers and IT,"In this paper, we explore the feasibility of universal parametrization in generating B-spline surfaces, which was proposed recently in the literature (Lim, 1999). We present an interesting property of the new parametrization that it guarantees Go continuity on B-spline surfaces when several independently constructed patches are put together without imposing any constraints. Also, a simple blending method of patchwork is proposed to construct C/sup n-1/ surfaces, where overlapping control nets are utilized. It takes into account the semi-localness property of universal parametrization. It effectively helps us construct very natural looking B-spline surfaces while keeping the deviation from given data points very low. Experimental results are shown with several sets of surface data points","in this paper, we explore the feasibility of universal parametrization ingenerating b-spline surfaces, which was proposed recently in the literature (lim, 1999). we present an interesting property of the new parametrization that it guarantees go continuity on b-spline surfaces when several independently constructed patches are put together without imposing any constraints. also, a simple blending method of patchwork is proposed to construct c/sup n-1/ surfaces, where overlapping control nets are utilized. it takes into account the semi-localness property of universal parametrization. it effectively helps us construct very natural looking b-spline surfaces while keeping the deviation from given data points very low. experimental results are shown with several sets of surface data points",112,0.444444444
8377,Computers and IT,"We elucidate the connection between Bezier curves in polar coordinates, also called p-Bezier or focal Bezier curves, and certain families of spirals and sectrix curves. p-Bezier curves are the analogue in polar coordinates of nonparametric Bezier curves in Cartesian coordinates. Such curves form a subset of rational Bezier curves characterized by control points on radial directions regularly spaced with respect to the polar angle, and weights equal to the inverse of the polar radius. We show that this subset encompasses several classical sectrix curves, which solve geometrically the problem of dividing an angle into equal spans, and also spirals defining the trajectories of particles in central fields. First, we identify as p-Bezier curves a family of sinusoidal spirals that includes Tschirnhausen's cubic. Second, the trisectrix of Maclaurin and their generalizations, called arachnidas. Finally, a special class of epi spirals that encompasses the trisectrix of Delanges","we elucidate the connection between bezi curve in polar coordinate , alsocalled p-bezier or focal bezier curve , and certain family of spiral and sectrix curve . p-bezi curve be the analog in polar coordinate of nonparametric bezier curve in cartesian coordinate . such curve form a subset of rational bezier curve characterize by control point on radial direction regularly space with respect to the polar angle , and weight equal to the inverse of the polar radius . we show that this subset encompass several classical sectrix curve , which solve geometrically the problem of divide an angle into equal span , and also spiral define the trajectory of particle in central field . first , we identify as p-bezi curve a family of sinusoidal spiral that include tschirnhausen 's cubic . second , the trisectrix of maclaurin and their generalization , call arachnida . finally , a special class of epi spiral that encompass the trisectrix of delange",144,0.842105263
8378,Computers and IT,"A planar algebraic curve C has an implicit equation and a tangential equation. The tangential equation defines a dual curve to C. Starting with a parametrization of C, we find a parametrization of the dual curve, and the tangential equation and implicit equation of C in a novel way. We also find equations whose roots are the parameter values of the cusps and inflection points of C. Methods include polar reciprocation and the theory of envelopes","a planar algebraic curve c have an implicit equation and a tangential equation . the tangential equation define a dual curve to c. start with a parametrization of c , we find a parametrization of the dual curve , and the tangential equation and implicit equation of c in a novel way . we also find equation whose root be the parameter value of the cusp and inflection point of c. method include polar reciprocation and the theory of envelope",76,0.8
8379,Computers and IT,"In this paper, we show that there exists a close dependence between the control polygon of a polynomial and the minimum of its blossom under symmetric linear constraints. We consider a given minimization problem P, for which a unique solution will be a point delta on the Bezier curve. For the minimization function f, two sufficient conditions exist that ensure the uniqueness of the solution, namely, the concavity of the control polygon of the polynomial and the characteristics of the Polya frequency-control polygon where the minimum coincides with a critical point of the polynomial. The use of the blossoming theory provides us with a useful geometrical interpretation of the minimization problem. In addition, this minimization approach leads us to a new method of discovering inequalities about the elementary symmetric polynomials","in this paper , we show that there exist a close dependence between the controlpolygon of a polynomial and the minimum of its blossom under symmetric linear constraint . we consider a give minimization problem p , for which a unique solution will be a point delta on the bezier curve . for the minimization function f , two sufficient condition exist that ensure the uniqueness of the solution , namely , the concavity of the control polygon of the polynomial and the characteristic of the polya frequency-control polygon where the minimum coincide with a critical point of the polynomial . the use of the blossom theory provide us with a useful geometrical interpretation of the minimization problem . in addition , this minimization approach lead us to a new method of discover inequality about the elementary symmetric polynomial",129,0.818181818
8380,Computers and IT,"The matrix forms for curves and surfaces were largely promoted in CAD/CAM. In this paper we have presented two matrix representation formulations for arbitrary degree NURBS curves and surfaces explicitly other than recursively. The two approaches are derived from the computation of divided difference and the Marsden identity respectively. The explicit coefficient matrix of B-spline with equally spaced knot and Bezier curves and surfaces can be obtained by these formulae. The coefficient formulae and the coefficient matrix formulae developed in this paper express non-uniform B-spline functions of arbitrary degree in explicit polynomial and matrix forms. . They are useful for the evaluation and the conversion of NURBS curves and surfaces, in CAD/CAM systems","the matrix form for curve and surface be largely promote in cad/cam . inthis paper we have present two matrix representation formulation for arbitrary degree nurb curve and surface explicitly other than recursively . the two approach be derive from the computation of divide difference and the marsden identity respectively . the explicit coefficient matrix of b-spline with equally spaced knot and bezi curve and surface can be obtain by these formula . the coefficient formula and the coefficient matrix formula develop in this paper express non-uniform b-spline function of arbitrary degree in explicit polynomial and matrix form . . they be useful for the evaluation and the conversion of nurb curve and surface , in cad/cam system",111,0.647058824
8381,Computers and IT,"The structural invariance of the four-polynomial characterization for three-dimensional Pythagorean hodographs introduced by Dietz et al. (1993), under arbitrary spatial rotations, is demonstrated. The proof relies on a factored-quaternion representation for Pythagorean hodographs in three-dimensional Euclidean space-a particular instance of the ""PH representation map"" proposed by Choi et al. (2002)-and the unit quaternion description of spatial rotations. This approach furnishes a remarkably simple derivation for the polynomials u(t), upsilon (t), p(t), q(t) that specify the canonical form of a rotated Pythagorean hodograph, in terms of the original polynomials u(t), upsilon (t), p(t), q(t) and the angle theta and axis n of the spatial rotation. The preservation of the canonical form of PH space curves under arbitrary spatial rotations is essential to their incorporation into computer-aided design and manufacturing applications, such as the contour machining of free-form surfaces using a ball-end mill and realtime PH curve CNC interpolators","the structural invariance of the four-polynomial characterization forthree-dimensional pythagorean hodographs introduced by dietz et al. (1993), under arbitrary spatial rotations, is demonstrated. the proof relies on a factored-quaternion representation for pythagorean hodographs in three-dimensional euclidean space-a particular instance of the ""ph representation map"" proposed by choi et al. (2002)-and the unit quaternion description of spatial rotations. this approach furnishes a remarkably simple derivation for the polynomials u(t), upsilon (t), p(t), q(t) that specify the canonical form of a rotated pythagorean hodograph, in terms of the original polynomials u(t), upsilon (t), p(t), q(t) and the angle theta and axis n of the spatial rotation. the preservation of the canonical form of ph space curves under arbitrary spatial rotations is essential to their incorporation into computer-aided design and manufacturing applications, such as the contour machining of free-form surfaces using a ball-end mill and realtime ph curve cnc interpolators",146,0.533333333
8382,Computers and IT,"In this paper, we present the logic ATCTL, which is intended to be used for model checking models that have been specified in a lightweight version of the Unified Modelling Language (UML). Elsewhere, we have defined a formal semantics for LUML to describe the models. This paper's goal is to give a specification language for properties that fits LUML; LUML includes states, actions and real time. ATCTL extends CTL with concurrent actions and real time. It is based on earlier extensions of CTL by R. De Nicola and F. Vaandrager (ACTL) (1990) and R. Alur et aL (TCTL) (1993). This makes it easier to adapt existing model checkers to ATCTL. To show that we can check properties specified in ATCTL in models specified in LUML, we give a small example using the Kronos model checker","in this paper, we present the logic atctl, which is intended to be used formodel checking models that have been specified in a lightweight version of the unified modelling language (uml). elsewhere, we have defined a formal semantics for luml to describe the models. this paper's goal is to give a specification language for properties that fits luml; luml includes states, actions and real time. atctl extends ctl with concurrent actions and real time. it is based on earlier extensions of ctl by r. de nicola and f. vaandrager (actl) (1990) and r. alur et al (tctl) (1993). this makes it easier to adapt existing model checkers to atctl. to show that we can check properties specified in atctl in models specified in luml, we give a small example using the kronos model checker",134,0.666666667
8383,Computers and IT,"This paper presents a new kind of uniform splines, called hyperbolic polynomial B-splines, generated over the space Omega =span{sinh t, cosh t, t/sup k-3/, t/sup k-3/, t/sup k-4/, . . . , t 1} in which k is an arbitrary integer larger than or equal to 3. Hyperbolic polynomial B-splines share most of the properties of B-splines in polynomial space. We give subdivision formulae for this new kind of curve and then prove that they have variation diminishing properties and the control polygons of the subdivisions converge. Hyperbolic polynomial B-splines can handle freeform curves as well as remarkable curves such as the hyperbola and the catenary. The generation of tensor product surfaces using these new splines is straightforward. Examples of such tensor product surfaces: the saddle surface, the catenary cylinder, and a certain kind of ruled surface are given","this paper present a new kind of uniform spline , call hyperbolic polynomialb-spline , generate over the space omega = span -LCB- sinh t , cosh t , t/sup k-3 / , t/sup k-3 / , t/sup k-4 / , ... , t 1 -RCB- in which k be an arbitrary integer large than or equal to 3 . hyperbolic polynomial b-splines share most of the property of b-spline in polynomial space . we give subdivision formula for this new kind of curve and then prove that they have variation diminish property and the control polygon of the subdivision converge . hyperbolic polynomial b-spline can handle freeform curve as well as remarkable curve such as the hyperbola and the catenary . the generation of tensor product surface use these new spline be straightforward . example of such tensor product surface : the saddle surface , the catenary cylinder , and a certain kind of rule surface be give",135,0.833333333
8384,Computers and IT,"Given a Bezier curve of degree n, the problem of optimal multi-degree reduction (degree reduction of more than one degree) by a Bezier curve of degree m (mor=0) orders can be preserved at two endpoints respectively. The method in the paper performs multi-degree reduction at one time and does not need stepwise computing. When applied to multi-degree reduction with endpoint continuity of any order, the MDR by L/sub 2/ obtains the best least squares approximation. Comparison with another method of multi-degree reduction (MDR by L/sub infinity /), which achieves the nearly best uniform approximation with respect to L/sub infinity / norm, is also given. The approximate effect of the MDR by L/sub 2/ is better than that of the MDR by L/sub infinity /. Explicit approximate error analysis of the multi-degree reduction methods is presented","given a bezier curve of degree n, the problem of optimal multi-degree reduction (degree reduction of more than one degree) by a bezier curve of degree m (mor=0) orders can be preserved at two endpoints respectively. the method in the paper performs multi-degree reduction at one time and does not need stepwise computing. when applied to multi-degree reduction with endpoint continuity of any order, the mdr by l/sub 2/ obtains the best least squares approximation. comparison with another method of multi-degree reduction (mdr by l/sub infinity /), which achieves the nearly best uniform approximation with respect to l/sub infinity / norm, is also given. the approximate effect of the mdr by l/sub 2/ is better than that of the mdr by l/sub infinity /. explicit approximate error analysis of the multi-degree reduction methods is presented",135,0.333333333
8385,Computers and IT,"Web services follow the trusting model of the Internet, but allow ever more powerful payloads to travel between businesses and consumers. Before you leap online, the author advises to scan the security concerns and the available fixes. He looks at how we define and store Web services and incorporate them into business processes","web service follow the trust model of the internet , but allow ever morepowerful payload to travel between business and consumer . before you leap online , the author advise to scan the security concern and the available fix . he look at how we define and store web service and incorporate them into business process",52,0.8
8386,Computers and IT,"A precise definition of specialization and inheritance promises to be as useful in organizational process modeling as it is in object modeling. It would help us better understand, maintain, reuse, and generate process models. However, even though object-oriented analysis and design methodologies take full advantage of the object specialization hierarchy, the process specialization hierarchy is not supported in major process representations, such as the state diagram, data flow diagram, and UML representations. Partly underlying this lack of support is an implicit assumption that we can always specialize a process by treating it as ""just another object. "" We argue in this paper that this is not so straightforward as it might seem; we argue that a process-specific approach must be developed. We propose such an approach in the form of a set of transformations which, when applied to a process description, always result in specialization. We illustrate this approach by applying it to the state diagram representation and demonstrate that this approach to process specialization is not only theoretically possible, but shows promise as a method for categorizing and analyzing processes. We point out apparent inconsistencies between our notion of process specialization and existing work on object specialization but show that these inconsistencies are superficial and that the definition we provide is compatible with the traditional notion of specialization","a precise definition of specialization and inheritance promise to be as usefulin organizational process modeling as it be in object modeling . it would help us better understand , maintain , reuse , and generate process model . however , even though object-oriented analysis and design methodology take full advantage of the object specialization hierarchy , the process specialization hierarchy be not support in major process representation , such as the state diagram , data flow diagram , and uml representation . partly underlie this lack of support be an implicit assumption that we can always specialize a process by treat it as `` just another object . '' we argue in this paper that this be not so straightforward as it might seem ; we argue that a process-specific approach must be develop . we propose such an approach in the form of a set of transformation which , when apply to a process description , always result in specialization . we illustrate this approach by apply it to the state diagram representation and demonstrate that this approach to process specialization be not only theoretically possible , but show promise as a method for categorize and analyze process . we point out apparent inconsistency between our notion of process specialization and exist work on object specialization but show that these inconsistency be superficial and that the definition we provide be compatible with the traditional notion of specialization",217,0.875
8387,Computers and IT,"Within the resource-based view of the firm, a dynamic story has emerged in which the knowledge accumulated over the history of a firm and embedded in organizational routines and structures influences the firm's ability to recognize the value of new resources and capabilities. This paper explores the possibility of firms to select organizational designs that increase the likelihood that they will recognize and value rent-producing resources and capabilities. A computational model is developed to study the tension between an organization's desire to explore its environment for new capabilities and the organization's need to exploit existing capabilities. Support is provided for the proposition that integration, both externally and internally, is an important source of dynamic capability. The model provides greater insight into the tradeoffs between these two forms of integration and suggests when one form may be preferred over another. In particular, evidence is provided that in uncertain environments, the ability to explore possible alternatives is critical while in more certain environments, the ability to transfer information internally is paramount","within the resource-based view of the firm , a dynamic story have emerge in which the knowledge accumulate over the history of a firm and embed in organizational routine and structure influence the firm 's ability to recognize the value of new resource and capability . this paper explore the possibility of firm to select organizational design that increase the likelihood that they will recognize and value rent-producing resource and capability . a computational model be develop to study the tension between an organization 's desire to explore its environment for new capability and the organization 's need to exploit exist capability . support be provide for the proposition that integration , both externally and internally , be an important source of dynamic capability . the model provide great insight into the tradeoff between these two form of integration and suggest when one form may be prefer over another . in particular , evidence be provide that in uncertain environment , the ability to explore possible alternative be critical while in more certain environment , the ability to transfer information internally be paramount",169,0.4
8388,Computers and IT,"This paper applies the two-party dependence theory (Castelfranchi, Cesta and Miceli, 1992, in Y. Demazeau and E. Werner (Eds. ) Decentralized AI-3, Elsevier, North Holland) to modelling multiagent and group dependence. These have theoretical potentialities for the study of emerging groups and collective structures, and more generally for understanding social and organisational complexity, and practical utility for both social-organisational and agent systems purposes. In the paper, the dependence theory is extended to describe multiagent links, with a special reference to group and collective phenomena, and is proposed as a framework for the study of emerging social structures, such as groups and collectives. In order to do so, we propose to extend the notion of dependence networks (applied to a single agent) to dependence graphs (applied to an agency). In its present version, the dependence theory is argued to provide (a) a theoretical instrument for the study of social complexity, and (b) a computational system for managing the negotiation process in competitive contexts and for monitoring complexity in organisational and other cooperative contexts","this paper applies the two-party dependence theory (castelfranchi, cesta andmiceli, 1992, in y. demazeau and e. werner (eds.) decentralized ai-3, elsevier, north holland) to modelling multiagent and group dependence. these have theoretical potentialities for the study of emerging groups and collective structures, and more generally for understanding social and organisational complexity, and practical utility for both social-organisational and agent systems purposes. in the paper, the dependence theory is extended to describe multiagent links, with a special reference to group and collective phenomena, and is proposed as a framework for the study of emerging social structures, such as groups and collectives. in order to do so, we propose to extend the notion of dependence networks (applied to a single agent) to dependence graphs (applied to an agency). in its present version, the dependence theory is argued to provide (a) a theoretical instrument for the study of social complexity, and (b) a computational system for managing the negotiation process in competitive contexts and for monitoring complexity in organisational and other cooperative contexts",170,0.727272727
8389,Computers and IT,"A combined discrete/continuous simulation methodology based on the DEVS (discrete event system specification) formalism is presented in this paper that satisfies the simulation requirements of 3-dimensional and dynamic systems with multi-components. We propose a geometric and kinematic DEVS (GK-DEVS) formalism that is able to describe the geometric and kinematic structure of a system and its continuous state dynamics as well as the interaction among the multi-components. To establish one model having dynamic behavior and a particular hierarchical structure, the atomic and the coupled model of the conventional DEVS are merged into one model in the proposed formalism. For simulation of the continuous motion of 3-D components, the sequential state set is partitioned into the discrete and the continuous state set and the rate of change function over the continuous state set is employed. Although modified from the conventional DEVS formalism, the GK-DEVS formalism preserves a hierarchical, modular modeling fashion and a coupling scheme. Furthermore, for the GK-DEVS model simulation, we propose an abstract simulation algorithm, called a GK-Simulator, in which data and control are separated and events are scheduled not globally but hierarchically so that an object-oriented principle is satisfied. The proposed GK-DEVS formalism and the GK-Simulator algorithm have been applied to the simulation of a flexible manufacturing system consisting of a 2-axis lathe, a 3-axis milling machine, and a vehicle-mounted robot","a combined discrete/continuous simulation methodology based on the devs (discrete event system specification) formalism is presented in this paper that satisfies the simulation requirements of 3-dimensional and dynamic systems with multi-components. we propose a geometric and kinematic devs (gk-devs) formalism that is able to describe the geometric and kinematic structure of a system and its continuous state dynamics as well as the interaction among the multi-components. to establish one model having dynamic behavior and a particular hierarchical structure, the atomic and the coupled model of the conventional devs are merged into one model in the proposed formalism. for simulation of the continuous motion of 3-d components, the sequential state set is partitioned into the discrete and the continuous state set and the rate of change function over the continuous state set is employed. although modified from the conventional devs formalism, the gk-devs formalism preserves a hierarchical, modular modeling fashion and a coupling scheme. furthermore, for the gk-devs model simulation, we propose an abstract simulation algorithm, called a gk-simulator, in which data and control are separated and events are scheduled not globally but hierarchically so that an object-oriented principle is satisfied. the proposed gk-devs formalism and the gk-simulator algorithm have been applied to the simulation of a flexible manufacturing system consisting of a 2-axis lathe, a 3-axis milling machine, and a vehicle-mounted robot",222,0.777777778
8390,Computers and IT,"This paper describes an original approach to discrete event control of continuous processes by means of expert knowledge. We present an application of this approach on the SACHEM diagnosis subsystem. The SACHEM system is a large-scale knowledge-based system that aims in helping a set of operators to control the dynamics of complex continuous systems (e. g. , blast furnaces). The proposed method is based on: (i) The definition of a language facilitating the acquisition and representation of expert knowledge, called ELP (Expert Language Process); (ii) The use of the DEVS formalism to make ELP models operational; (iii) Algorithms for exploiting operational models","this paper describes an original approach to discrete event control ofcontinuous processes by means of expert knowledge. we present an application of this approach on the sachem diagnosis subsystem. the sachem system is a large-scale knowledge-based system that aims in helping a set of operators to control the dynamics of complex continuous systems (e.g., blast furnaces). the proposed method is based on: (i) the definition of a language facilitating the acquisition and representation of expert knowledge, called elp (expert language process); (ii) the use of the devs formalism to make elp models operational; (iii) algorithms for exploiting operational models",99,0.8
8391,Computers and IT,"An intrusion detection system (IDS) attempts to identify unauthorized use, misuse, and abuse of computer and network systems. As intrusions become more sophisticated, dealing with them moves beyond the scope of one IDS. The need arises for systems to cooperate with one another, to manage diverse attacks across networks. The feature of recent attacks is that the packet delivery is moderately slow, and the attack sources and attack targets are distributed. These attacks are called ""stealthy attacks. "" To detect these attacks, the deployment of distributed IDSs is needed. In such an environment, the ability of an IDS to share advanced information about these attacks is especially important. In this research, the IDS model exploits blacklist facts to detect the attacks that are based on either slow or highly distributed packets. To maintain the valid blacklist facts in the knowledge base of each IDS, the model should communicate with the other IDSs. When attack level goes beyond the interaction threshold, ID agents send interaction messages to ID agents in other hosts. Each agent model is developed as an interruptible atomic-expert model in which the expert system is embedded as a model component","an intrusion detection system (ids) attempts to identify unauthorized use,misuse, and abuse of computer and network systems. as intrusions become more sophisticated, dealing with them moves beyond the scope of one ids. the need arises for systems to cooperate with one another, to manage diverse attacks across networks. the feature of recent attacks is that the packet delivery is moderately slow, and the attack sources and attack targets are distributed. these attacks are called ""stealthy attacks."" to detect these attacks, the deployment of distributed idss is needed. in such an environment, the ability of an ids to share advanced information about these attacks is especially important. in this research, the ids model exploits blacklist facts to detect the attacks that are based on either slow or highly distributed packets. to maintain the valid blacklist facts in the knowledge base of each ids, the model should communicate with the other idss. when attack level goes beyond the interaction threshold, id agents send interaction messages to id agents in other hosts. each agent model is developed as an interruptible atomic-expert model in which the expert system is embedded as a model component",190,0.571428571
8392,Computers and IT,"A new class of dynamical systems, Quantized State Systems or QSS, is introduced in this paper. QSS are continuous time systems where the input trajectories are piecewise constant functions and the state variable trajectories - being themselves piecewise linear functions - are converted into piecewise constant functions via a quantization function equipped with hysteresis. It is shown that QSS can be exactly represented and simulated by a discrete event model, within the framework of the DEVS-approach. Further, it is shown that QSS can be used to approximate continuous systems, thus allowing their discrete-event simulation in opposition to the classical discrete-time simulation. It is also shown that in an approximating QSS, some stability properties of the original system are conserved and the solutions of the QSS go to the solutions of the original system when the quantization goes to zero","a new class of dynamical system , quantize state system or qs , be introducedin this paper . qs be continuous time system where the input trajectory be piecewise constant function and the state variable trajectory - be themselves piecewise linear function - be convert into piecewise constant function via a quantization function equip with hysteresis . it be show that qs can be exactly represent and simulated by a discrete event model , within the framework of the devs-approach . far , it be show that qs can be use to approximate continuous system , thus allow their discrete-event simulation in opposition to the classical discrete-time simulation . it be also show that in an approximating qs , some stability property of the original system be conserve and the solution of the qs go to the solution of the original system when the quantization go to zero",138,0.833333333
8393,Computers and IT,"We provide an extension of the language of linear tense logic with future and past connectives F and P, respectively, by a modality that quantifies over the points of some set which is assumed to increase in the course of time. In this way we obtain a general framework for modelling growth qualitatively. We develop an appropriate logical system, prove a corresponding completeness and decidability result and discuss the various kinds of flow of time in the new context. We also consider decreasing sets briefly","we provide an extension of the language of linear tense logic with future andpast connective f and p , respectively , by a modality that quantify over the point of some set which be assume to increase in the course of time . in this way we obtain a general framework for model growth qualitatively . we develop an appropriate logical system , prove a corresponding completeness and decidability result and discuss the various kind of flow of time in the new context . we also consider decrease set briefly",84,0.714285714
8394,Computers and IT,"Supersaturated designs are factorial designs in which the number of main effects is greater than the number of experimental runs. In this paper, a discrete discrepancy is proposed as a measure of uniformity for supersaturated designs, and a lower bound of this discrepancy is obtained as, a benchmark of design uniformity. A construction method for uniform supersaturated designs via resolvable balanced incomplete block designs is also presented along with the investigation of properties of the resulting designs. The construction method shows a strong link between these two different kinds of designs","supersaturated design be factorial design in which the number of maineffect be great than the number of experimental run . in this paper , a discrete discrepancy be propose as a measure of uniformity for supersaturated design , and a low bind of this discrepancy be obtain as , a benchmark of design uniformity . a construction method for uniform supersaturated design via resolvable balanced incomplete block design be also present along with the investigation of property of the result design . the construction method show a strong link between these two different kind of design",89,1
8395,Computers and IT,"In this paper, a definition of the optimization of operator equations in the average case setting is given. And the general result about the relevant optimization problem is obtained. This result is applied to the optimization of approximate solution of some classes of integral equations","in this paper , a definition of the optimization of operator equation in the average case setting be give . and the general result about the relevant optimization problem be obtain . this result be apply to the optimization of approximate solution of some class of integral equation",45,0.666666667
8396,Computers and IT,"In this paper we use Dedekind zeta functions of two real quadratic number fields at -1 to denote Dedekind sums of high rank. Our formula is different from that of Siegel's (1969). As an application, we get a polynomial representation of zeta /sub K/(-1) = zeta /sub K/(-1) = 1/45(26n/sup 3/ - 41n +or- 9), n identical to +or-2(mod 5), where K = Q( square root (5q)), prime q = 4n/sup 2/ + 1, and the class number of quadratic number field K/sub 2/ = Q( square root q) is 1","in this paper we use dedekind zeta functions of two real quadratic numberfields at -1 to denote dedekind sums of high rank. our formula is different from that of siegel's (1969). as an application, we get a polynomial representation of zeta /sub k/(-1) = zeta /sub k/(-1) = 1/45(26n/sup 3/ - 41n +or- 9), n identical to +or-2(mod 5), where k = q( square root (5q)), prime q = 4n/sup 2/ + 1, and the class number of quadratic number field k/sub 2/ = q( square root q) is 1",90,0.5
8397,Computers and IT,"For Part 1 see ibid. vol. 124 (2002). This is the second part of a two-part paper presenting a new methodology for analytically simulating multi-axis machining of complex sculptured surfaces. The first section of this paper offers a detailed explanation of the model calibration procedure. A new methodology is presented for accurately determining the cutting force coefficients for multi-axis machining. The force model presented in Part 1 is reformulated so that the cutting force coefficients account for the effects of feed rate, cutting speed, and a complex cutting edge design. Experimental results are presented for the calibration procedure. Model verification tests were conducted with these cutting force coefficients. These tests demonstrate that the predicted forces are within 5% of experimentally measured forces. Simulated results are also shown for predicting dynamic cutting forces and static/dynamic tool deflection. The second section of the paper discusses how the modeling methodology can be applied for feed rate scheduling in an industrial application. A case study for process optimization of machining an airfoil-like surface is used for demonstration. Based on the predicted instantaneous chip load and/or a specified force constraint, the feed rate scheduling is utilized to increase the metal removal rate. The feed rate scheduling implementation results in a 30% reduction in machining time for the airfoil-like surface","for part 1 see ibid. vol.124 (2002). this is the second part of a two-part paper presenting a new methodology for analytically simulating multi-axis machining of complex sculptured surfaces. the first section of this paper offers a detailed explanation of the model calibration procedure. a new methodology is presented for accurately determining the cutting force coefficients for multi-axis machining. the force model presented in part 1 is reformulated so that the cutting force coefficients account for the effects of feed rate, cutting speed, and a complex cutting edge design. experimental results are presented for the calibration procedure. model verification tests were conducted with these cutting force coefficients. these tests demonstrate that the predicted forces are within 5% of experimentally measured forces. simulated results are also shown for predicting dynamic cutting forces and static/dynamic tool deflection. the second section of the paper discusses how the modeling methodology can be applied for feed rate scheduling in an industrial application. a case study for process optimization of machining an airfoil-like surface is used for demonstration. based on the predicted instantaneous chip load and/or a specified force constraint, the feed rate scheduling is utilized to increase the metal removal rate. the feed rate scheduling implementation results in a 30% reduction in machining time for the airfoil-like surface",213,0.6
8398,Computers and IT,"This paper presents a new methodology for analytically simulating multi-axis machining of complex sculptured surfaces. A generalized approach is developed for representing an arbitrary cutting edge design, and the local surface topology of a complex sculptured surface. A NURBS curve is used to represent the cutting edge profile. This approach offers the advantages of representing any arbitrary cutting edge design in a generic way, as well as providing standardized techniques for manipulating the location and orientation of the cutting edge. The local surface topology of the part is defined as those surfaces generated by previous tool paths in the vicinity of the current tool position. The local surface topology of the part is represented without using a computationally expensive CAD system. A systematic prediction technique is then developed to determine the instantaneous tool/part interaction during machining. The methodology employed here determines the cutting edge in-cut segments by determining the intersection between the NURBS curve representation of the cutting edge and the defined local surface topology. These in-cut segments are then utilized for predicting instantaneous chip load, static and dynamic cutting forces, and tool deflection. Part 1 of this paper details the modeling methodology and demonstrates the capabilities of the simulation for machining a complex surface","this paper present a new methodology for analytically simulate multi-axis machining of complex sculptured surface . a generalized approach be develop for represent an arbitrary cutting edge design , and the local surface topology of a complex sculptured surface . a nurb curve be use to represent the cut edge profile . this approach offer the advantage of represent any arbitrary cutting edge design in a generic way , as well as provide standardized technique for manipulate the location and orientation of the cut edge . the local surface topology of the part be define as those surface generate by previous tool path in the vicinity of the current tool position . the local surface topology of the part be represent without use a computationally expensive cad system . a systematic prediction technique be then develop to determine the instantaneous tool/part interaction during machining . the methodology employ here determine the cut edge in-cut segment by determine the intersection between the nurb curve representation of the cut edge and the define local surface topology . these in-cut segment be then utilize for predict instantaneous chip load , static and dynamic cutting force , and tool deflection . part 1 of this paper detail the modeling methodology and demonstrate the capability of the simulation for machining a complex surface",205,0.555555556
8399,Computers and IT,"Over the last few years, the semiconductor industry has put much emphasis on ways to improve the accuracy of thermal mass flow controllers (TMFCs). Although issues involving TMFC mounting orientation and pressure effects have received much attention, little has been done to address the effect of changes in ambient temperature or process gas temperature. Scientists and engineers at Qualiflow have succeeded to solve the problem using a temperature correction algorithm for digital TMFCs. Using an in situ environmental temperature compensation technique, we calculated correction factors for the temperature effect and obtained satisfactory results with both the traditional sensor and the new, improved thin-film sensors","over the last few years, the semiconductor industry has put much emphasis onways to improve the accuracy of thermal mass flow controllers (tmfcs). although issues involving tmfc mounting orientation and pressure effects have received much attention, little has been done to address the effect of changes in ambient temperature or process gas temperature. scientists and engineers at qualiflow have succeeded to solve the problem using a temperature correction algorithm for digital tmfcs. using an in situ environmental temperature compensation technique, we calculated correction factors for the temperature effect and obtained satisfactory results with both the traditional sensor and the new, improved thin-film sensors",103,0.6
8400,Computers and IT,"The current Web service model treats all requests equivalently, both while being processed by servers and while being transmitted over the network. For some uses, such as multiple priority schemes, different levels of service are desirable. We propose application-level TCP connection management mechanisms for Web servers to provide two different levels of Web service, high and low service, by setting different time-outs for inactive TCP connections. We evaluated the performance of the mechanism under heavy and light loading conditions on the Web server. Our experiments show that, though heavy traffic saturates the network, high level class performance is improved by as much as 25-28%. Therefore, this mechanism can effectively provide QoS guaranteed services even in the absence of operating system and network supports","the current web service model treat all request equivalently , both whilebe process by server and while be transmit over the network . for some us , such as multiple priority scheme , different level of service be desirable . we propose application-level tcp connection management mechanism for web server to provide two different level of web service , high and low service , by set different time-out for inactive tcp connection . we evaluate the performance of the mechanism under heavy and light load condition on the web server . our experiment show that , though heavy traffic saturate the network , high level class performance be improve by as much as 25-28 % . therefore , this mechanism can effectively provide qo guarantee service even in the absence of operate system and network support",122,0.444444444
8401,Computers and IT,"Constant advances in audio/video compression, the development of the multicast protocol as well as fast improvement in computing devices (e. g. higher speed, larger memory) have set forth the opportunity to have resource demanding videoconferencing (VC) sessions on the Internet. Multicast is supported by the multicast backbone (Mbone), which is a special portion of the Internet where this protocol is being deployed. Mbone VC tools are steadily emerging and the user population is growing fast. VC is a fascinating application that has the potential to greatly impact the way we remotely communicate and work. Yet, the adoption of VC is not as fast as one could have predicted. Hence, it is important to examine the factors that affect a widespread adoption of VC. This paper examines the enabling technology and the social issues. It discusses the achievements and identifies the future challenges. It suggests an integration of many emerging multimedia tools into VC in order to enhance its versatility for more effectiveness","constant advances in audio/video compression, the development of the multicastprotocol as well as fast improvement in computing devices (e.g. higher speed, larger memory) have set forth the opportunity to have resource demanding videoconferencing (vc) sessions on the internet. multicast is supported by the multicast backbone (mbone), which is a special portion of the internet where this protocol is being deployed. mbone vc tools are steadily emerging and the user population is growing fast. vc is a fascinating application that has the potential to greatly impact the way we remotely communicate and work. yet, the adoption of vc is not as fast as one could have predicted. hence, it is important to examine the factors that affect a widespread adoption of vc. this paper examines the enabling technology and the social issues. it discusses the achievements and identifies the future challenges. it suggests an integration of many emerging multimedia tools into vc in order to enhance its versatility for more effectiveness",160,0.75
8402,Computers and IT,"In broadband networks, such as ATM, the importance of dynamic migration of data resources is increasing because of its potential to improve performance especially for transaction processing. In environments with migratory data resources, it is necessary to have mechanisms to manage the locations of each data resource. In this paper, we present an algorithm that makes use of system state information and heuristics to manage locations of data resources in a distributed network. In the proposed algorithm, each site maintains information about state of other sites with respect to each data resource of the system and uses it to find: (1) a subset of sites likely to have the requested data resource; and (2) the site where the data resource is to be migrated from the current site. The proposed algorithm enhances its effectiveness by continuously updating system state information stored at each site. It focuses on reducing the overall average time delay needed by the transaction requests to locate and access the migratory data resources. We evaluated the performance of the proposed algorithm and also compared it with one of the existing location management algorithms, by simulation studies under several system parameters such as the frequency of requests generation, frequency of data resource migrations, network topology and scale of network. The experimental results show the effectiveness of the proposed algorithm in all cases","in broadband networks, such as atm, the importance of dynamic migration of dataresources is increasing because of its potential to improve performance especially for transaction processing. in environments with migratory data resources, it is necessary to have mechanisms to manage the locations of each data resource. in this paper, we present an algorithm that makes use of system state information and heuristics to manage locations of data resources in a distributed network. in the proposed algorithm, each site maintains information about state of other sites with respect to each data resource of the system and uses it to find: (1) a subset of sites likely to have the requested data resource; and (2) the site where the data resource is to be migrated from the current site. the proposed algorithm enhances its effectiveness by continuously updating system state information stored at each site. it focuses on reducing the overall average time delay needed by the transaction requests to locate and access the migratory data resources. we evaluated the performance of the proposed algorithm and also compared it with one of the existing location management algorithms, by simulation studies under several system parameters such as the frequency of requests generation, frequency of data resource migrations, network topology and scale of network. the experimental results show the effectiveness of the proposed algorithm in all cases",223,0.714285714
8403,Computers and IT,"In this note we give some comments on the recent results concerning a simultaneous method of the fourth-order for finding complex zeros in circular interval arithmetic. The main discussion is directed to a rediscovered iterative formula and its modification, presented recently in Sun and Kosmol, (2001). The presented comments include some critical parts of the papers Petkovic, Trickovic, Herceg, (1998) and Sun and Kosmol, (2001) which treat the same subject","in this note we give some comments on the recent results concerning a simultaneous method of the fourth-order for finding complex zeros in circular interval arithmetic. the main discussion is directed to a rediscovered iterative formula and its modification, presented recently in sun and kosmol, (2001). the presented comments include some critical parts of the papers petkovic, trickovic, herceg, (1998) and sun and kosmol, (2001) which treat the same subject",70,0.8
8404,Computers and IT,"We consider the relationship between omega -automata and a specific logical formulation based on a normal form for temporal logic formulae. While this normal form was developed for use with execution and clausal resolution in temporal logics, we show how it can represent, syntactically, omega -automata in a high-level way. Technical proofs of the correctness of this representation are given","we consider the relationship between omega - automaton and a specific logicalformulation base on a normal form for temporal logic formula . while this normal form be develop for use with execution and clausal resolution in temporal logic , we show how it can represent , syntactically , omega - automaton in a high-level way . technical proof of the correctness of this representation be give",59,0.4
8405,Computers and IT,"This paper studies adaptive thinning strategies for approximating a large set of scattered data by piecewise linear functions over triangulated subsets. Our strategies depend on both the locations of the data points in the plane, and the values of the sampled function at these points - adaptive thinning. All our thinning strategies remove data points one by one, so as to minimize an estimate of the error that results by the removal of a point from the current set of points (this estimate is termed ""anticipated error""). The thinning process generates subsets of ""most significant"" points, such that the piecewise linear interpolants over the Delaunay triangulations of these subsets approximate progressively the function values sampled at the original scattered points, and such that the approximation errors are small relative to the number of points in the subsets. We design various methods for computing the anticipated error at reasonable cost, and compare and test the performance of the methods. It is proved that for data sampled from a convex function, with the strategy of convex triangulation, the actual error is minimized by minimizing the best performing measure of anticipated error. It is also shown that for data sampled from certain quadratic polynomials, adaptive thinning is equivalent to thinning which depends only on the locations of the data points - nonadaptive thinning. Based on our numerical tests and comparisons, two practical adaptive thinning algorithms are proposed for thinning large data sets, one which is more accurate and another which is faster","this paper studies adaptive thinning strategies for approximating a large setof scattered data by piecewise linear functions over triangulated subsets. our strategies depend on both the locations of the data points in the plane, and the values of the sampled function at these points - adaptive thinning. all our thinning strategies remove data points one by one, so as to minimize an estimate of the error that results by the removal of a point from the current set of points (this estimate is termed ""anticipated error""). the thinning process generates subsets of ""most significant"" points, such that the piecewise linear interpolants over the delaunay triangulations of these subsets approximate progressively the function values sampled at the original scattered points, and such that the approximation errors are small relative to the number of points in the subsets. we design various methods for computing the anticipated error at reasonable cost, and compare and test the performance of the methods. it is proved that for data sampled from a convex function, with the strategy of convex triangulation, the actual error is minimized by minimizing the best performing measure of anticipated error. it is also shown that for data sampled from certain quadratic polynomials, adaptive thinning is equivalent to thinning which depends only on the locations of the data points - nonadaptive thinning. based on our numerical tests and comparisons, two practical adaptive thinning algorithms are proposed for thinning large data sets, one which is more accurate and another which is faster",248,0.714285714
8406,Computers and IT,"When faced with a plethora of applications to design, it's essential to have a versatile microcontroller in hand. The author describes the AT89C51/52 microcontrollers. To get you started, he'll describe his inexpensive microcontroller programmer","when face with a plethora of application to design , it ' essential to have aversatile microcontroller in hand . the author describe the at89c51/52 microcontroller . to get you start , he 'll describe his inexpensive microcontroller programmer",33,0.6
8407,Computers and IT,"Despite the rumors, the HCS II project is not dead. In fact, HCS has been licensed and is now an open-source project. In this article, the author brings us up to speed on the HCS II project's past, present, and future. The HCS II is an expandable, standalone, network-based (RS-485), intelligent-node, industrial-oriented supervisory control (SC) system intended for demanding home control applications. The HCS incorporates direct and remote digital inputs and outputs, direct and remote analog inputs and outputs, real time or Boolean decision event triggering, X10 transmission and reception, infrared remote control transmission and reception, remote LCDs, and a master console. Its program is compiled on a PC with the XPRESS compiler and then downloaded to the SC where it runs independently of the PC","despite the rumors, the hcs ii project is not dead. in fact, hcs has beenlicensed and is now an open-source project. in this article, the author brings us up to speed on the hcs ii project's past, present, and future. the hcs ii is an expandable, standalone, network-based (rs-485), intelligent-node, industrial-oriented supervisory control (sc) system intended for demanding home control applications. the hcs incorporates direct and remote digital inputs and outputs, direct and remote analog inputs and outputs, real time or boolean decision event triggering, x10 transmission and reception, infrared remote control transmission and reception, remote lcds, and a master console. its program is compiled on a pc with the xpress compiler and then downloaded to the sc where it runs independently of the pc",125,0.5
8408,Computers and IT,"The author looks at several projects that Cornell University students entered in the Atmel Design 2001 contest. Those covered include a vertical plotter; BiLines, an electronic game; a wireless Internet pager; Cooking Coach; Barbie's zip drive; and a model train controller","the author look at several project that cornell university student enteredin the atmel design 2001 contest . those cover include a vertical plotter ; biline , an electronic game ; a wireless internet pager ; cooking coach ; barbie 's zip drive ; and a model train controller",40,0.6
8409,Computers and IT,"Whether your message is one of workplace safety or world peace, the long nights of brooding over ways to tell the world are over. Part 1 described the basic interface to drive the Smartswitch. Part 2 adds the bells and whistles to allow both text and messages to be placed anywhere on the screen. It considers character generation, graphic generation and the user interface","whether your message be one of workplace safety or world peace , the long nightsof brooding over way to tell the world be over . part 1 describe the basic interface to drive the smartswitch . part 2 add the bell and whistle to allow both text and message to be place anywhere on the screen . it consider character generation , graphic generation and the user interface",63,0.714285714
8410,Computers and IT,We study the optimal control problem subject to the semilinear equation with a state constraint. We prove certain theorems and give examples of state constraints so that the maximum principle holds. The main difficulty of the problem is to make the sensitivity analysis of the state with respect to the control caused by the unboundedness and nonlinearity of an operator,we study the optimal control problem subject to the semilinear equation with a state constraint . we prove certain theorem and give example of state constraint so that the maximum principle hold . the main difficulty of the problem be to make the sensitivity analysis of the state with respect to the control cause by the unboundedness and nonlinearity of an operator,60,0.6
8411,Computers and IT,We present necessary conditions of optimality for optimal control problems arising in systems governed by impulsive evolution equations on Banach spaces. Basic notations and terminologies are first presented and necessary conditions of optimality are presented. Special cases are discussed and we present an application to the classical linear quadratic regulator problem,we present necessary condition of optimality for optimal control problemsaris in system govern by impulsive evolution equation on banach space . basic notation and terminology be first present and necessary condition of optimality be present . special case be discuss and we present an application to the classical linear quadratic regulator problem,50,0.857142857
8412,Computers and IT,"In this paper we establish two new projection-type methods for the solution of the monotone linear complementarity problem (LCP). The methods are a combination of the extragradient method and the Newton method, in which the active set strategy is used and only one linear system of equations with lower dimension is solved at each iteration. It is shown that under the assumption of monotonicity, these two methods are globally and linearly convergent. Furthermore, under a nondegeneracy condition they have a finite termination property. Finally, the methods are extended to solving the monotone affine variational inequality problem","in this paper we establish two new projection-type methods for the solution ofthe monotone linear complementarity problem (lcp). the methods are a combination of the extragradient method and the newton method, in which the active set strategy is used and only one linear system of equations with lower dimension is solved at each iteration. it is shown that under the assumption of monotonicity, these two methods are globally and linearly convergent. furthermore, under a nondegeneracy condition they have a finite termination property. finally, the methods are extended to solving the monotone affine variational inequality problem",95,0.733333333
8413,Computers and IT,"A new characteristic finite element scheme is presented for convection-diffusion problems. It is of second order accuracy in time increment, symmetric, and unconditionally stable. Optimal error estimates are proved in the framework of L/sup 2/-theory. Numerical results are presented for two examples, which show the advantage of the scheme","a new characteristic finite element scheme be present for convection-diffusion problem . it be of second order accuracy in time increment , symmetric , and unconditionally stable . optimal error estimate be prove in the framework of l/sup 2 / - theory . numerical result be present for two example , which show the advantage of the scheme",49,0.8
8414,Computers and IT,"This paper introduces and analyzes the convergence properties of a method that computes an approximation to the invariant subspace associated with a group of eigenvalues of a large not necessarily diagonalizable matrix. The method belongs to the family of projection type methods. At each step, it refines the approximate invariant subspace using a linearized Riccati's equation which turns out to be the block analogue of the correction used in the Jacobi-Davidson method. The analysis conducted in this paper shows that the method converges at a rate quasi-quadratic provided that the approximate invariant subspace is close to the exact one. The implementation of the method based on multigrid techniques is also discussed and numerical experiments are reported","this paper introduce and analyze the convergence property of a method that compute an approximation to the invariant subspace associate with a group of eigenvalue of a large not necessarily diagonalizable matrix . the method belong to the family of projection type method . at each step , it refine the approximate invariant subspace use a linearize riccati 's equation which turn out to be the block analog of the correction use in the jacobi-davidson method . the analysis conduct in this paper show that the method converge at a rate quasi-quadratic provide that the approximate invariant subspace be close to the exact one . the implementation of the method base on multigrid technique be also discuss and numerical experiment be report",116,0.75
8415,Computers and IT,"Current techniques for interactively proving temporal properties of concurrent systems translate transition systems into temporal formulas by introducing program counter variables. Proofs are not intuitive, because control flow is not explicitly considered. For sequential programs symbolic execution is a very intuitive, interactive proof strategy. In this paper we adopt this technique for parallel programs. Properties are formulated in interval temporal logic. An implementation in the interactive theorem prover KIV has shown that this technique offers a high degree of automation and allows simple, local invariants","current technique for interactively prove temporal property of concurrentsystem translate transition system into temporal formula by introduce program counter variable . proof be not intuitive , because control flow be not explicitly consider . for sequential program symbolic execution be a very intuitive , interactive proof strategy . in this paper we adopt this technique for parallel program . property be formulate in interval temporal logic . an implementation in the interactive theorem prover kiv have show that this technique offer a high degree of automation and allow simple , local invariant",84,0.818181818
8416,Computers and IT,"A. Bermudez and C. Moreno (1981) presented a duality numerical algorithm for solving variational inequalities of the second kind. The performance of this algorithm strongly depends on the choice of two constant parameters. Assuming a further hypothesis of the inf-sup type, we present here a convergence theorem that improves on the one presented by A. Bermudez and C. Moreno. We prove that the convergence is linear, and we give the expression of the asymptotic error constant and the explicit form of the optimal parameters, as a function of some constants related to the variational inequality. Finally, we present some numerical examples that confirm the theoretical results","a. bermudez and c. moreno (1981) presented a duality numerical algorithm forsolving variational inequalities of the second kind. the performance of this algorithm strongly depends on the choice of two constant parameters. assuming a further hypothesis of the inf-sup type, we present here a convergence theorem that improves on the one presented by a. bermudez and c. moreno. we prove that the convergence is linear, and we give the expression of the asymptotic error constant and the explicit form of the optimal parameters, as a function of some constants related to the variational inequality. finally, we present some numerical examples that confirm the theoretical results",105,0.857142857
8417,Computers and IT,"We consider an initial value problem for the second-order differential equation with a Dirichlet-to-Neumann operator coefficient. For the numerical solution we carry out semi-discretization by the Laguerre transformation with respect to the time variable. Then an infinite system of the stationary operator equations is obtained. By potential theory, the operator equations are reduced to boundary integral equations of the second kind with logarithmic or hypersingular kernels. The full discretization is realized by Nystrom's method which is based on the trigonometric quadrature rules. Numerical tests confirm the ability of the method to solve these types of nonstationary problems","we consider an initial value problem for the second-order differential equation with a dirichlet-to-neumann operator coefficient . for the numerical solution we carry out semi-discretization by the laguerre transformation with respect to the time variable . then an infinite system of the stationary operator equation be obtain . by potential theory , the operator equation be reduce to boundary integral equation of the second kind with logarithmic or hypersingular kernel . the full discretization be realize by nystrom 's method which be base on the trigonometric quadrature rule . numerical test confirm the ability of the method to solve these type of nonstationary problem",97,0.75
8418,Computers and IT,"We study stability of Runge-Kutta (RK) methods for delay integro-differential equations with a constant delay on the basis of the linear equation du/dt = Lu(t) + Mu(t- tau ) + K integral /sub t- tau //sup t/ u( theta )d theta , where L, M, K are constant complex matrices. In particular, we show that the same result as in the case K = 0 (Koto, 1994) holds for this test equation, i. e. , every A-stable RK method preserves the delay-independent stability of the exact solution whenever a step-size of the form h = tau /m is used, where m is a positive integer","we study stability of runge-kutta (rk) methods for delay integro-differentialequations with a constant delay on the basis of the linear equation du/dt = lu(t) + mu(t- tau ) + k integral /sub t- tau //sup t/ u( theta )d theta , where l, m, k are constant complex matrices. in particular, we show that the same result as in the case k = 0 (koto, 1994) holds for this test equation, i.e., every a-stable rk method preserves the delay-independent stability of the exact solution whenever a step-size of the form h = tau /m is used, where m is a positive integer",102,0.5
8419,Computers and IT,"For a two-dimensional heat conduction problem, we consider its initial boundary value problem and the related inverse problem of determining the initial temperature distribution from transient temperature measurements. The conditional stability for this inverse problem and the error analysis for the Tikhonov regularization are presented. An implicit inversion method, which is based on the regularization technique and the successive over-relaxation (SOR) iteration process, is established. Due to the explicit difference scheme for a direct heat problem developed in this paper, the inversion process is very efficient, while the application of SOR technique makes our inversion convergent rapidly. Numerical results illustrating our method are also given","for a two-dimensional heat conduction problem, we consider its initial boundary value problem and the related inverse problem of determining the initial temperature distribution from transient temperature measurements. the conditional stability for this inverse problem and the error analysis for the tikhonov regularization are presented. an implicit inversion method, which is based on the regularization technique and the successive over-relaxation (sor) iteration process, is established. due to the explicit difference scheme for a direct heat problem developed in this paper, the inversion process is very efficient, while the application of sor technique makes our inversion convergent rapidly. numerical results illustrating our method are also given",105,0.888888889
8420,Computers and IT,"For complex-valued n-dimensional vector functions t to s(t), supposed to be sufficiently smooth, the differentiability properties of the mapping t to ||s(t)||/sub p/ at every point t = t/sub 0/ epsilon R/sub 0//sup +/:= {t epsilon R | t or= 0} are investigated, where || . ||/sub p/ is the usual vector norm in C/sup n/ resp. R/sup n/, for p epsilon [1, o infinity ]. Moreover, formulae for the first three right derivatives D/sub +//sup k/||s(t)||/sub p/, k = 1, 2, 3 are determined. These formulae are applied to vibration problems by computing the best upper bounds on ||s(t)||/sub p/ in certain classes of bounds. These results cannot be obtained by the methods used so far. The systematic use of the differential calculus for vector norms, as done here for the first time, could lead to major advances also in other branches of mathematics and other sciences","for complex-valued n-dimensional vector functions t to s(t), supposed to be sufficiently smooth, the differentiability properties of the mapping t to ||s(t)||/sub p/ at every point t = t/sub 0/ epsilon r/sub 0//sup +/:= {t epsilon r | t >or= 0} are investigated, where || . ||/sub p/ is the usual vector norm in c/sup n/ resp. r/sup n/, for p epsilon [1, o infinity ]. moreover, formulae for the first three right derivatives d/sub +//sup k/||s(t)||/sub p/, k = 1, 2,3 are determined. these formulae are applied to vibration problems by computing the best upper bounds on ||s(t)||/sub p/ in certain classes of bounds. these results cannot be obtained by the methods used so far. the systematic use of the differential calculus for vector norms, as done here for the first time, could lead to major advances also in other branches of mathematics and other sciences",147,1
8421,Computers and IT,"We discuss the properties of a new family of multi-index Lucas type polynomials, which are often encountered in problems of intracavity photon statistics. We develop an approach based on the integral representation method and show that this class of polynomials can be derived from recently introduced multi-index Hermite like polynomials","we discuss the property of a new family of multi-index lucas type polynomial , which be often encounter in problem of intracavity photon statistic . we develop an approach base on the integral representation method and show that this class of polynomial can be derive from recently introduce multi-index hermite like polynomial",50,0.5
8422,Computers and IT,"In this paper the regularity of nine Pal-type interpolation problems is proved. In the literature interpolation on the zeros of the pair W/sub n//sup ( alpha )/(z) = (z + alpha )/sup n/ + (1 + alpha z)/sup n/, v/sub n//sup ( alpha )/(z) = (z + alpha )/sup n/ - (1 + alpha z)/sup n/ with 0 alpha 1 has been studied. Here the nodes form a subset of these sets of zeros","in this paper the regularity of nine pal-type interpolation problems is proved.in the literature interpolation on the zeros of the pair w/sub n//sup ( alpha )/(z) = (z + alpha )/sup n/ + (1 + alpha z)/sup n/, v/sub n//sup ( alpha )/(z) = (z + alpha )/sup n/ - (1 + alpha z)/sup n/ with 0 < alpha < 1 has been studied. here the nodes form a subset of these sets of zeros",75,1
8423,Computers and IT,"Let I be a finite or infinite interval, and let W:I to (0, infinity ). Assume that W/sup 2/ is a weight, so that we may define orthonormal polynomials corresponding to W/sup 2/. For f :R to R, let s/sub m/ [f] denote the mth partial sum of the orthonormal expansion of f with respect to these polynomials. We investigate boundedness in weighted L/sub p/ spaces of the (C, 1) means 1/n /sub m=1/ Sigma /sup n/s/sub m/[f]. The class of weights W/sup 2/ considered includes even and noneven exponential weights","let i be a finite or infinite interval, and let w:i to (0, infinity ). assume that w/sup 2/ is a weight, so that we may define orthonormal polynomials corresponding to w/sup 2/. for f :r to r, let s/sub m/ [f] denote the mth partial sum of the orthonormal expansion of f with respect to these polynomials. we investigate boundedness in weighted l/sub p/ spaces of the (c, 1) means 1/n /sub m=1/ sigma /sup n/s/sub m/[f]. the class of weights w/sup 2/ considered includes even and noneven exponential weights",91,0.857142857
8424,Computers and IT,"It is well known that the Jacobi polynomials P/sub n//sup ( alpha , beta )/(x) are orthogonal with respect to a quasi-definite linear functional whenever alpha , beta , and alpha + beta + 1 are not negative integer numbers. Recently, Sobolev orthogonality for these polynomials has been obtained for alpha a negative integer and beta not a negative integer and also for the case alpha = beta negative integer numbers. In this paper, we give a Sobolev orthogonality for the Jacobi polynomials in the remainder cases","it is well known that the jacobi polynomials p/sub n//sup ( alpha , beta )/(x)are orthogonal with respect to a quasi-definite linear functional whenever alpha , beta , and alpha + beta + 1 are not negative integer numbers. recently, sobolev orthogonality for these polynomials has been obtained for alpha a negative integer and beta not a negative integer and also for the case alpha = beta negative integer numbers. in this paper, we give a sobolev orthogonality for the jacobi polynomials in the remainder cases",86,0.8
8425,Computers and IT,"A. D. Gunawardena et al. (1991) have reported the modified Gauss-Seidel method with a preconditioner (I + S). In this article, we propose to use a preconditioner (I + S/sub max/) instead of (I + S). Here, S/sub max/ is constructed by only the largest element at each row of the upper triangular part of A. By using the lemma established by M. Neumann and R. J. Plemmons (1987), we get the comparison theorem for the proposed method. Simple numerical examples are also given","a.d. gunawardena et al. (1991) have reported the modified gauss-seidel method with a preconditioner (i + s). in this article, we propose to use a preconditioner (i + s/sub max/) instead of (i + s). here, s/sub max/ is constructed by only the largest element at each row of the upper triangular part of a. by using the lemma established by m. neumann and r.j. plemmons (1987), we get the comparison theorem for the proposed method. simple numerical examples are also given",82,0.5
8426,Computers and IT,"In this paper the theory of fuzzy logic and fuzzy reasoning is combined with the theory of Markov systems and the concept of a fuzzy non-homogeneous Markov system is introduced for the first time. This is an effort to deal with the uncertainty introduced in the estimation of the transition probabilities and the input probabilities in Markov systems. The asymptotic behaviour of the fuzzy Markov system and its asymptotic variability is considered and given in closed analytic form. Moreover, the asymptotically attainable structures of the system are estimated also in a closed analytic form under some realistic assumptions. The importance of this result lies in the fact that in most cases the traditional methods for estimating the probabilities can not be used due to lack of data and measurement errors. The introduction of fuzzy logic into Markov systems represents a powerful tool for taking advantage of the symbolic knowledge that the experts of the systems possess","in this paper the theory of fuzzy logic and fuzzy reasoning be combine withthe theory of markov system and the concept of a fuzzy non-homogeneous markov system be introduce for the first time . this be an effort to deal with the uncertainty introduce in the estimation of the transition probability and the input probability in markov system . the asymptotic behavior of the fuzzy markov system and its asymptotic variability be consider and give in closed analytic form . moreover , the asymptotically attainable structure of the system be estimate also in a closed analytic form under some realistic assumption . the importance of this result lie in the fact that in most case the traditional method for estimate the probability can not be use due to lack of data and measurement error . the introduction of fuzzy logic into markov system represent a powerful tool for take advantage of the symbolic knowledge that the expert of the system possess",155,0.8
8427,Computers and IT,"It is shown that two solvable chaotic systems, the arithmetic-harmonic mean (ARM) algorithm and the Ulam-von Neumann (UvN) map, have determinantal solutions. An additional formula for certain determinants and Riccati difference equations play a key role in both cases. Two infinite hierarchies of solvable chaotic systems are presented which have determinantal solutions","it is shown that two solvable chaotic systems, the arithmetic-harmonic mean(arm) algorithm and the ulam-von neumann (uvn) map, have determinantal solutions. an additional formula for certain determinants and riccati difference equations play a key role in both cases. two infinite hierarchies of solvable chaotic systems are presented which have determinantal solutions",51,0.571428571
8428,Computers and IT,"Let an analytic or a piecewise analytic function on a compact interval be given. We present algorithms that produce enclosures for the integral or the function itself. Under certain conditions on the representation of the function, this is done with the minimal order of numbers of operations. The integration algorithm is implemented and numerical comparisons to non-validating integration software are presented","let an analytic or a piecewise analytic function on a compact interval begiven . we present algorithm that produce enclosure for the integral or the function itself . under certain condition on the representation of the function , this be do with the minimal order of number of operation . the integration algorithm be implement and numerical comparison to non-validating integration software be present",60,0.625
8429,Computers and IT,"The two-dimensional integral equation 1/ pi integral integral /sub D/( phi (r, theta )/R/sup 2/)dS=f(r/sub 0/, theta /sub 0/) defined on a circular disk D: r/sub 0/or=a, 0or= theta /sub 0/or=2 pi , is considered in the present paper. Here R in the kernel denotes the distance between two points P(r, theta ) and P/sub 0/(r/sub 0/, theta /sub 0/) in D, and 0 alpha 2 or 2 alpha 4. Based on some known results of Bessel functions, integral representations of the kernel are established for 0 alpha 2 and 2 alpha 4, respectively, and employed to solve the corresponding two-dimensional integral equation. The solutions of the weakly singular integral equation for 0 alpha 2 and of the hypersingular integral equation for 2 alpha 4 are obtained, respectively","the two-dimensional integral equation 1/ pi integral integral /sub d/( phi (r,theta )/r/sup 2/)ds=f(r/sub 0/, theta /sub 0/) defined on a circular disk d: r/sub 0/<or=a, 0<or= theta /sub 0/<or=2 pi , is considered in the present paper. here r in the kernel denotes the distance between two points p(r, theta ) and p/sub 0/(r/sub 0/, theta /sub 0/) in d, and 0< alpha <2 or 2< alpha <4. based on some known results of bessel functions, integral representations of the kernel are established for 0< alpha <2 and 2< alpha <4, respectively, and employed to solve the corresponding two-dimensional integral equation. the solutions of the weakly singular integral equation for 0< alpha <2 and of the hypersingular integral equation for 2< alpha <4 are obtained, respectively",127,0.857142857
8430,Computers and IT,"This paper describes two different embeddings of the manifolds corresponding to many elliptical probability distributions with the informative geometry into the manifold of positive-definite matrices with the Siegel metric, generalizing a result published previously elsewhere. These new general embeddings are applicable to a wide class of elliptical probability distributions, in which the normal, t-Student and Cauchy are specific examples. A lower bound for the Rao distance is obtained, which is itself a distance, and, through these embeddings, a number of statistical tests of hypothesis are derived","this paper describe two different embedding of the manifold corresponding to many elliptical probability distribution with the informative geometry into the manifold of positive-definite matrix with the siegel metric , generalize a result publish previously elsewhere . these new general embedding be applicable to a wide class of elliptical probability distribution , in which the normal , t-student and cauchy be specific example . a low bind for the rao distance be obtain , which be itself a distance , and , through these embedding , a number of statistical test of hypothesis be derive",86,0.571428571
8431,Computers and IT,"In this paper for a wide class of goodness-of-fit statistics based K/sub phi /-divergences, the asymptotic normality is established under the assumption n/m/sub n/ to a in (0, infinity ), where n denotes sample size and m/sub n/ the number of cells. This result is extended to contiguous alternatives to study asymptotic efficiency","in this paper for a wide class of goodness-of-fit statistics based k/sub phi/-divergences, the asymptotic normality is established under the assumption n/m/sub n/ to a in (0, infinity ), where n denotes sample size and m/sub n/ the number of cells. this result is extended to contiguous alternatives to study asymptotic efficiency",52,0.666666667
8432,Computers and IT,Lag windows whose corresponding spectral windows are Jacobi polynomials or sums of Jacobi polynomials are introduced. The bias and variance of their spectral density estimators are investigated and their window bandwidth and characteristic exponent are determined,lag window whose corresponding spectral window be jacobi polynomial or sumsof jacobi polynomial be introduce . the bias and variance of their spectral density estimator be investigate and their window bandwidth and characteristic exponent be determine,35,1
8433,Computers and IT,This paper gives a numerical C/sup 1/-shadowing between the exact solutions of a functional differential equation and its numerical approximations. The shadowing result is obtained by comparing exact solutions with numerical approximation which do not share the same initial value. Behavior of stable manifolds of functional differential equations under numerics will follow from the shadowing result,this paper give a numerical c/sup 1 / - shadow between the exact solution of a functional differential equation and its numerical approximation . the shadow result be obtain by compare exact solution with numerical approximation which do not share the same initial value . behavior of stable manifold of functional differential equation under numeric will follow from the shadow result,56,0.8
8434,Computers and IT,"We derive asymptotic expansions for the zeros of the cosine-integral Ci(x) and the Struve function H/sub 0/(x), and extend the available formulae for the zeros of Kelvin functions. Numerical evidence is provided to illustrate the accuracy of the expansions","we derive asymptotic expansions for the zeros of the cosine-integral ci(x) andthe struve function h/sub 0/(x), and extend the available formulae for the zeros of kelvin functions. numerical evidence is provided to illustrate the accuracy of the expansions",38,1
8435,Computers and IT,"This article aims to show that linguistics, in particular the study of the lexico-syntactic aspects of language, provides fertile ground for artificial life modeling. A survey of the models that have been developed over the last decade and a half is presented to demonstrate that ALife techniques have a lot to offer an explanatory theory of language. It is argued that this is because much of the structure of language is determined by the interaction of three complex adaptive systems: learning, culture, and biological evolution. Computational simulation, informed by theoretical linguistics, is an appropriate response to the challenge of explaining real linguistic data in terms of the processes that underpin human language","this article aim to show that linguistics , in particular the study of thelexico-syntactic aspect of language , provide fertile ground for artificial life modeling . a survey of the model that have be develop over the last decade and a half be present to demonstrate that alife technique have a lot to offer an explanatory theory of language . it be argue that this be because much of the structure of language be determine by the interaction of three complex adaptive system : learning , culture , and biological evolution . computational simulation , inform by theoretical linguistics , be an appropriate response to the challenge of explain real linguistic data in term of the process that underpin human language",111,0.9
8436,Computers and IT,"Self-replicating loops presented to date are essentially worlds unto themselves, inaccessible to the observer once the replication process is launched. We present the design of an interactive self-replicating loop of arbitrary size, wherein the user can physically control the loop's replication and induce its destruction. After introducing the BioWall, a reconfigurable electronic wall for bio-inspired applications, we describe the design of our novel loop and delineate its hardware implementation in the wall","self-replicating loop present to date be essentially world untothemselve , inaccessible to the observer once the replication process be launch . we present the design of an interactive self-replicating loop of arbitrary size , wherein the user can physically control the loop 's replication and induce its destruction . after introduce the biowall , a reconfigurable electronic wall for bio-inspired application , we describe the design of our novel loop and delineate its hardware implementation in the wall",71,0.454545455
8437,Computers and IT,"Alarm correlation is a necessity in large mobile phone networks, where the alarm bursts resulting from severe failures would otherwise overload the network operators. We describe how to realize alarm-correlation in cellular phone networks using extended logic programming. To this end, we describe an algorithm and system solving the problem, a model of a mobile phone network application, and a detailed solution for a specific scenario","alarm correlation be a necessity in large mobile phone network , where the alarm burst result from severe failure would otherwise overload the network operator . we describe how to realize alarm-correlation in cellular phone network use extended logic programming . to this end , we describe an algorithm and system solve the problem , a model of a mobile phone network application , and a detailed solution for a specific scenario",66,0.833333333
8438,Computers and IT,"Due to inevitable power dissipation, it is said that nano-scaled computing devices should perform their computing processes in a reversible manner. This will be a large problem in constructing three-dimensional nano-scaled functional objects. Reversible cellular automata (RCA) are used for modeling physical phenomena such as power dissipation, by studying the dissipation of garbage signals. We construct a three-dimensional self-inspective self-reproducing reversible cellular automaton by extending the two-dimensional version SR/sub 8/. It can self-reproduce various patterns in three-dimensional reversible cellular space without dissipating garbage signals","due to inevitable power dissipation, it is said that nano-scaled computingdevices should perform their computing processes in a reversible manner. this will be a large problem in constructing three-dimensional nano-scaled functional objects. reversible cellular automata (rca) are used for modeling physical phenomena such as power dissipation, by studying the dissipation of garbage signals. we construct a three-dimensional self-inspective self-reproducing reversible cellular automaton by extending the two-dimensional version sr/sub 8/. it can self-reproduce various patterns in three-dimensional reversible cellular space without dissipating garbage signals",83,0.428571429
8439,Computers and IT,"This work continues investigation into Gaia theory (Lovelock, The ages of Gaia, Oxford University Press, 1995) from an artificial life perspective (Downing, Proceedings of the 7th International Conference on Artificial Life, p. 90-99, MIT Press, 2000), with the aim of assessing the general compatibility of emergent distributed environmental control with conventional natural selection. Our earlier system, GUILD (Downing and Zvirinsky, Artificial Life, 5, p. 291-318, 1999), displayed emergent regulation of the chemical environment by a population of metabolizing agents, but the chemical model underlying those results was trivial, essentially admitting all possible reactions at a single energy cost. The new model, METAMIC, utilizes abstract chemistries that are both (a) constrained to a small set of legal reactions, and (b) grounded in basic fundamental relationships between energy, entropy, and biomass synthesis/breakdown. To explore the general phenomena of emergent homeostasis, we generate 100 different chemistries and use each as the basis for several METAMIC runs, as part of a Gaia hunt. This search discovers 20 chemistries that support microbial populations capable of regulating a physical environmental factor within their growth-optimal range, despite the extra metabolic cost. Case studies from the Gaia hunt illustrate a few simple mechanisms by which real biota might exploit the underlying chemistry to achieve some control over their physical environment. Although these results shed little light on the question of Gaia on Earth, they support the possibility of emergent environmental control at the microcosmic level","this work continues investigation into gaia theory (lovelock, the ages of gaia, oxford university press, 1995) from an artificial life perspective (downing, proceedings of the 7th international conference on artificial life, p. 90-99, mit press, 2000), with the aim of assessing the general compatibility of emergent distributed environmental control with conventional natural selection. our earlier system, guild (downing and zvirinsky, artificial life, 5, p.291-318, 1999), displayed emergent regulation of the chemical environment by a population of metabolizing agents, but the chemical model underlying those results was trivial, essentially admitting all possible reactions at a single energy cost. the new model, metamic, utilizes abstract chemistries that are both (a) constrained to a small set of legal reactions, and (b) grounded in basic fundamental relationships between energy, entropy, and biomass synthesis/breakdown. to explore the general phenomena of emergent homeostasis, we generate 100 different chemistries and use each as the basis for several metamic runs, as part of a gaia hunt. this search discovers 20 chemistries that support microbial populations capable of regulating a physical environmental factor within their growth-optimal range, despite the extra metabolic cost. case studies from the gaia hunt illustrate a few simple mechanisms by which real biota might exploit the underlying chemistry to achieve some control over their physical environment. although these results shed little light on the question of gaia on earth, they support the possibility of emergent environmental control at the microcosmic level",236,0.4
8440,Computers and IT,"We study the relationship between the two techniques known as ant colony optimization (ACO) and stochastic gradient descent. More precisely, we show that some empirical ACO algorithms approximate stochastic gradient descent in the space of pheromones, and we propose an implementation of stochastic gradient descent that belongs to the family of ACO algorithms. We then use this insight to explore the mutual contributions of the two techniques","we study the relationship between the two techniques known as ant colonyoptimization (aco) and stochastic gradient descent. more precisely, we show that some empirical aco algorithms approximate stochastic gradient descent in the space of pheromones, and we propose an implementation of stochastic gradient descent that belongs to the family of aco algorithms. we then use this insight to explore the mutual contributions of the two techniques",66,0.272727273
8441,Computers and IT,"Alex Fry looks at how content management systems can be used to ensure website access for one important customer group, the disabled","alex fry look at how content management system can be use to ensure websiteacces for one important customer group , the disabled",21,0.666666667
8442,Computers and IT,Records management standards are now playing a key role in e-business strategy,record management standard be now play a key role in e-busines strategy,12,1
8443,Computers and IT,This paper presents a novel algorithm for the extraction of the eye and mouth (facial features) fields from 2D gray level images. Eigenfeatures are derived from the eigenvalues and eigenvectors of the binary edge data set constructed from eye and mouth fields. Such eigenfeatures are ideal features for finely locating fields efficiently. The eigenfeatures are extracted from a set of the positive and negative training samples for facial features and are used to train a multilayer perceptron (MLP) whose output indicates the degree to which a particular image window contains the eyes or the mouth within itself. An ensemble network consisting of a multitude of independent MLPs was used to enhance the generalization performance of a single MLP. It was experimentally verified that the proposed algorithm is robust against facial size and even slight variations of the pose,this paper presents a novel algorithm for the extraction of the eye and mouth (facial features) fields from 2d gray level images. eigenfeatures are derived from the eigenvalues and eigenvectors of the binary edge data set constructed from eye and mouth fields. such eigenfeatures are ideal features for finely locating fields efficiently. the eigenfeatures are extracted from a set of the positive and negative training samples for facial features and are used to train a multilayer perceptron (mlp) whose output indicates the degree to which a particular image window contains the eyes or the mouth within itself. an ensemble network consisting of a multitude of independent mlps was used to enhance the generalization performance of a single mlp. it was experimentally verified that the proposed algorithm is robust against facial size and even slight variations of the pose,138,0.615384615
8444,Computers and IT,"Information management vendors are rushing to re-position themselves and put a portal spin on their products, says ITNET's Graham Urquhart. The result is confusion, with a range of different definitions and claims clouding the true picture","information management vendor be rush to re-position themselves and put aportal spin on their product , say itnet 's graham urquhart . the result be confusion , with a range of different definition and claim clouding the true picture",35,0
8445,Computers and IT,"Graeme Muir of SchlumbergerSema cuts through the confusion between content, document and records management","graeme muir of schlumbergersema cut through the confusion between content , document and record management",13,0
8446,Computers and IT,Andersen's William Yarker and Richard Young outline the route to a successful content management strategy,andersen 's william yarker and richard young outline the route to a successfulcontent management strategy,14,0.25
8447,Computers and IT,International law firm Linklaters has installed a global document and content management system that is accessible to clients and which has helped it move online,international law firm linklater have instal a global document and contentmanagement system that be accessible to client and which have help it move online,24,0
8448,Computers and IT,"We propose a unified approach for integrating implicit and explicit knowledge in neurosymbolic systems as a combination of neural and neuro-fuzzy modules. In the developed hybrid system, a training data set is used for building neuro-fuzzy modules, and represents implicit domain knowledge. The explicit domain knowledge on the other hand is represented by fuzzy rules, which are directly mapped into equivalent neural structures. The aim of this approach is to improve the abilities of modular neural structures, which are based on incomplete learning data sets, since the knowledge acquired from human experts is taken into account for adapting the general neural architecture. Three methods to combine the explicit and implicit knowledge modules are proposed. The techniques used to extract fuzzy rules from neural implicit knowledge modules are described. These techniques improve the structure and the behavior of the entire system. The proposed methodology has been applied in the field of air quality prediction with very encouraging results. These experiments show that the method is worth further investigation","we propose a unified approach for integrate implicit and explicit knowledge in neurosymbolic system as a combination of neural and neuro-fuzzy module . in the develop hybrid system , a training data set be use for build neuro-fuzzy module , and represent implicit domain knowledge . the explicit domain knowledge on the other hand be represent by fuzzy rule , which be directly map into equivalent neural structure . the aim of this approach be to improve the ability of modular neural structure , which be base on incomplete learning data set , since the knowledge acquire from human expert be take into account for adapt the general neural architecture . three method to combine the explicit and implicit knowledge module be propose . the technique use to extract fuzzy rule from neural implicit knowledge module be describe . these technique improve the structure and the behavior of the entire system . the propose methodology have be apply in the field of air quality prediction with very encouraging result . these experiment show that the method be worth further investigation",167,0
8449,Computers and IT,Marc Fresko of Cornwell Management Consultants says 'think ahead' when developing your electronic records management policy,marc fresko of cornwell management consultant say ` think ahead ' whendevelop your electronic record management policy,15,1
8450,Computers and IT,"Riemannian quadratics are C/sup 1/ curves on Riemannian manifolds, obtained by performing the quadratic recursive deCastlejeau algorithm in a Riemannian setting. They are of interest for interpolation problems in Riemannian manifolds, such as trajectory-planning for rigid body motion. Some interpolation properties of Riemannian quadratics are analysed when the ambient manifold is a sphere or projective space, with the usual Riemannian metrics","riemannian quadratic be c/sup 1 / curve on riemannian manifold , obtain byperform the quadratic recursive decastlejeau algorithm in a riemannian setting . they be of interest for interpolation problem in riemannian manifold , such as trajectory-planning for rigid body motion . some interpolation property of riemannian quadratic be analyze when the ambient manifold be a sphere or projective space , with the usual riemannian metric",60,0.5
8451,Computers and IT,"The interpolation of first-order Hermite data by spatial Pythagorean-hodograph curves that exhibit closure under arbitrary 3-dimensional rotations is addressed. The hodographs of such curves correspond to certain combinations of four polynomials, given by Dietz et al. (1993), that admit compact descriptions in terms of quaternions - an instance of the ""PH representation map"" proposed by Choi et al. (2002). The lowest-order PH curves that interpolate arbitrary first-order spatial Hermite data are quintics. It is shown that, with PH quintics, the quaternion representation yields a reduction of the Hermite interpolation problem to three ""simple"" quadratic equations in three quaternion unknowns. This system admits a closed-form solution, expressing all PH quintic interpolants to given spatial Hermite data as a two-parameter family. An integral shape measure is invoked to fix these two free parameters","the interpolation of first-order hermite data by spatial pythagorean-hodograph curves that exhibit closure under arbitrary 3-dimensional rotations is addressed. the hodographs of such curves correspond to certain combinations of four polynomials, given by dietz et al. (1993), that admit compact descriptions in terms of quaternions - an instance of the ""ph representation map"" proposed by choi et al. (2002). the lowest-order ph curves that interpolate arbitrary first-order spatial hermite data are quintics. it is shown that, with ph quintics, the quaternion representation yields a reduction of the hermite interpolation problem to three ""simple"" quadratic equations in three quaternion unknowns. this system admits a closed-form solution, expressing all ph quintic interpolants to given spatial hermite data as a two-parameter family. an integral shape measure is invoked to fix these two free parameters",131,0.857142857
8452,Computers and IT,"In this paper, we take the parabolic equation with periodic boundary conditions as a model to present a spectral method with the Fourier approximation in spatial and single/multi-interval Legendre Petrov-Galerkin methods in time. For the single interval spectral method in time, we obtain the optimal error estimate in L/sup 2/-norm. For the multi-interval spectral method in time, the L/sup 2/-optimal error estimate is valid in spatial. Numerical results show the efficiency of the methods","in this paper , we take the parabolic equation with periodic boundary conditionsa a model to present a spectral method with the fourier approximation in spatial and single/multi-interval legendre petrov-galerkin method in time . for the single interval spectral method in time , we obtain the optimal error estimate in l/sup 2 / - norm . for the multi-interval spectral method in time , the l/sup 2 / - optimal error estimate be valid in spatial . numerical result show the efficiency of the method",73,0.444444444
8453,Computers and IT,"Motivated by the problem of training multilayer perceptrons in neural networks, we consider the problem of minimizing E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/. x), where xi /sub i/ in R/sup S/, 1or=ior=n, and each f/sub i/( xi /sub i/. x) is a ridge function. We show that when n is small the problem of minimizing E can be treated as one of minimizing univariate functions, and we use the gradient algorithms for minimizing E when n is moderately large. For a large n, we present the online gradient algorithms and especially show the monotonicity and weak convergence of the algorithms","motivated by the problem of training multilayer perceptrons in neural networks,we consider the problem of minimizing e(x)= sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), where xi /sub i/ in r/sup s/, 1<or=i<or=n, and each f/sub i/( xi /sub i/.x) is a ridge function. we show that when n is small the problem of minimizing e can be treated as one of minimizing univariate functions, and we use the gradient algorithms for minimizing e when n is moderately large. for a large n, we present the online gradient algorithms and especially show the monotonicity and weak convergence of the algorithms",101,0.777777778
8454,Computers and IT,"In this paper, we consider the finite element approximations of a recently proposed Ginzburg-Landau-type model for d-wave superconductors. In contrast to the conventional Ginzburg-Landau model the scalar complex valued order-parameter is replaced by a multicomponent complex order-parameter and the free energy is modified according to the d-wave paring symmetry. Convergence and optimal error estimates and some super-convergent estimates for the derivatives are derived. Furthermore, we propose a multilevel linearization procedure to solve the nonlinear systems. It is proved that the optimal error estimates and super-convergence for the derivatives are preserved by the multi-level linearization algorithm","in this paper , we consider the finite element approximation of a recently propose ginzburg-landau-type model for d-wave superconductor . in contrast to the conventional ginzburg-landau model the scalar complex value order-parameter be replace by a multicomponent complex order-parameter and the free energy be modify accord to the d-wave par symmetry . convergence and optimal error estimate and some super-convergent estimate for the derivative be derive . furthermore , we propose a multilevel linearization procedure to solve the nonlinear system . it be prove that the optimal error estimate and super-convergence for the derivative be preserve by the multi-level linearization algorithm",95,0.555555556
8455,Computers and IT,"In this paper we consider a wavelet algorithm for the piecewise constant collocation method applied to the boundary element solution of a first kind integral equation arising in acoustic scattering. The conventional stiffness matrix is transformed into the corresponding matrix with respect to wavelet bases, and it is approximated by a compressed matrix. Finally, the stiffness matrix is multiplied by diagonal preconditioners such that the resulting matrix of the system of linear equations is well conditioned and sparse. Using this matrix, the boundary integral equation can be solved effectively","in this paper we consider a wavelet algorithm for the piecewise constant collocation method apply to the boundary element solution of a first kind integral equation arise in acoustic scattering . the conventional stiffness matrix be transform into the corresponding matrix with respect to wavelet base , and it be approximate by a compress matrix . finally , the stiffness matrix be multiply by diagonal preconditioner such that the result matrix of the system of linear equation be well condition and sparse . use this matrix , the boundary integral equation can be solve effectively",89,0.7
8456,Computers and IT,"In this paper we focus on the test selection problem. It is modeled after a real-life problem that arises in telecommunication when one has to check the reliability of an application. We apply different metaheuristics, namely Reactive Tabu Search (RTS), Genetic Algorithms (GA) and Simulated Annealing (SA) to solve the problem. We propose some modifications to the conventional schemes including an adaptive neighbourhood sampling in RTS, an adaptive variable mutation rate in GA and an adaptive variable neighbourhood structure in SA. The performance of the algorithms is evaluated in different models for existing protocols. Computational results show that GA and SA can provide high-quality solutions in acceptable time compared to the results of a commercial software, which makes them applicable in practical test selection","in this paper we focus on the test selection problem. it is modeled after areal-life problem that arises in telecommunication when one has to check the reliability of an application. we apply different metaheuristics, namely reactive tabu search (rts), genetic algorithms (ga) and simulated annealing (sa) to solve the problem. we propose some modifications to the conventional schemes including an adaptive neighbourhood sampling in rts, an adaptive variable mutation rate in ga and an adaptive variable neighbourhood structure in sa. the performance of the algorithms is evaluated in different models for existing protocols. computational results show that ga and sa can provide high-quality solutions in acceptable time compared to the results of a commercial software, which makes them applicable in practical test selection",123,0.538461538
8457,Computers and IT,"The art (and science) of successful product/service positioning generally hinges on the firm's ability to select a set of attractively priced consumer benefits that are: valued by the buyer, distinctive in one or more respects, believable, deliverable, and sustainable (under actual or potential competitive abilities to imitate, neutralize, or overcome) in the target markets that the firm selects. For many years, the ubiquitous quadrant chart has been used to provide a simple graph of product/service benefits (usually called product/service attributes) described in terms of consumers' perceptions of the importance of attributes (to brand/supplier choice) and the performance of competing firms on these attributes. This paper describes a model that extends the quadrant chart concept to a decision support system that optimizes a firm's market share for a specified product/service. In particular, we describe a decision support model that utilizes relatively simple marketing research data on consumers' judged benefit importances, and supplier performances on these benefits to develop message components for specified target buyers. A case study is used to illustrate the model. The study deals with developing advertising message components for a relatively new entrant in the US air shipping market. We also discuss, more briefly, management reactions to application of the model to date, and areas for further research and model extension","the art (and science) of successful product/service positioning generallyhinges on the firm's ability to select a set of attractively priced consumer benefits that are: valued by the buyer, distinctive in one or more respects, believable, deliverable, and sustainable (under actual or potential competitive abilities to imitate, neutralize, or overcome) in the target markets that the firm selects. for many years, the ubiquitous quadrant chart has been used to provide a simple graph of product/service benefits (usually called product/service attributes) described in terms of consumers' perceptions of the importance of attributes (to brand/supplier choice) and the performance of competing firms on these attributes. this paper describes a model that extends the quadrant chart concept to a decision support system that optimizes a firm's market share for a specified product/service. in particular, we describe a decision support model that utilizes relatively simple marketing research data on consumers' judged benefit importances, and supplier performances on these benefits to develop message components for specified target buyers. a case study is used to illustrate the model. the study deals with developing advertising message components for a relatively new entrant in the us air shipping market. we also discuss, more briefly, management reactions to application of the model to date, and areas for further research and model extension",212,0.470588235
8458,Computers and IT,"In this paper, experts' opinions are described in linguistic terms which can be expressed in trapezoidal (or triangular) fuzzy numbers. To make the consensus of the experts consistent, we utilize the fuzzy Delphi method to adjust the fuzzy rating of every expert to achieve the consensus condition. For the aggregate of many experts' opinions, we take the operation of fuzzy numbers to get the mean of fuzzy rating, x/sub ij/ and the mean of weight, w/sub . j/. In multi-alternatives and multi-attributes cases, the fuzzy decision matrix X=[x/sub ij/]/sub m*n/ is constructed by means of the fuzzy rating, x/sub ij/. Then, we can derive the aggregate fuzzy numbers by multiplying the fuzzy decision matrix with the corresponding fuzzy attribute weights. The final results become a problem of ranking fuzzy numbers. We also propose an easy procedure of using fuzzy numbers to rank aggregate fuzzy numbers A/sub i/. In this way, we can obtain the best selection for evaluating the system. For practical application, we propose an algorithm for evaluating the best main battle tank by fuzzy decision theory and comparing it with other methods","in this paper, experts' opinions are described in linguistic terms which can be expressed in trapezoidal (or triangular) fuzzy numbers. to make the consensus of the experts consistent, we utilize the fuzzy delphi method to adjust the fuzzy rating of every expert to achieve the consensus condition. for the aggregate of many experts' opinions, we take the operation of fuzzy numbers to get the mean of fuzzy rating, x/sub ij/ and the mean of weight, w/sub .j/. in multi-alternatives and multi-attributes cases, the fuzzy decision matrix x=[x/sub ij/]/sub m*n/ is constructed by means of the fuzzy rating, x/sub ij/. then, we can derive the aggregate fuzzy numbers by multiplying the fuzzy decision matrix with the corresponding fuzzy attribute weights. the final results become a problem of ranking fuzzy numbers. we also propose an easy procedure of using fuzzy numbers to rank aggregate fuzzy numbers a/sub i/. in this way, we can obtain the best selection for evaluating the system. for practical application, we propose an algorithm for evaluating the best main battle tank by fuzzy decision theory and comparing it with other methods",183,0.4375
8459,Computers and IT,"In this paper, PRIMES (Progressive Reasoning and Intelligent multiple MEthods System), a new architecture for resource-bounded reasoning that combines a form of progressive reasoning and the so-called multiple methods approach is presented. Each time-critical reasoning unit is designed in such a way that it delivers an approximate result in time whenever an overload or a failure prevents the system from producing the most accurate result. Indeed, reasoning units use approximate processing based on two salient features. First, an incremental processing unit constructs an approximate solution quickly and then refines it incrementally. Second, a multiple methods approach proposes different alternatives to solve the problem, each of them being selected according to the available resources. In allowing several resource-bounded reasoning paradigms to be combined, we hope to extend their actual scope to cover more real-world application domains","in this paper, primes (progressive reasoning and intelligent multiple methodssystem), a new architecture for resource-bounded reasoning that combines a form of progressive reasoning and the so-called multiple methods approach is presented. each time-critical reasoning unit is designed in such a way that it delivers an approximate result in time whenever an overload or a failure prevents the system from producing the most accurate result. indeed, reasoning units use approximate processing based on two salient features. first, an incremental processing unit constructs an approximate solution quickly and then refines it incrementally. second, a multiple methods approach proposes different alternatives to solve the problem, each of them being selected according to the available resources. in allowing several resource-bounded reasoning paradigms to be combined, we hope to extend their actual scope to cover more real-world application domains",134,0.5
8460,Computers and IT,"The paper describes the process of the safe ship control in a collision situation using a differential game model with j participants. As an approximated model of the manoeuvring process, a model of a multi-step matrix game is adopted here. RISKTRAJ computer program is designed in the Matlab language in order to determine the ship's trajectory as a certain sequence of manoeuvres executed by altering the course and speed, in the online navigator decision support system. These considerations are illustrated with examples of a computer simulation of the safe ship's trajectories in real situation at sea when passing twelve of the encountered objects","the paper describe the process of the safe ship control in a collisionsituation use a differential game model with j participant . as an approximate model of the manoeuvring process , a model of a multi-step matrix game be adopt here . risktraj computer program be design in the matlab language in order to determine the ship 's trajectory as a certain sequence of maneuver execute by alter the course and speed , in the online navigator decision support system . these consideration be illustrate with example of a computer simulation of the safe ship 's trajectory in real situation at sea when pass twelve of the encounter object",102,0.444444444
8461,Computers and IT,"Allocating donor kidneys to patients is a complex, multicriteria decision-making problem which involves not only medical, but also ethical and political issues. In this paper, a fuzzy logic expert system approach was proposed as an innovative way to deal with the vagueness and complexity faced by medical doctors in kidney allocation decision making. A pilot fuzzy logic expert system for kidney allocation was developed and evaluated in comparison with two existing allocation algorithms: a priority sorting system used by multiple organ retrieval and exchange (MORE) in Canada and a point scoring systems used by united network for organ sharing (UNOS) in US. Our simulated experiment based on real data indicated that the fuzzy logic system can represent the expert's thinking well in handling complex tradeoffs, and overall, the fuzzy logic derived recommendations were more acceptable to the expert than those from the MORE and UNOS algorithms","allocating donor kidneys to patients is a complex, multicriteria decision-making problem which involves not only medical, but also ethical and political issues. in this paper, a fuzzy logic expert system approach was proposed as an innovative way to deal with the vagueness and complexity faced by medical doctors in kidney allocation decision making. a pilot fuzzy logic expert system for kidney allocation was developed and evaluated in comparison with two existing allocation algorithms: a priority sorting system used by multiple organ retrieval and exchange (more) in canada and a point scoring systems used by united network for organ sharing (unos) in us. our simulated experiment based on real data indicated that the fuzzy logic system can represent the expert's thinking well in handling complex tradeoffs, and overall, the fuzzy logic derived recommendations were more acceptable to the expert than those from the more and unos algorithms",146,0.454545455
8462,Computers and IT,In this paper we propose five heuristic procedures for the solution of the multiple competitive facilities location problem. A franchise of several facilities is to be located in a trade area where competing facilities already exist. The objective is to maximize the market share captured by the franchise as a whole. We perform extensive computational tests and conclude that a two-step heuristic procedure combining simulated annealing and an ascent algorithm provides the best solutions,in this paper we propose five heuristic procedure for the solution of themultiple competitive facility location problem . a franchise of several facility be to be locate in a trade area where compete facility already exist . the objective be to maximize the market share capture by the franchise as a whole . we perform extensive computational test and conclude that a two-step heuristic procedure combine simulated annealing and an ascent algorithm provide the best solution,73,0.75
8463,Computers and IT,"It is well acknowledged in software engineering that there is a great potential for accomplishing significant productivity improvements through the implementation of a successful software reuse program. On the other hand, such gains are attainable only by instituting detailed action plans at both the organizational and program level. Given this need, the paucity of research papers related to planning, and in particular, optimized planning is surprising. This research, which is aimed at this gap, brings out an application of optimization for the planning of reusable software components (SCs). We present a model that selects a set of SCs that must be built, in order to lower development and adaptation costs. We also provide implications to project management based on simulation, an approach that has been adopted by other cost models in the software engineering literature. Such a prescriptive model does not exist in the literature","it is well acknowledged in software engineering that there is a great potentialfor accomplishing significant productivity improvements through the implementation of a successful software reuse program. on the other hand, such gains are attainable only by instituting detailed action plans at both the organizational and program level. given this need, the paucity of research papers related to planning, and in particular, optimized planning is surprising. this research, which is aimed at this gap, brings out an application of optimization for the planning of reusable software components (scs). we present a model that selects a set of scs that must be built, in order to lower development and adaptation costs. we also provide implications to project management based on simulation, an approach that has been adopted by other cost models in the software engineering literature. such a prescriptive model does not exist in the literature",144,0.818181818
8464,Computers and IT,"The purpose of this paper is to illustrate the importance of using structural results in dynamic programming algorithms. We consider the problem of approximating optimal strategies for the batch service of customers at a service station. Customers stochastically arrive at the station and wait to be served, incurring a waiting cost and a service cost. Service of customers is performed in groups of a fixed service capacity. We investigate the structure of cost functions and establish some theoretical results including monotonicity of the value functions. Then, we use our adaptive dynamic programming monotone algorithm that uses structure to preserve monotonicity of the estimates at each iterations to approximate the value functions. Since the problem with homogeneous customers can be solved optimally, we have a means of comparison to evaluate our heuristic. Finally, we compare our algorithm to classical forward dynamic programming methods","the purpose of this paper be to illustrate the importance of use structural result in dynamic programming algorithm . we consider the problem of approximate optimal strategy for the batch service of customer at a service station . customer stochastically arrive at the station and wait to be serve , incur a wait cost and a service cost . service of customer be perform in group of a fixed service capacity . we investigate the structure of cost function and establish some theoretical result include monotonicity of the value function . then , we use our adaptive dynamic programming monotone algorithm that use structure to preserve monotonicity of the estimate at each iteration to approximate the value function . since the problem with homogeneous customer can be solve optimally , we have a mean of comparison to evaluate our heuristic . finally , we compare our algorithm to classical forward dynamic programming method",142,0.454545455
8465,Computers and IT,"In this paper, we analyze how sharing advance demand information (ADI) can improve supply-chain performance. We consider two types of ADI, aggregated ADI (A-ADI) and detailed ADI (D-ADI). With A-ADI, customers share with manufacturers information about whether they will place an order for some product in the next time period, but do not share information about which product they will order and which of several potential manufacturers will receive the order. With D-ADI, customers additionally share information about which product they will order, but which manufacturer will receive the order remains uncertain. We develop and solve mathematical models of supply chains where ADI is shared. We derive exact expressions and closed-form approximations for expected costs, expected base-stock levels, and variations of the production quantities. We show that both the manufacturer and the customers benefit from sharing ADI, but that sharing ADI increases the bullwhip effect. We also show that under certain conditions it is optimal to collect ADI from either none or all of the customers. We study two supply chains in detail: a supply chain with an arbitrary number of products that have identical demand rates, and a supply chain with two products that have arbitrary demand rates. For these two supply chains, we analyze how the values of A-ADI and D-ADI depend on the characteristics of the supply chain and on the quality of the shared information, and we identify conditions under which sharing A-ADI and D-ADI can significantly reduce cost. Our results can be used by decision makers to analyze the cost savings that can be achieved by sharing ADI and help them to determine if sharing ADI is beneficial for their supply chains","in this paper, we analyze how sharing advance demand information (adi) canimprove supply-chain performance. we consider two types of adi, aggregated adi (a-adi) and detailed adi (d-adi). with a-adi, customers share with manufacturers information about whether they will place an order for some product in the next time period, but do not share information about which product they will order and which of several potential manufacturers will receive the order. with d-adi, customers additionally share information about which product they will order, but which manufacturer will receive the order remains uncertain. we develop and solve mathematical models of supply chains where adi is shared. we derive exact expressions and closed-form approximations for expected costs, expected base-stock levels, and variations of the production quantities. we show that both the manufacturer and the customers benefit from sharing adi, but that sharing adi increases the bullwhip effect. we also show that under certain conditions it is optimal to collect adi from either none or all of the customers. we study two supply chains in detail: a supply chain with an arbitrary number of products that have identical demand rates, and a supply chain with two products that have arbitrary demand rates. for these two supply chains, we analyze how the values of a-adi and d-adi depend on the characteristics of the supply chain and on the quality of the shared information, and we identify conditions under which sharing a-adi and d-adi can significantly reduce cost. our results can be used by decision makers to analyze the cost savings that can be achieved by sharing adi and help them to determine if sharing adi is beneficial for their supply chains",276,0.526315789
8466,Computers and IT,"Arc routing problems (ARPs) consist of finding a traversal on a graph satisfying some conditions related to the links of the graph. In the Chinese postman problem (CPP) the aim is to find a minimum cost tour (closed walk) traversing all the links of the graph at least once. Both the Undirected CPP, where all the links are edges that can be traversed in both ways, and the Directed CPP, where all the links are arcs that must be traversed in a specified way, are known to be polynomially solvable. However, if we deal with a mixed graph (having edges and arcs), the problem turns out to be NP-hard. In this paper, we present a heuristic algorithm for this problem, the so-called Mixed CPP (MCPP), based on greedy randomized adaptive search procedure (GRASP) techniques. The algorithm has been tested and compared with other known and recent methods from the literature on a wide collection of randomly generated instances, with up to 200 nodes and 600 links, producing encouraging computational results. As far as we know, this is the best heuristic algorithm for the MCPP, with respect to solution quality, published up to now","arc routing problems (arps) consist of finding a traversal on a graphsatisfying some conditions related to the links of the graph. in the chinese postman problem (cpp) the aim is to find a minimum cost tour (closed walk) traversing all the links of the graph at least once. both the undirected cpp, where all the links are edges that can be traversed in both ways, and the directed cpp, where all the links are arcs that must be traversed in a specified way, are known to be polynomially solvable. however, if we deal with a mixed graph (having edges and arcs), the problem turns out to be np-hard. in this paper, we present a heuristic algorithm for this problem, the so-called mixed cpp (mcpp), based on greedy randomized adaptive search procedure (grasp) techniques. the algorithm has been tested and compared with other known and recent methods from the literature on a wide collection of randomly generated instances, with up to 200 nodes and 600 links, producing encouraging computational results. as far as we know, this is the best heuristic algorithm for the mcpp, with respect to solution quality, published up to now",192,0.181818182
8467,Computers and IT,"This paper deals with the single machine earliness and tardiness scheduling problem with a common due date and resource-dependent release dates. It is assumed that the cost of resource consumption of a job is a non-increasing linear function of the job release date, and this function is common for all jobs. The objective is to find a schedule and job release dates that minimize the total resource consumption, and earliness and tardiness penalties. It is shown that the problem is NP-hard in the ordinary sense even if the due date is unrestricted (the number of jobs that can be scheduled before the due date is unrestricted). An exact dynamic programming (DP) algorithm for small and medium size problems is developed. A heuristic algorithm for large-scale problems is also proposed and the results of a computational comparison between heuristic and optimal solutions are discussed","this paper deals with the single machine earliness and tardiness scheduling problem with a common due date and resource-dependent release dates. it is assumed that the cost of resource consumption of a job is a non-increasing linear function of the job release date, and this function is common for all jobs. the objective is to find a schedule and job release dates that minimize the total resource consumption, and earliness and tardiness penalties. it is shown that the problem is np-hard in the ordinary sense even if the due date is unrestricted (the number of jobs that can be scheduled before the due date is unrestricted). an exact dynamic programming (dp) algorithm for small and medium size problems is developed. a heuristic algorithm for large-scale problems is also proposed and the results of a computational comparison between heuristic and optimal solutions are discussed",143,0.428571429
8468,Computers and IT,"The performance of new railway networks cannot be measured or simulated, as no detailed train schedules are available. Railway infrastructure and capacities are to be determined long before the actual traffic is known. This paper therefore proposes a solvable queueing network model to compute performance measures of interest without requiring train schedules (timetables). Closed form expressions for mean delays are obtained. New network designs, traffic scenarios, and capacity expansions can so be evaluated. A comparison with real delay data for the Netherlands supports the practical value of the model. A special Dutch cargo-line application is included","the performance of new railway networks cannot be measured or simulated, as no detailed train schedules are available. railway infrastructure and capacities are to be determined long before the actual traffic is known. this paper therefore proposes a solvable queueing network model to compute performance measures of interest without requiring train schedules (timetables). closed form expressions for mean delays are obtained. new network designs, traffic scenarios, and capacity expansions can so be evaluated. a comparison with real delay data for the netherlands supports the practical value of the model. a special dutch cargo-line application is included",96,0.833333333
8469,Computers and IT,"In this paper, a monotone process model for a one-component degenerative system with k+1 states (k failure states and one working state) is studied. We show that this model is equivalent to a geometric process (GP) model for a two-state one component system such that both systems have the same long-run average cost per unit time and the same optimal policy. Furthermore, an explicit expression for the determination of an optimal policy is derived","in this paper, a monotone process model for a one-component degenerative systemwith k+1 states (k failure states and one working state) is studied. we show that this model is equivalent to a geometric process (gp) model for a two-state one component system such that both systems have the same long-run average cost per unit time and the same optimal policy. furthermore, an explicit expression for the determination of an optimal policy is derived",73,0.545454545
8470,Computers and IT,"Data envelopment analysis (DEA) measures the relative efficiency of decision making units (DMUs) with multiple performance factors which are grouped into outputs and inputs. Once the efficient frontier is determined, inefficient DMUs can improve their performance to reach the efficient frontier by either increasing their current output levels or decreasing their current input levels. However, both desirable (good) and undesirable (bad) factors may be present. For example, if inefficiency exists in production processes where final products are manufactured with a production of wastes and pollutants, the outputs of wastes and pollutants are undesirable and should be reduced to improve the performance. Using the classification invariance property, we show that the standard DEA model can be used to improve the performance via increasing the desirable outputs and decreasing the undesirable outputs. The method can also be applied to situations when some inputs need to be increased to improve the performance. The linearity and convexity of DEA are preserved through our proposal","data envelopment analysis (dea) measures the relative efficiency of decisionmaking units (dmus) with multiple performance factors which are grouped into outputs and inputs. once the efficient frontier is determined, inefficient dmus can improve their performance to reach the efficient frontier by either increasing their current output levels or decreasing their current input levels. however, both desirable (good) and undesirable (bad) factors may be present. for example, if inefficiency exists in production processes where final products are manufactured with a production of wastes and pollutants, the outputs of wastes and pollutants are undesirable and should be reduced to improve the performance. using the classification invariance property, we show that the standard dea model can be used to improve the performance via increasing the desirable outputs and decreasing the undesirable outputs. the method can also be applied to situations when some inputs need to be increased to improve the performance. the linearity and convexity of dea are preserved through our proposal",159,0.6875
8471,Computers and IT,"Generally acknowledged as a critical problem for many information professionals, the massive flow of documents, paper trails, and information needs efficient and dependable approaches for processing and storing and finding items and information","generally acknowledge as a critical problem for many informationprofessional , the massive flow of document , paper trail , and information need efficient and dependable approach for processing and store and find item and information",32,0
8472,Computers and IT,We develop a multicriteria approach to the problem of space heating under a time varying price of electricity. In our dynamic goal programming model the goals are ideal temperature intervals and the other criteria are the costs and energy consumption. We discuss the modelling requirements in multicriteria problems with a dynamic structure and present a new relaxation method combining the traditional epsilon -constraint and goal programming (GP) methods. The multi-objective heating optimization (MOHO) application in a spreadsheet environment with numerical examples is described,we develop a multicriteria approach to the problem of space heating under atime varying price of electricity. in our dynamic goal programming model the goals are ideal temperature intervals and the other criteria are the costs and energy consumption. we discuss the modelling requirements in multicriteria problems with a dynamic structure and present a new relaxation method combining the traditional epsilon -constraint and goal programming (gp) methods. the multi-objective heating optimization (moho) application in a spreadsheet environment with numerical examples is described,82,0.714285714
8473,Computers and IT,"Organizations have lately realized that just processing transactions and/or information faster and more efficiently no longer provides them with a competitive advantage vis-a-vis their competitors for achieving business excellence. Information technology (IT) tools that are oriented towards knowledge processing can provide the edge that organizations need to survive and thrive in the current era of fierce competition. Enterprises are no longer satisfied with business information system(s); they require business intelligence system(s). The increasing competitive pressures and the desire to leverage information technology techniques have led many organizations to explore the benefits of new emerging technology, data warehousing and data mining. The paper discusses data warehouses and data mining tools and applications","organizations have lately realized that just processing transactions and/orinformation faster and more efficiently no longer provides them with a competitive advantage vis-a-vis their competitors for achieving business excellence. information technology (it) tools that are oriented towards knowledge processing can provide the edge that organizations need to survive and thrive in the current era of fierce competition. enterprises are no longer satisfied with business information system(s); they require business intelligence system(s). the increasing competitive pressures and the desire to leverage information technology techniques have led many organizations to explore the benefits of new emerging technology, data warehousing and data mining. the paper discusses data warehouses and data mining tools and applications",110,1
8474,Computers and IT,This paper presents the input current waveform control of the rectifier circuit which realizes simultaneously the high input power factor and the harmonics suppression of the receiving-end voltage and the source current under the distorted receiving-end voltage. The proposed input current waveform includes the harmonic components which are in phase with the receiving-end voltage harmonics. The control parameter in the proposed waveform is designed by examining the characteristics of both the harmonic suppression effect in the distribution system and the input power factor of the rectifier circuit. The effectiveness of the proposed current waveform has been confirmed experimentally,this paper present the input current waveform control of the rectifier circuit which realize simultaneously the high input power factor and the harmonic suppression of the receiving-end voltage and the source current under the distorted receiving-end voltage . the propose input current waveform include the harmonic component which be in phase with the receiving-end voltage harmonic . the control parameter in the propose waveform be design by examine the characteristic of both the harmonic suppression effect in the distribution system and the input power factor of the rectifier circuit . the effectiveness of the propose current waveform have be confirm experimentally,98,0.533333333
8475,Computers and IT,"This paper describes the locating system of line-to-ground faults on a power transmission line by using a wavelet transform. The possibility of the location with the surge generated by a fault has been theoretically proposed. In order to make the method practicable, the authors realize very fast processors. They design the wavelet transform and location chips, and construct a very fast fault location system by processing the measured data in parallel. This system is realized by a computer with three FPGA processor boards on a PCI bus. The processors are controlled by UNIX and the system has a graphical user interface with an X window system","this paper describe the locate system of line-to-ground fault on a powertransmission line by use a wavelet transform . the possibility of the location with the surge generate by a fault have be theoretically propose . in order to make the method practicable , the author realize very fast processor . they design the wavelet transform and location chip , and construct a very fast fault location system by process the measure data in parallel . this system be realize by a computer with three fpga processor board on a pci bus . the processor be control by unix and the system have a graphical user interface with an x window system",105,0.6
8476,Computers and IT,"Systems characterised by fractional power poles can be called fractional systems. Here, Laguerre orthogonal polynomials are employed to approximate fractional systems by minimum phase, reduced order, rational transfer functions. Both the time and the frequency-domain analysis exhibit the accuracy of the approximation","system characterise by fractional power pole can be call fractionalsystem . here , laguerre orthogonal polynomial be employ to approximate fractional system by minimum phase , reduce order , rational transfer function . both the time and the frequency-domain analysis exhibit the accuracy of the approximation",41,0.636363636
8477,Computers and IT,A novel pitch post-processing technique based on robust statistics is proposed. Performances in terms of pitch error rates and pitch contours show the superiority of the proposed method compared with the median filtering technique. Further improvement is achieved through incorporating an uncertainty term in the robust statistics model,a novel pitch post-processing technique base on robust statistic be propose . performance in term of pitch error rate and pitch contor show the superiority of the propose method compare with the median filter technique . further improvement be achieve through incorporate an uncertainty term in the robust statistic model,47,0.75
8478,Computers and IT,An adaptive array antenna is proposed based on the radial basis function (RBF) network as a multiuser detector for a WCDMA system. The proposed system calculates the optimal combining weight coefficients using sample matrix inversion with a common correlation matrix algorithm and obtains the channel response vector using the RBF output signal,an adaptive array antenna is proposed based on the radial basis function (rbf) network as a multiuser detector for a wcdma system. the proposed system calculates the optimal combining weight coefficients using sample matrix inversion with a common correlation matrix algorithm and obtains the channel response vector using the rbf output signal,52,0.4
8479,Computers and IT,"The proposed modifying of the structure of the radial basis function (RBF) network by introducing the weight matrix to the input layer (in contrast to the direct connection of the input to the hidden layer of a conventional RBF) so that the training space in the RBF network is adaptively separated by the resultant decision boundaries and class regions is reported. The training of this weight matrix is carried out as for a single-layer perceptron together with the clustering process. In this way the network is capable of dealing with complicated problems, which have a high degree of interference in the training data, and achieves a higher classification rate over the current classifiers using RBF","the proposed modifying of the structure of the radial basis function (rbf)network by introducing the weight matrix to the input layer (in contrast to the direct connection of the input to the hidden layer of a conventional rbf) so that the training space in the rbf network is adaptively separated by the resultant decision boundaries and class regions is reported. the training of this weight matrix is carried out as for a single-layer perceptron together with the clustering process. in this way the network is capable of dealing with complicated problems, which have a high degree of interference in the training data, and achieves a higher classification rate over the current classifiers using rbf",114,0.333333333
8480,Computers and IT,A novel rate allocation algorithm for video transmission over lossy networks subject to bursty packet losses is presented. A Gilbert-Elliot model is used at the encoder to drive the selection of coding parameters. Experimental results using the H. 26L test model show a significant performance improvement with respect to the assumption of independent packet losses,a novel rate allocation algorithm for video transmission over lossy networkssubject to bursty packet loss be present . a gilbert-elliot model be use at the encoder to drive the selection of cod parameter . experimental result use the h. 26l test model show a significant performance improvement with respect to the assumption of independent packet loss,53,0.75
8481,Computers and IT,"E-books will survive, but not in the consumer market - at least not until reading devices become much cheaper and much better in quality (which is not likely to happen soon). Library Journal's review of major events of the year 2001 noted that two requirements for the success of E-books were development of a sustainable business model and development of better reading devices. The E-book revolution has therefore become more of an evolution. We can look forward to further developments and advances in the future","e-books will survive, but not in the consumer market - at least not untilreading devices become much cheaper and much better in quality (which is not likely to happen soon). library journal's review of major events of the year 2001 noted that two requirements for the success of e-books were development of a sustainable business model and development of better reading devices. the e-book revolution has therefore become more of an evolution. we can look forward to further developments and advances in the future",84,0.666666667
8482,Computers and IT,"The exact three-dimensional (3D) design of a coaxial Cauer filter employing a new filter model, a 3D field simulator and a circuit simulator, is demonstrated. Only a few iterations between the field simulator and the circuit simulator are necessary to meet a given specification","the exact three-dimensional (3d) design of a coaxial cauer filter employing anew filter model, a 3d field simulator and a circuit simulator, is demonstrated. only a few iterations between the field simulator and the circuit simulator are necessary to meet a given specification",43,0.555555556
8483,Computers and IT,A new method to implement an arbitrary piece-wise-linear characteristic in current mode is presented. Each of the breaking points and each slope is separately controllable. As an example a block that implements an N-shaped piece-wise-linearity has been designed. The N-shaped block operates in the subthreshold region and uses only ten transistors. These characteristics make it especially suitable for large arrays of neuro-fuzzy systems where the number of transistors and power consumption per cell is an important concern. A prototype of this block has been fabricated in a 0. 35 mu m CMOS technology. The functionality and programmability of this circuit has been verified through experimental results,a new method to implement an arbitrary piece-wise-linear characteristic in current mode be present . each of the breaking point and each slope be separately controllable . as an example a block that implement an n-shaped piece-wise-linearity have be design . the n-shaped block operate in the subthreshold region and use only ten transistor . these characteristic make it especially suitable for large array of neuro-fuzzy system where the number of transistor and power consumption per cell be an important concern . a prototype of this block have be fabricate in a 0.35 mu m cmo technology . the functionality and programmability of this circuit have be verify through experimental result,105,0.727272727
8484,Computers and IT,Recently much attention has been focused on video streaming through IP-based networks. An error resilient RD intra macro-block refresh scheme for H. 26L Internet video streaming is introduced. Various channel simulations have proved that this scheme is more effective than those currently adopted in H. 26L,recently much attention have be focus on video stream through ip-basednetwork . an error resilient rd intra macro-block refresh scheme for h. 26l internet video stream be introduce . various channel simulation have prove that this scheme be more effective than those currently adopt in h. 26l,43,0.3
8485,Computers and IT,"Multimedia applications require high bandwidth and guaranteed quality of service (QoS). The current Internet, which provides 'best effort' services, cannot meet the stringent QoS requirements for delivering MPEG videos. It is proposed that MPEG frames are transported through various service models of DiffServ. Performance analysis and simulation results show that the proposed approach can not only guarantee QoS but can also achieve high bandwidth utilisation","multimedia applications require high bandwidth and guaranteed quality ofservice (qos). the current internet, which provides 'best effort' services, cannot meet the stringent qos requirements for delivering mpeg videos. it is proposed that mpeg frames are transported through various service models of diffserv. performance analysis and simulation results show that the proposed approach can not only guarantee qos but can also achieve high bandwidth utilisation",64,0.555555556
8486,Computers and IT,A time domain optimisation algorithm using a genetic algorithm in conjunction with a linear search scheme has been developed to find the smallest or near-smallest subset of inputs and outputs to control a multi-input-multi-output system. Experimental results have shown that this proposed algorithm has a very fast convergence rate and high computation efficiency,a time domain optimisation algorithm use a genetic algorithm in conjunction with a linear search scheme have be develop to find the small or near-small subset of input and output to control a multi-input-multi-output system . experimental result have show that this propose algorithm have a very fast convergence rate and high computation efficiency,53,0.615384615
8487,Computers and IT,An investigation is presented into the feasibility of incorporating a fuzzy weighting scheme into the calculation of an autocorrelation function for pitch extraction. Simulation results reveal that the proposed method provides better robustness against background noise than the conventional approaches for extracting pitch period in a noisy environment,an investigation be present into the feasibility of incorporate a fuzzy weighting scheme into the calculation of an autocorrelation function for pitch extraction . simulation result reveal that the propose method provide better robustness against background noise than the conventional approach for extract pitch period in a noisy environment,48,0.555555556
8488,Computers and IT,"In describing the mean square convergence of the LMS algorithm, the update formula based on independence assumption will bring explicit errors, especially when step-size is large. A modifier formula that describes the convergence well, is proposed. Simulations support the proposed formula in different conditions","in describe the mean square convergence of the lm algorithm , the updateformula base on independence assumption will bring explicit error , especially when step-size be large . a modifi formula that describe the convergence well , be propose . simulation support the propose formula in different condition",43,0.571428571
8489,Computers and IT,A new adaptive algorithm for blind identification of time-varying MA channels is derived. This algorithm proposes the use of a novel system of equations derived by combining the third- and fourth-order statistics of the output signals of MA models. This overdetermined system of equations has the important property that it can be solved adaptively because of their symmetries via an overdetermined recursive instrumental variable-type algorithm. This algorithm shows good behaviour in arbitrary noisy environments and good performance in tracking time-varying systems,a new adaptive algorithm for blind identification of time-varying ma channelsi derive . this algorithm propose the use of a novel system of equation derive by combine the third - and fourth-order statistic of the output signal of ma model . this overdetermined system of equation have the important property that it can be solve adaptively because of their symmetry via an overdetermined recursive instrumental variable-type algorithm . this algorithm show good behavior in arbitrary noisy environment and good performance in track time-varying system,80,0.357142857
8490,Computers and IT,An adaptive quasi-Newton algorithm is first developed to extract a single minor component corresponding to the smallest eigenvalue of a stationary sample covariance matrix. A deflation technique instead of the commonly used inflation method is then applied to extract the higher-order minor components. The algorithm enjoys the advantage of having a simpler computational complexity and a highly modular and parallel structure for efficient implementation. Simulation results are given to demonstrate the effectiveness of the proposed algorithm for extracting multiple minor components adaptively,an adaptive quasi-newton algorithm be first develop to extract a single minorcomponent corresponding to the small eigenvalue of a stationary sample covariance matrix . a deflation technique instead of the commonly use inflation method be then apply to extract the higher-ord minor component . the algorithm enjoy the advantage of have a simple computational complexity and a highly modular and parallel structure for efficient implementation . simulation result be give to demonstrate the effectiveness of the propose algorithm for extract multiple minor component adaptively,81,0.615384615
8491,Computers and IT,"The use of a new type of neural network (NN) for controlling the trajectory of a robot is discussed. A control system is described which comprises an NN-based controller and a fixed-gain feedback controller. The NN-based controller employs a modified recurrent NN, the weights of which are obtained by training another NN to identify online the inverse dynamics of the robot. The work has confirmed the superiority of the proposed NN-based control system in rejecting large disturbances","the use of a new type of neural network (nn) for controlling the trajectory ofa robot is discussed. a control system is described which comprises an nn-based controller and a fixed-gain feedback controller. the nn-based controller employs a modified recurrent nn, the weights of which are obtained by training another nn to identify online the inverse dynamics of the robot. the work has confirmed the superiority of the proposed nn-based control system in rejecting large disturbances",76,0.25
8492,Computers and IT,"Web access to news sites all over the world allows us the opportunity to have an electronic news stand readily available and stocked with a variety of foreign (to us) news sites. A large number of currently available foreign sites are English-language publications or English language versions of non-North American sites. These sites are quite varied in terms of quality, coverage, and style. Finding them can present a challenge. Using them effectively requires critical-thinking skills that are a part of media awareness or digital literacy","web access to news sites all over the world allows us the opportunity to havean electronic news stand readily available and stocked with a variety of foreign (to us) news sites. a large number of currently available foreign sites are english-language publications or english language versions of non-north american sites. these sites are quite varied in terms of quality, coverage, and style. finding them can present a challenge. using them effectively requires critical-thinking skills that are a part of media awareness or digital literacy",84,0.571428571
8493,Computers and IT,A digital-domain self-calibration technique for video-rate pipeline A/D converters based on a Gaussian white noise input signal is presented. The proposed algorithm is simple and efficient. A design example is shown to illustrate that the overall linearity of a pipeline ADC can be highly improved using this technique,a digital-domain self-calibration technique for video-rate pipeline a/d converter base on a gaussian white noise input signal be present . the propose algorithm be simple and efficient . a design example be show to illustrate that the overall linearity of a pipeline adc can be highly improve use this technique,48,0.75
8494,Computers and IT,"At the RSA cryptosystem implementation stage, a major security concern is resistance against so-called side-channel attacks. Solutions are known but they increase the overall complexity by a non-negligible factor (typically, a protected RSA exponentiation is 133% slower). For the first time, protected solutions are proposed that do not penalise the running time of an exponentiation","at the rsa cryptosystem implementation stage, a major security concern isresistance against so-called side-channel attacks. solutions are known but they increase the overall complexity by a non-negligible factor (typically, a protected rsa exponentiation is 133% slower). for the first time, protected solutions are proposed that do not penalise the running time of an exponentiation",54,0.333333333
8495,Computers and IT,"A wide-bandwidth, high-resolution fractional delay filter (FDF) structure with a small number of multipliers per output sample and a short coefficient computing time is presented. The proposal is based on the use of a frequency FDF design method up to only half of the Nyquist frequency, in a multirate structure","a wide-bandwidth, high-resolution fractional delay filter (fdf) structure witha small number of multipliers per output sample and a short coefficient computing time is presented. the proposal is based on the use of a frequency fdf design method up to only half of the nyquist frequency, in a multirate structure",49,0.444444444
8496,Computers and IT,"A master-slave D-type flip-flop (MS DFF) fabricated in a self-aligned InP DHBT technology is presented. The packaged circuit shows full-rate clock operation at 48 Gbit/s. Very low time jitter and good retiming capabilities are observed. Layout aspects, packaging and measurement issues are discussed in particular","a master-slave d-type flip-flop (ms dff) fabricated in a self-aligned inp dhbttechnology is presented. the packaged circuit shows full-rate clock operation at 48 gbit/s. very low time jitter and good retiming capabilities are observed. layout aspects, packaging and measurement issues are discussed in particular",44,0.4
8497,Computers and IT,"The author is impressed with the upgrade to the mathematics package Maple 8, finding it genuinely useful to scientists and educators. The developments Waterloo Maple class as revolutionary include a student calculus package, and Maplets. The first provides a high-level command set for calculus exploration and plotting (removing the need to work with, say, plot primitives). The second is a package for hand-coding custom graphical user interfaces (GUIs) using elements such as check boxes, radio buttons, slider bars and pull-down menus. When called, a Maplet launches a runtime Java environment that pops up a window-analogous to a Java applet-to perform a programmed routine, if required passing the result back to the Maple worksheet","the author is impressed with the upgrade to the mathematics package maple 8,finding it genuinely useful to scientists and educators. the developments waterloo maple class as revolutionary include a student calculus package, and maplets. the first provides a high-level command set for calculus exploration and plotting (removing the need to work with, say, plot primitives). the second is a package for hand-coding custom graphical user interfaces (guis) using elements such as check boxes, radio buttons, slider bars and pull-down menus. when called, a maplet launches a runtime java environment that pops up a window-analogous to a java applet-to perform a programmed routine, if required passing the result back to the maple worksheet",112,0.75
8498,Computers and IT,"FlexPro 5. 0, from Weisang and Co. , is one of those products which aim to serve an often ignored range of data users: those who, in FlexPro's words, are interested in documenting, analysing and archiving data in the simplest way possible. The online help system is clearly designed to promote the product in this market segment, with a very clear introduction from first principles and a hands-on tutorial, and the live project to which it was applied was selected with this in mind","flexpro 5.0 , from weisang and co. , be one of those product which aim to servean often ignore range of data user : those who , in flexpro 's word , be interested in document , analyze and archive data in the simple way possible . the online help system be clearly design to promote the product in this market segment , with a very clear introduction from first principle and a hands-on tutorial , and the live project to which it be apply be select with this in mind",81,0.5
8499,Computers and IT,"During the past several years, initiatives which bring together librarians, researchers, university administrators and independent publishers have re-invigorated the scholarly publishing marketplace. These initiatives take advantage of electronic technology and show great potential for restoring science to scientists. The author outlines SPARC (the Scholarly Publishing and Academic Resources Coalition), an initiative to make scientific journals more accessible","during the past several years, initiatives which bring together librarians,researchers, university administrators and independent publishers have re-invigorated the scholarly publishing marketplace. these initiatives take advantage of electronic technology and show great potential for restoring science to scientists. the author outlines sparc (the scholarly publishing and academic resources coalition), an initiative to make scientific journals more accessible",56,0.4
8500,Computers and IT,"The business of publishing journals is in transition. Nobody knows exactly how it will work in the future, but everybody knows that the electronic publishing revolution will ensure it won't work as it does now. This knowledge has provoked a growing sense of nervous anticipation among those concerned, some edgy and threatened by potential changes to their business, others excited by the prospect of change and opportunity. The paper discusses the open publishing model for dissemination of research","the business of publish journal be in transition . nobody know exactly howit will work in the future , but everybody know that the electronic publishing revolution will ensure it wo n't work as it do now . this knowledge have provoke a grow sense of nervous anticipation among those concerned , some edgy and threaten by potential change to their business , other excite by the prospect of change and opportunity . the paper discuss the open publishing model for dissemination of research",77,0.5
8501,Computers and IT,"It has always been more difficult for chemistry to keep up in the Internet age but a new language could herald a new era for the discipline. The paper discusses CML, or chemical mark-up language. The eXtensible Mark-up Language provides a universal format for structured documents and data on the Web and so offers a way for scientists and others to carry a wide range of information types across the net in a transparent way. All that is needed is an XML browser","it have always be more difficult for chemistry to keep up in the internet agebut a new language could herald a new era for the discipline . the paper discuss cml , or chemical mark-up language . the extensible mark-up language provide a universal format for structured document and data on the web and so offer a way for scientist and other to carry a wide range of information type across the net in a transparent way . all that be need be an xml browser",82,0.75
8502,Computers and IT,"Warwick University in the UK is on the up and up. Sometimes considered a typical 1960s, middle-of-the-road redbrick institution-not known for their distinction the 2001 UK Research Assessment Exercise (RAE) shows its research to be the fifth most highly-rated in the country, with outstanding standards in the sciences. This impressive performance has rightly given Warwick a certain amount of muscle, which it is flexing rather effectively, aided by a snappy approach to making things happen that leaves some older institutions standing. The result is a brand new Centre for Scientific Computing (CSC), launched within a couple of years of its initial conception","warwick university in the uk is on the up and up. sometimes considered atypical 1960s, middle-of-the-road redbrick institution-not known for their distinction the 2001 uk research assessment exercise (rae) shows its research to be the fifth most highly-rated in the country, with outstanding standards in the sciences. this impressive performance has rightly given warwick a certain amount of muscle, which it is flexing rather effectively, aided by a snappy approach to making things happen that leaves some older institutions standing. the result is a brand new centre for scientific computing (csc), launched within a couple of years of its initial conception",101,0
8503,Computers and IT,"Since so few end-users make use of Boolean searching, it is obvious that any effective solution needs to take this reality into account. The most important aspect of a technical solution should be that it does not require any effort on the part of users. What is clearly needed is for search engine designers and programmers to take account of the information-seeking behavior of Internet users. Users must be able to enter a series of words at random and have those words automatically treated as a carefully constructed Boolean AND search statement","since so few end-user make use of boolean search , it be obvious that anyeffective solution need to take this reality into account . the most important aspect of a technical solution should be that it do not require any effort on the part of user . what be clearly need be for search engine designer and programmer to take account of the information-seeking behavior of internet user . user must be able to enter a series of word at random and have those word automatically treat as a carefully construct boolean and search statement",91,0.666666667
8504,Computers and IT,"For software manufacturers, blessings come in the form of fast-moving application areas. In the case of LIMS, biotechnology is still in the driving seat, inspiring developers to maintain consistently rapid and creative levels of innovation. Current advancements are no exception. Integration and linking initiatives are still popular and much of the activity appears to be coming from a very productive minority","for software manufacturer , blessing come in the form of fast-movingapplication area . in the case of lim , biotechnology be still in the driving seat , inspiring developer to maintain consistently rapid and creative level of innovation . current advancement be no exception . integration and link initiative be still popular and much of the activity appear to be come from a very productive minority",60,1
8505,Computers and IT,"Mainstream observers commonly underestimate the role of fringe activities in propelling science and technology. Well-known examples are how wars have fostered innovation in areas such as communications, cryptography, medicine and aerospace; and how erotica has been a major factor in pioneering visual media, from the first printed books to photography, cinematography, videotape, or the latest online video streaming. The article aims to be a sampler of a less controversial, but still often underrated, symbiosis between scientific computing and computing for leisure and entertainment","mainstream observer commonly underestimate the role of fringe activity inpropell science and technology . well-known example be how war have foster innovation in area such as communication , cryptography , medicine and aerospace ; and how erotica have be a major factor in pioneering visual medium , from the first print book to photography , cinematography , videotape , or the late online video stream . the article aim to be a sampler of a less controversial , but still often underrated , symbiosis between scientific computing and computing for leisure and entertainment",82,0.6
8506,Computers and IT,"information system Based on previous research and properties of organizational memory, a conceptual model for navigation and retrieval functions in an intranet portal organizational memory information system was proposed, and two human-centred features (memory structure map and history-based tool) were developed to support user's navigation and retrieval in a well-known organizational memory. To test two hypotheses concerning the validity of the conceptual model and two human-centred features, an experiment was conducted with 30 subjects. Testing of the two hypotheses indicated the following: (1) the memory structure map's users showed 29% better performance in navigation, and (2) the history-based tool's users outperformed by 34% in identifying information. The results of the study suggest that a conceptual model and two human-centred features could be used in a user-adaptive interface design to improve user's performance in an intranet portal organizational memory information system","information system based on previous research and properties of organizational memory, a conceptual model for navigation and retrieval functions in an intranet portal organizational memory information system was proposed, and two human-centred features (memory structure map and history-based tool) were developed to support user's navigation and retrieval in a well-known organizational memory. to test two hypotheses concerning the validity of the conceptual model and two human-centred features, an experiment was conducted with 30 subjects. testing of the two hypotheses indicated the following: (1) the memory structure map's users showed 29% better performance in navigation, and (2) the history-based tool's users outperformed by 34% in identifying information. the results of the study suggest that a conceptual model and two human-centred features could be used in a user-adaptive interface design to improve user's performance in an intranet portal organizational memory information system",140,0.636363636
8507,Computers and IT,"For different levels of user performance, different types of information are processed and users will make different types of errors. Based on the error's immediate cause and the information being processed, usability problems can be classified into three categories. They are usability problems associated with skill-based, rule-based and knowledge-based levels of performance. In this paper, a user interface for a Web-based software program was evaluated with two usability evaluation methods, user testing and heuristic evaluation. The experiment discovered that the heuristic evaluation with human factor experts is more effective than user testing in identifying usability problems associated with skill-based and rule-based levels of performance. User testing is more effective than heuristic evaluation in finding usability problems associated with the knowledge-based level of performance. The practical application of this research is also discussed in the paper","for different level of user performance , different type of information be process and user will make different type of error . base on the error 's immediate cause and the information be process , usability problem can be classify into three category . they be usability problem associate with skill-based , rule-based and knowledge-based level of performance . in this paper , a user interface for a web-based software program be evaluate with two usability evaluation method , user testing and heuristic evaluation . the experiment discover that the heuristic evaluation with human factor expert be more effective than user testing in identify usability problem associate with skill-based and rule-based level of performance . user testing be more effective than heuristic evaluation in find usability problem associate with the knowledge-based level of performance . the practical application of this research be also discuss in the paper",135,0.666666667
8508,Computers and IT,diagnosis Machine induction has been extensively used in order to develop knowledge bases for decision support systems and predictive systems. The extent to which developers and domain experts can comprehend these knowledge structures and gain useful insights into the basis of decision making has become a challenging research issue. This article examines the knowledge structures generated by the C4. 5 induction technique in a fault diagnostic task and proposes to use a model of human learning in order to guide the process of making comprehensive the results of machine induction. The model of learning is used to generate hierarchical representations of diagnostic knowledge by adjusting the level of abstraction and varying the goal structures between 'shallow' and 'deep' ones. Comprehensibility is assessed in a global way in an experimental comparison where subjects are required to acquire the knowledge structures and transfer to new tasks. This method of addressing the issue of comprehensibility appears promising especially for machine induction techniques that are rather inflexible with regard to the number and sorts of interventions allowed to system developers,diagnosis machine induction have be extensively use in order to develop knowledge base for decision support system and predictive system . the extent to which developer and domain expert can comprehend these knowledge structure and gain useful insight into the basis of decision making have become a challenging research issue . this article examine the knowledge structure generate by the c4 .5 induction technique in a fault diagnostic task and propose to use a model of human learning in order to guide the process of make comprehensive the result of machine induction . the model of learning be use to generate hierarchical representation of diagnostic knowledge by adjust the level of abstraction and vary the goal structure between ` shallow ' and ` deep ' one . comprehensibility be assess in a global way in an experimental comparison where subject be require to acquire the knowledge structure and transfer to new task . this method of address the issue of comprehensibility appear promising especially for machine induction technique that be rather inflexible with regard to the number and sort of intervention allow to system developer,175,0.416666667
8509,Computers and IT,"A new work organization was introduced in administrative surveying work in Sweden during 1998. The new work organization implied a transition to a client-centred team-based organization and required a change in competence from specialist to generalist knowledge as well as a transition to a new information technology, implying a greater integration within the company. The aim of this study was to follow the surveyors for two years from the start of the transition and investigate how perceived consequences of the transition, job, organizational factors, well-being and effectiveness measures changed between 1998 and 2000. The Teamwork Profile and QPS Nordic questionnaire were used. The 205 surveyors who participated in all three study phases constituted the study group. The result showed that surveyors who perceived that they were working as generalists rated the improvements in job and organizational factors significantly higher than those who perceived that they were not yet generalists. Improvements were noted in 2000 in quality of service to clients, time available to handle a case and effectiveness of teamwork in a transfer to a team-based work organization group, cohesion and continuous improvement practices-for example, learning by doing, mentoring and guided delegation-were important to improve the social effectiveness of group work","a new work organization be introduce in administrative survey work in sweden during 1998 . the new work organization imply a transition to a client-centred team-based organization and require a change in competence from specialist to generalist knowledge as well as a transition to a new information technology , imply a great integration within the company . the aim of this study be to follow the surveyor for two year from the start of the transition and investigate how perceive consequence of the transition , job , organizational factor , well-being and effectiveness measure change between 1998 and 2000 . the teamwork profile and qp nordic questionnaire be use . the 205 surveyor who participate in all three study phase constitute the study group . the result show that surveyor who perceive that they be work as generalist rat the improvement in job and organizational factor significantly high than those who perceive that they be not yet generalist . improvement be note in 2000 in quality of service to client , time available to handle a case and effectiveness of teamwork in a transfer to a team-based work organization group , cohesion and continuous improvement practices-for example , learn by do , mentor and guide delegation-were important to improve the social effectiveness of group work",201,0.818181818
8510,Computers and IT,"The present experiments investigate point-to-point mapping of perspective transformations of 2D outline figures under diverse viewing conditions: binocular free viewing, monocular perspective with 2D cues masked by an optic tunnel, and stereoptic viewing through an optic tunnel. The first experiment involved upright figures, and served to determine baseline point-to-point mapping accuracy, which was found to be very good. Three shapes were used: square, circle and irregularly round. The main experiment, with slanted figures, involved only two shapes-square and irregularly shaped-showed at several slant degrees. Despite the accumulated evidence for shape constancy when the outline of perspective projections is considered, metric perception of the inner structure of such projections was quite limited. Systematic distortions were found, especially with more extreme slants, and attributed to the joint effect of several factors: anchors, 3D information, and slant underestimation. Contradictory flatness cues did not detract from performance, while stereoptic information improved it","the present experiment investigate point-to-point mapping of perspective transformation of 2d outline figure under diverse viewing condition : binocular free viewing , monocular perspective with 2d cue mask by an optic tunnel , and stereoptic view through an optic tunnel . the first experiment involve upright figure , and serve to determine baseline point-to-point mapping accuracy , which be find to be very good . three shape be use : square , circle and irregularly round . the main experiment , with slanted figure , involve only two shapes-square and irregularly shaped-showed at several slant degree . despite the accumulate evidence for shape constancy when the outline of perspective projection be consider , metric perception of the inner structure of such projection be quite limited . systematic distortion be find , especially with more extreme slant , and attribute to the joint effect of several factor : anchor , 3d information , and slant underestimation . contradictory flatness cue do not detract from performance , while stereoptic information improve it",148,0.705882353
8511,Computers and IT,"This study investigated how common online text affects reading performance of elementary school-age children by examining the actual and perceived readability of four computer-displayed typefaces at 12- and 14-point sizes. Twenty-seven children, ages 9 to 11, were asked to read eight children's passages and identify erroneous/substituted words while reading. Comic Sans MS, Arial and Times New Roman typefaces, regardless of size, were found to be more readable (as measured by a reading efficiency score) than Courier New. No differences in reading speed were found for any of the typeface combinations. In general, the 14-point size and the examined sans serif typefaces were perceived as being the easiest to read, fastest, most attractive, and most desirable for school-related material. In addition, participants significantly preferred Comic Sans MS and 14-point Arial to 12-point Courier. Recommendations for appropriate typeface combinations for children reading on computers are discussed","this study investigated how common online text affects reading performance of elementary school-age children by examining the actual and perceived readability of four computer-displayed typefaces at 12- and 14-point sizes. twenty-seven children, ages 9 to 11, were asked to read eight children's passages and identify erroneous/substituted words while reading. comic sans ms, arial and times new roman typefaces, regardless of size, were found to be more readable (as measured by a reading efficiency score) than courier new. no differences in reading speed were found for any of the typeface combinations. in general, the 14-point size and the examined sans serif typefaces were perceived as being the easiest to read, fastest, most attractive, and most desirable for school-related material. in addition, participants significantly preferred comic sans ms and 14-point arial to 12-point courier. recommendations for appropriate typeface combinations for children reading on computers are discussed",144,0.333333333
8512,Computers and IT,"Take the stress out of the office by considering the design of furniture and staff needs, before major buying decisions","take the stress out of the office by consider the design of furniture andstaff need , before major buying decision",19,0.333333333
8513,Computers and IT,"Make purchasing stationery a relatively simple task through effective planning and management of stock, and identifying the right supplier","make purchase stationery a relatively simple task through effective planningand management of stock , and identify the right supplier",18,0.75
8514,Computers and IT,"Information literacy is important in academic and other libraries. The paper looks at whether it would be more useful to librarians and to instructors, as well as the students, to deal with information-literacy skill levels of students beginning their academic careers, rather than checking them at the end. Approaching the situation with an eye toward the broader scope of critical media literacy opens the discussion beyond a skills inventory to the broader range of intellectual activity","information literacy be important in academic and other library . the paperlook at whether it would be more useful to librarian and to instructor , as well as the student , to deal with information-literacy skill level of student begin their academic career , rather than check them at the end . approach the situation with an eye toward the broad scope of critical medium literacy open the discussion beyond a skill inventory to the broad range of intellectual activity",75,0.6
8515,Computers and IT,"The low to mid-speed copier market is being transformed by the almost universal adoption of digital solutions. The days of the analogue copier are numbered as the remaining vendors plan to withdraw from this sector by 2005. Reflecting the growing market for digital, vendors are reducing prices, making a digital solution much more affordable. The battle for the copier market is intense, and the popularity of the multifunctional device is going to transform the office equipment market. As total cost of ownership becomes increasingly important and as budgets are squeezed, the most cost-effective solutions are those that will survive this shake-down","the low to mid-spe copier market be be transform by the almost universaladoption of digital solution . the day of the analog copier be number as the remain vendor plan to withdraw from this sector by 2005 . reflect the grow market for digital , vendor be reduce price , make a digital solution much more affordable . the battle for the copier market be intense , and the popularity of the multifunctional device be go to transform the office equipment market . as total cost of ownership become increasingly important and as budget be squeeze , the most cost-effective solution be those that will survive this shake-down",100,1
8516,Computers and IT,"We previously developed a data-sparse and accurate approximation to parabolic solution operators in the case of a rather general elliptic part given by a strongly P-positive operator . Also a class of matrices (H-matrices) has been analysed which are data-sparse and allow an approximate matrix arithmetic with almost linear complexity. In particular, the matrix-vector/matrix-matrix product with such matrices as well as the computation of the inverse have linear-logarithmic cost. In this paper, we apply the H-matrix techniques to approximate the exponent of an elliptic operator. Starting with the Dunford-Cauchy representation for the operator exponent, we then discretise the integral by the exponentially convergent quadrature rule involving a short sum of resolvents. The latter are approximated by the H-matrices. Our algorithm inherits a two-level parallelism with respect to both the computation of resolvents and the treatment of different time values. In the case of smooth data (coefficients, boundaries), we prove the linear-logarithmic complexity of the method","we previously developed a data-sparse and accurate approximation to parabolicsolution operators in the case of a rather general elliptic part given by a strongly p-positive operator . also a class of matrices (h-matrices) has been analysed which are data-sparse and allow an approximate matrix arithmetic with almost linear complexity. in particular, the matrix-vector/matrix-matrix product with such matrices as well as the computation of the inverse have linear-logarithmic cost. in this paper, we apply the h-matrix techniques to approximate the exponent of an elliptic operator. starting with the dunford-cauchy representation for the operator exponent, we then discretise the integral by the exponentially convergent quadrature rule involving a short sum of resolvents. the latter are approximated by the h-matrices. our algorithm inherits a two-level parallelism with respect to both the computation of resolvents and the treatment of different time values. in the case of smooth data (coefficients, boundaries), we prove the linear-logarithmic complexity of the method",154,0.5
8517,Computers and IT,"One approximates the entropy weak solution u of a nonlinear parabolic degenerate equation u/sub t/+div(qf(u))- Delta phi (u)=0 by a piecewise constant function u/sub D/ using a discretization D in space and time and a finite volume scheme. The convergence of u/sub D/ to u is shown as the size of the space and time steps tend to zero. In a first step, estimates on u/sub D/ are used to prove the convergence, up to a subsequence, of u/sub D/ to a measure valued entropy solution (called here an entropy process solution). A result of uniqueness of the entropy process solution is proved, yielding the strong convergence of u/sub D/ to u. Some numerical results on a model equation are shown","one approximates the entropy weak solution u of a nonlinear parabolic degenerate equation u/sub t/+div(qf(u))- delta phi (u)=0 by a piecewise constant function u/sub d/ using a discretization d in space and time and a finite volume scheme. the convergence of u/sub d/ to u is shown as the size of the space and time steps tend to zero. in a first step, estimates on u/sub d/ are used to prove the convergence, up to a subsequence, of u/sub d/ to a measure valued entropy solution (called here an entropy process solution). a result of uniqueness of the entropy process solution is proved, yielding the strong convergence of u/sub d/ to u. some numerical results on a model equation are shown",121,0.857142857
8518,Computers and IT,"For pt. I. see SIAM J. Numer. Anal. , vol. 38, p. 876-896. Circulant-type preconditioners have been proposed previously for ill-conditioned Hermitian Toeplitz systems that are generated by nonnegative continuous functions with a zero of even order. The proposed circulant preconditioners can be constructed without requiring explicit knowledge of the generating functions. It was shown that the spectra of the preconditioned matrices are uniformly bounded except for a fixed number of outliers and that all eigenvalues are uniformly bounded away from zero. Therefore the conjugate gradient method converges linearly when applied to solving the circulant preconditioned systems. Previously it was claimed that this result can be extended to the case where the generating functions have multiple zeros. The main aim of this paper is to give a complete convergence proof of the method for this class of generating functions","for pt . i. see siam j. numer . anal. , vol . 38 , p. 876-896 . circulant-type preconditioner have be propose previously for ill-conditioned hermitian toeplitz system that be generate by nonnegative continuous function with a zero of even order . the propose circulant preconditioner can be construct without require explicit knowledge of the generate function . it be show that the spectrum of the preconditioned matrix be uniformly bound except for a fix number of outlier and that all eigenvalue be uniformly bound away from zero . therefore the conjugate gradient method converge linearly when apply to solve the circulant precondition system . previously it be claim that this result can be extend to the case where the generate function have multiple zero . the main aim of this paper be to give a complete convergence proof of the method for this class of generate function",137,0.875
8519,Computers and IT,"This paper proposes a validation method for solutions of nonlinear complementarity problems. The validation procedure performs a computational test. If the result of the test is positive, then it is guaranteed that a given multi-dimensional interval either includes a solution or excludes all solutions of the nonlinear complementarity problem","this paper propose a validation method for solution of nonlinear complementarity problem . the validation procedure perform a computational test . if the result of the test be positive , then it be guarantee that a give multi-dimensional interval either include a solution or exclude all solution of the nonlinear complementarity problem",49,0.5
8520,Computers and IT,"The study reported here investigates the influence of ""interactivity"" on the learning outcomes of users in a multimedia systems environment. Drawing from past literature base and based on key tenets of three learning theories - behaviorist, cognitivist, and constructivist - the study first proposes a measurement scheme for ""interactivity"" and then hypothesizes that ""interactivity"" would influence the learning outcomes positively in terms of users' learning achievement and attitude. Three prototypes of a multimedia instructional/training system to represent high, low, and noninteractive modes of use were developed and implemented and the hypothesized influences were investigated using a controlled laboratory research design. Multiple analysis of variance (MANOVA) results indicate that while interactivity does not necessarily enable enhanced gain in user learning, it positively influences participants' attitude. The study finds no support for hypothesized moderating effects of learning styles (measured using Kolb's Learning Style Inventory scale) on the relationship between interactivity and user outcomes. The results of this study have important implications for both education and corporations' training efforts and investments. Implications and future research directions are discussed","the study reported here investigates the influence of ""interactivity"" on the learning outcomes of users in a multimedia systems environment. drawing from past literature base and based on key tenets of three learning theories - behaviorist, cognitivist, and constructivist - the study first proposes a measurement scheme for ""interactivity"" and then hypothesizes that ""interactivity"" would influence the learning outcomes positively in terms of users' learning achievement and attitude. three prototypes of a multimedia instructional/training system to represent high, low, and noninteractive modes of use were developed and implemented and the hypothesized influences were investigated using a controlled laboratory research design. multiple analysis of variance (manova) results indicate that while interactivity does not necessarily enable enhanced gain in user learning, it positively influences participants' attitude. the study finds no support for hypothesized moderating effects of learning styles (measured using kolb's learning style inventory scale) on the relationship between interactivity and user outcomes. the results of this study have important implications for both education and corporations' training efforts and investments. implications and future research directions are discussed",175,0.533333333
8521,Computers and IT,"Pressured by the growing need for fast response times, mass customization, and globalization, many organizations are turning to flexible organizational forms, such as virtual teams. Virtual teams consist of cooperative relationships supported by information technology to overcome limitations of time and/or location. Virtual teams require their members to rely heavily on the use of information technology and trust in coworkers. This study investigates the impacts that the reliance on information technology (operationalized in our study via the user satisfaction construct) and trust have on the job satisfaction of virtual team members. The study findings reveal that both user satisfaction and trust are positively related to job satisfaction in virtual teams, while system use was not found to play a significant role. These findings emphasize that organizations seeking the benefits of flexible, IT-enabled virtual teams must consider both the level of trust among colleagues, and the users' satisfaction with the information technology on which virtual teams rely","pressured by the growing need for fast response times, mass customization, andglobalization, many organizations are turning to flexible organizational forms, such as virtual teams. virtual teams consist of cooperative relationships supported by information technology to overcome limitations of time and/or location. virtual teams require their members to rely heavily on the use of information technology and trust in coworkers. this study investigates the impacts that the reliance on information technology (operationalized in our study via the user satisfaction construct) and trust have on the job satisfaction of virtual team members. the study findings reveal that both user satisfaction and trust are positively related to job satisfaction in virtual teams, while system use was not found to play a significant role. these findings emphasize that organizations seeking the benefits of flexible, it-enabled virtual teams must consider both the level of trust among colleagues, and the users' satisfaction with the information technology on which virtual teams rely",155,1
8522,Computers and IT,"Many CEO and managers understand the importance of knowledge sharing among their employees and are eager to introduce the knowledge management paradigm in their organizations. However little is known about the determinants of the individual's knowledge sharing behavior. The purpose of this study is to develop an understanding of the factors affecting the individual's knowledge sharing behavior in the organizational context. The research model includes various constructs based on social exchange theory, self-efficacy, and theory of reasoned action. Research results from the field survey of 467 employees of four large, public organizations show that expected associations and contribution are the major determinants of the individual's attitude toward knowledge sharing. Expected rewards, believed by many to be the most important motivating factor for knowledge sharing, are not significantly related to the attitude toward knowledge sharing. As expected, positive attitude toward knowledge sharing is found to lead to positive intention to share knowledge and, finally, to actual knowledge sharing behaviors","many ceo and manager understand the importance of knowledge sharing among their employee and be eager to introduce the knowledge management paradigm in their organization . however little be know about the determinant of the individual 's knowledge sharing behavior . the purpose of this study be to develop an understanding of the factor affect the individual 's knowledge sharing behavior in the organizational context . the research model include various construct base on social exchange theory , self-efficacy , and theory of reason action . research result from the field survey of 467 employee of four large , public organization show that expect association and contribution be the major determinant of the individual 's attitude toward knowledge sharing . expect reward , believe by many to be the most important motivate factor for knowledge sharing , be not significantly relate to the attitude toward knowledge sharing . as expect , positive attitude toward knowledge sharing be find to lead to positive intention to share knowledge and , finally , to actual knowledge share behavior",158,0.875
8523,Computers and IT,"This paper introduces chaos theory as a means of studying information systems. It argues that chaos theory, combined with new techniques for discovering patterns in complex quantitative and qualitative evidence, offers a potentially more substantive approach to understand the nature of information systems in a variety of contexts. The paper introduces chaos theory concepts by way of an illustrative research design","this paper introduce chaos theory as a mean of study information system . it argue that chaos theory , combine with new technique for discover pattern in complex quantitative and qualitative evidence , offer a potentially more substantive approach to understand the nature of information system in a variety of context . the paper introduce chaos theory concept by way of an illustrative research design",60,0.6
8524,Computers and IT,"This editorial preface investigates current developments in mobile commerce (M-commerce) and proposes an integrated architecture that supports business and consumer needs in an optimal way to successfully implement M-commerce business processes. The key line of thought is based on the heuristic observation that customers will not want to receive M-commerce offerings to their mobile telephones. As a result, a pull as opposed to a push approach becomes a necessary requirement to conduct M-commerce. In addition, M-commerce has to rely on local, regional, demographic and many other variables to be truly effective. Both observations necessitate an M-commerce architecture that allows the coherent integration of enterprise-level systems as well as the aggregation of product and service offerings from many different and partially competing parties into a collaborative M-commerce platform. The key software component within this integrated architecture is an event management engine to monitor, detect, store, process and measure information about outside events that are relevant to all participants in M-commerce","this editorial preface investigates current developments in mobile commerce(m-commerce) and proposes an integrated architecture that supports business and consumer needs in an optimal way to successfully implement m-commerce business processes. the key line of thought is based on the heuristic observation that customers will not want to receive m-commerce offerings to their mobile telephones. as a result, a pull as opposed to a push approach becomes a necessary requirement to conduct m-commerce. in addition, m-commerce has to rely on local, regional, demographic and many other variables to be truly effective. both observations necessitate an m-commerce architecture that allows the coherent integration of enterprise-level systems as well as the aggregation of product and service offerings from many different and partially competing parties into a collaborative m-commerce platform. the key software component within this integrated architecture is an event management engine to monitor, detect, store, process and measure information about outside events that are relevant to all participants in m-commerce",158,0.555555556
8525,Computers and IT,"The author looks at how we can focus on what we want, finding small stories in vast oceans of news. There is no one tool that will scan every news resource available and give alerts on new available materials. Every one has a slightly different focus. Some are paid sources, while many are free. If used wisely, an excellent news monitoring system for a large number of topics can be set up for surprisingly little cost","the author look at how we can focus on what we want , find small story invast ocean of news . there be no one tool that will scan every news resource available and give alert on new available material . every one have a slightly different focus . some be pay source , while many be free . if use wisely , an excellent news monitoring system for a large number of topic can be set up for surprisingly little cost",75,0.333333333
8526,Computers and IT,"A class of regularized conjugate gradient methods is presented for solving the large sparse system of linear equations of which the coefficient matrix is an ill-conditioned symmetric positive definite matrix. The convergence properties of these methods are discussed in depth, and the best possible choices of the parameters involved in the new methods are investigated in detail. Numerical computations show that the new methods are more efficient and robust than both classical relaxation methods and classical conjugate direction methods","a class of regularize conjugate gradient method be present for solve the large sparse system of linear equation of which the coefficient matrix be an ill-conditioned symmetric positive definite matrix . the convergence property of these method be discuss in depth , and the best possible choice of the parameter involve in the new method be investigate in detail . numerical computation show that the new method be more efficient and robust than both classical relaxation method and classical conjugate direction method",79,0.777777778
8527,Computers and IT,"With annual revenues increasing 17. 0% to 20. 0% consistently over the last three years and more than 2, 500 new stores opened from 1998 through 2001, Dollar General is on the fast track. However, the road to riches could have easily become the road to ruin had the retailer not exerted control over its inventory management","with annual revenue increase 17.0 % to 20.0 % consistently over the last threeyear and more than 2,500 new store open from 1998 through 2001 , dollar general be on the fast track . however , the road to rich could have easily become the road to ruin have the retailer not exert control over its inventory management",53,1
8528,Computers and IT,"Choosing to migrate to IP-based applications also means deciding whether terrestrial technologies such as frame relay, DSL or ""plain old telephone service"" (POTS) can provide the scalability, flexibility and high bandwidth required to support those applications, and whether these technologies can do so affordably. Each option has its tradeoffs. Also, in each case, retailers with nationwide chains have to deal with multiple last-mile service providers for service installation and network maintenance. Because of this, many retailers are selecting two-way satellite networking technology (frequently referred to as VSAT) as the technology of choice for always-on, nationwide, high-speed connectivity coupled with end-to-end network ownership and favorable economics. Enterprises are adopting VSAT platforms not only for emerging IP and Web-based applications, but also for mission-critical, front-office functions such as credit authorization and point-of-sale polling","choosing to migrate to ip-based applications also means deciding whetherterrestrial technologies such as frame relay, dsl or ""plain old telephone service"" (pots) can provide the scalability, flexibility and high bandwidth required to support those applications, and whether these technologies can do so affordably. each option has its tradeoffs. also, in each case, retailers with nationwide chains have to deal with multiple last-mile service providers for service installation and network maintenance. because of this, many retailers are selecting two-way satellite networking technology (frequently referred to as vsat) as the technology of choice for always-on, nationwide, high-speed connectivity coupled with end-to-end network ownership and favorable economics. enterprises are adopting vsat platforms not only for emerging ip and web-based applications, but also for mission-critical, front-office functions such as credit authorization and point-of-sale polling",130,1
8529,Computers and IT,"CPFR remains at the forefront of CIOs' minds, but a number of barriers, such as secretive corporate cultures and spotty data integrity, stand between retail organizations and true supply-chain collaboration. CIOs remain vexed at these obstacles, as was evidenced at a roundtable discussion by retail and consumer-goods IT leaders at the Retail Systems 2002 conference, held in Chicago by the consultancy MoonWatch Media Inc. , Newton Upper Falls, Mass. Other annoyances discussed by retail CIOs include poorly designed business processes and retail's poor image with the IT talent emerging from school into the job market","cpfr remain at the forefront of cio ' mind , but a number of barrier , such assecretive corporate culture and spotty data integrity , stand between retail organization and true supply-chain collaboration . cio remain vex at these obstacle , as be evidence at a roundtable discussion by retail and consumer-good it leader at the retail system 2002 conference , hold in chicago by the consultancy moonwatch medium inc. , newton upper fall , mass. other annoyance discuss by retail cio include poorly design business process and retail 's poor image with the it talent emerge from school into the job market",93,0.8
8530,Computers and IT,"The explosive growth in air traffic as well as the widespread adoption of Operations Research techniques in airline scheduling has given rise to tight flight schedules at major airports. An undesirable consequence of this is that a minor incident such as a delay in the arrival of a small number of flights can result in a chain reaction of events involving several flights and airports, causing disruption throughout the system. This paper reviews recent literature in the area of recovery from schedule disruptions. First we review how disturbances at a given airport could be handled, including the effects of runways and fixes. Then we study the papers on recovery from airline schedule perturbations, which involve adjustments in flight schedules, aircraft, and crew. The mathematical programming techniques used in ground holding are covered in some detail. We conclude the review with suggestions on how singular perturbation theory could play a role in analyzing disruptions to such highly sensitive schedules as those in the civil aviation industry","the explosive growth in air traffic as well as the widespread adoption ofoperation research technique in airline scheduling have give rise to tight flight schedule at major airport . an undesirable consequence of this be that a minor incident such as a delay in the arrival of a small number of flight can result in a chain reaction of event involve several flight and airport , cause disruption throughout the system . this paper review recent literature in the area of recovery from schedule disruption . first we review how disturbance at a give airport could be handle , include the effect of runway and fix . then we study the paper on recovery from airline schedule perturbation , which involve adjustment in flight schedule , aircraft , and crew . the mathematical programming technique use in ground holding be cover in some detail . we conclude the review with suggestion on how singular perturbation theory could play a role in analyze disruption to such highly sensitive schedule as those in the civil aviation industry",164,0.705882353
8531,Computers and IT,"A system for rigorous airline base schedule optimisation is described. The architecture of the system reflects the underlying problem structure. The architecture is hierarchical consisting of a master problem for logical aircraft schedule optimisation and a sub-problem for schedule evaluation. The sub-problem is made up of a number of component sub-problems including connection generation, passenger choice modelling, passenger traffic allocation by simulation and revenue and cost determination. Schedule optimisation is carried out by means of simulated annealing of flight networks. The operators for the simulated annealing process are feasibility preserving and form a complete set of operators","a system for rigorous airline base schedule optimisation be describe . thearchitecture of the system reflect the underlie problem structure . the architecture be hierarchical consist of a master problem for logical aircraft schedule optimisation and a sub-problem for schedule evaluation . the sub-problem be make up of a number of component sub-problem include connection generation , passenger choice modelling , passenger traffic allocation by simulation and revenue and cost determination . schedule optimisation be carry out by mean of simulated annealing of flight network . the operator for the simulated annealing process be feasibility preserve and form a complete set of operator",96,0.714285714
8532,Computers and IT,"Provides details of a successful application where the column generation algorithm was used to combine constraint programming and linear programming. In the past, constraint programming and linear programming were considered to be two competing technologies that solved similar types of problems. Both these technologies had their strengths and weaknesses. The paper shows that the two technologies can be combined together to extract the strengths of both these technologies. Details of a real-world application to optimize bus driver duties are given. This system was developed by ILOG for a major software house in Japan using ILOG-Solver and ILOG-CPLEX, constraint programming and linear programming C/C++ libraries","provide detail of a successful application where the column generation algorithm be use to combine constraint programming and linear programming . in the past , constraint programming and linear programming be consider to be two compete technology that solve similar type of problem . both these technology have their strength and weakness . the paper show that the two technology can be combine together to extract the strength of both these technology . detail of a real-world application to optimize bus driver duty be give . this system be develop by ilog for a major software house in japan use ilog-solver and ilog-cplex , constraint programming and linear programming c/c + + library",104,0.875
8533,Computers and IT,The Wedelin algorithm is a Lagrangian based heuristic that is being successfully used by Carmen Systems to solve large crew pairing problems within the airline industry. We extend the Wedelin approach by developing an implementation for personnel scheduling problems (also termed staff rostering problems) that exploits the special structure of these problems. We also introduce elastic constraint branching with the twin aims of improving the performance of our new approach and making it more column generation friendly. Numerical results show that our approach can outperform the commercial solver CPLEX on difficult commercial rostering problems,the wedelin algorithm is a lagrangian based heuristic that is being successfully used by carmen systems to solve large crew pairing problems within the airline industry. we extend the wedelin approach by developing an implementation for personnel scheduling problems (also termed staff rostering problems) that exploits the special structure of these problems. we also introduce elastic constraint branching with the twin aims of improving the performance of our new approach and making it more column generation friendly. numerical results show that our approach can outperform the commercial solver cplex on difficult commercial rostering problems,94,0.6
8534,Computers and IT,"We describe the formulation and development of a supply-chain optimisation model for Fletcher Challenge Paper Australasia (FCPA). This model, known as the paper industry value optimisation tool (PIVOT), is a large mixed integer program that finds an optimal allocation of supplier to mill, product to paper machine, and paper machine to customer, while at the same time modelling many of the supply chain details and nuances which are peculiar to FCPA. PIVOT has assisted FCPA in solving a number of strategic and tactical decision problems, and provided significant economic benefits for the company","we describe the formulation and development of a supply-chain optimisationmodel for fletcher challenge paper australasia (fcpa). this model, known as the paper industry value optimisation tool (pivot), is a large mixed integer program that finds an optimal allocation of supplier to mill, product to paper machine, and paper machine to customer, while at the same time modelling many of the supply chain details and nuances which are peculiar to fcpa. pivot has assisted fcpa in solving a number of strategic and tactical decision problems, and provided significant economic benefits for the company",92,0.777777778
8535,Computers and IT,"There is a new vision of the WWW - the semantic Web - that will dramatically improve Web-based services and products. It creates a setting where software agents perform everyday jobs for end-users. Deploying hierarchies, metadata, and structured vocabularies, the semantic Web expands basic Internet functions","there be a new vision of the www - the semantic web - that will dramaticallyimprove web-based service and product . it create a set where software agent perform everyday job for end-user . deploy hierarchy , metadata , and structured vocabulary , the semantic web expand basic internet function",45,1
8536,Computers and IT,"Train crew management involves the development of a duty timetable for each of the drivers (crew) to cover a given train timetable in a rail transport organization. This duty timetable is spread over a certain period, known as the roster planning horizon. Train crew management may arise either from the planning stage, when the total number of crew and crew distributions are to be determined, or from the operating stage when the number of crew at each depot is known as input data. In this paper, we are interested in train crew management in the planning stage. In the literature, train crew management is decomposed into two stages: crew scheduling and crew rostering which are solved sequentially. We propose an integrated optimization model to solve both crew scheduling and crew rostering. The model enables us to generate either cyclic rosters or non-cyclic rosters. Numerical experiments are carried out over data sets arising from a practical application","train crew management involves the development of a duty timetable for each ofthe drivers (crew) to cover a given train timetable in a rail transport organization. this duty timetable is spread over a certain period, known as the roster planning horizon. train crew management may arise either from the planning stage, when the total number of crew and crew distributions are to be determined, or from the operating stage when the number of crew at each depot is known as input data. in this paper, we are interested in train crew management in the planning stage. in the literature, train crew management is decomposed into two stages: crew scheduling and crew rostering which are solved sequentially. we propose an integrated optimization model to solve both crew scheduling and crew rostering. the model enables us to generate either cyclic rosters or non-cyclic rosters. numerical experiments are carried out over data sets arising from a practical application",155,0.7
8537,Computers and IT,"In Australia, cane transport is the largest unit cost in the manufacturing of raw sugar, making up around 35% of the total manufacturing costs. Producing efficient schedules for the cane railways can result in significant cost savings. The paper presents a study using constraint logic programming (CLP) to solve the cane transport scheduling problem. Tailored heuristic labelling order and constraints strategies are proposed and encouraging results of application to several test problems and one real-life case are presented. The preliminary results demonstrate that CLP can be used as an effective tool for solving the cane transport scheduling problem, with a potential decrease in development costs of the scheduling system. It can also be used as an efficient tool for rescheduling tasks which the existing cane transport scheduling system cannot perform well","in australia, cane transport is the largest unit cost in the manufacturing of raw sugar, making up around 35% of the total manufacturing costs. producing efficient schedules for the cane railways can result in significant cost savings. the paper presents a study using constraint logic programming (clp) to solve the cane transport scheduling problem. tailored heuristic labelling order and constraints strategies are proposed and encouraging results of application to several test problems and one real-life case are presented. the preliminary results demonstrate that clp can be used as an effective tool for solving the cane transport scheduling problem, with a potential decrease in development costs of the scheduling system. it can also be used as an efficient tool for rescheduling tasks which the existing cane transport scheduling system cannot perform well",131,0.625
8538,Computers and IT,"Motivated by a problem facing the Police Communication Centre in Auckland, New Zealand, we consider the setting of staffing levels in a call centre with priority customers. The choice of staffing level over any particular time period (e. g. , Monday from 8 am-9 am) relies on accurate arrival rate information. The usual method for identifying the arrival rate based on historical data can, in some cases, lead to considerable errors in performance estimates for a given staffing level. We explain why, identify three potential causes of the difficulty, and describe a method for detecting and addressing such a problem","motivated by a problem facing the police communication centre in auckland, newzealand, we consider the setting of staffing levels in a call centre with priority customers. the choice of staffing level over any particular time period (e.g., monday from 8 am-9 am) relies on accurate arrival rate information. the usual method for identifying the arrival rate based on historical data can, in some cases, lead to considerable errors in performance estimates for a given staffing level. we explain why, identify three potential causes of the difficulty, and describe a method for detecting and addressing such a problem",97,0.4
8539,Computers and IT,"The regional surveillance problem discussed involves formulating a flight route for an aircraft to scan a given geographical region. Aerial surveillance is conducted using a synthetic aperture radar device mounted on the aircraft to compose a complete, high-resolution image of the region. Two models for determining an optimised flight route are described, the first employing integer programming and the second, genetic algorithms. A comparison of the solution optimality in terms of the total distance travelled, and model efficiency of the two techniques in terms of their required CPU times, is made in order to identify the conditions under which it is appropriate to apply each model","the regional surveillance problem discuss involve formulate a flight routefor an aircraft to scan a give geographical region . aerial surveillance be conduct use a synthetic aperture radar device mount on the aircraft to compose a complete , high-resolution image of the region . two model for determine an optimised flight route be describe , the first employ integer programming and the second , genetic algorithm . a comparison of the solution optimality in term of the total distance travel , and model efficiency of the two technique in term of their require cpu time , be make in order to identify the condition under which it be appropriate to apply each model",105,0.833333333
8540,Computers and IT,"We consider a problem of delivery planning over multiple time periods. Deliveries must be made to customers having nominated demand in each time period. Demand must be met in each time period by use of some combination of inhomogeneous service providers. Each service provider has a different delivery capacity, different cost of delivery to each customer, a different utilisation requirement, and different rules governing the spread of deliveries in time. The problem is to plan deliveries so as to minimise overall costs, subject to demand being met and service rules obeyed. A natural integer programming model was found to be intractable, except on problems with loose demand constraints, with gaps between best lower bound and best feasible solution of up to 35. 1%, with an average of 15. 4% over the test data set. In all but the problem with loosest demand constraints, Cplex 6. 5 applied to this formulation failed to find the optimal solution before running out of memory. However a column generation approach improved the lower bound by between 0. 6% and 21. 9%, with an average of 9. 9%, and in all cases found the optimal solution at the root node, without requiring branching","we consider a problem of delivery planning over multiple time period . delivery must be make to customer have nominate demand in each time period . demand must be meet in each time period by use of some combination of inhomogeneous service provider . each service provider have a different delivery capacity , different cost of delivery to each customer , a different utilisation requirement , and different rule govern the spread of delivery in time . the problem be to plan delivery so as to minimize overall cost , subject to demand be meet and service rule obey . a natural integer programming model be find to be intractable , except on problem with loose demand constraint , with gap between best low bind and best feasible solution of up to 35.1 % , with an average of 15.4 % over the test data set . in all but the problem with loose demand constraint , cplex 6.5 apply to this formulation fail to find the optimal solution before run out of memory . however a column generation approach improve the low bind by between 0.6 % and 21.9 % , with an average of 9.9 % , and in all case find the optimal solution at the root node , without require branch",192,0.571428571
8541,Computers and IT,"Selection for superior clones is the most important aspect of sugar cane improvement programs, and is a long and expensive process. While studies have investigated different components of selection independently, there has not been a whole system approach to improve the process. This study observes the problem as an integrated system, where if one parameter changes the state of the whole system changes. A computer based stochastic simulation model that accurately represents the selection was developed. The paper describes the simulation model, showing its accuracy as well as how a combination of dynamic programming and branch and bound can be applied to the model to optimise the selection system, giving a new application of these techniques. The model can be directly applied to any region targeted by sugar cane breeding programs or to other clonally propagated crops","selection for superior clone be the most important aspect of sugar caneimprovement program , and be a long and expensive process . while study have investigate different component of selection independently , there have not be a whole system approach to improve the process . this study observe the problem as an integrated system , where if one parameter change the state of the whole system change . a computer base stochastic simulation model that accurately represent the selection be develop . the paper describe the simulation model , show its accuracy as well as how a combination of dynamic programming and branch and bind can be apply to the model to optimise the selection system , give a new application of these technique . the model can be directly apply to any region target by sugar cane breed program or to other clonally propagate crop",136,0.7
8542,Computers and IT,"Singapore Mass Rapid Transit (SMRT) operates two train lines with 83 kilometers of track and 48 stations. A total of 77 trains are in operation during peak hours and 41 during off-peak hours. We report on an optimization based approach to develop a computerized train-operator scheduling system that has been implemented at SMRT. The approach involves a bipartite matching algorithm for the generation of night duties and a tabu search algorithm for the generation of day duties. The system automates the train-operator scheduling process at SMRT and produces favorable schedules in comparison with the manual process. It is also able to handle the multiple objectives inherent in the crew scheduling system. While trying to minimize the system wide crew-related costs, the system is also able to address concern with respect to the number of split duties","singapore mass rapid transit (smrt) operates two train lines with 83 kilometers of track and 48 stations. a total of 77 trains are in operation during peak hours and 41 during off-peak hours. we report on an optimization based approach to develop a computerized train-operator scheduling system that has been implemented at smrt. the approach involves a bipartite matching algorithm for the generation of night duties and a tabu search algorithm for the generation of day duties. the system automates the train-operator scheduling process at smrt and produces favorable schedules in comparison with the manual process. it is also able to handle the multiple objectives inherent in the crew scheduling system. while trying to minimize the system wide crew-related costs, the system is also able to address concern with respect to the number of split duties",136,0.5
8543,Computers and IT,"Recently, various algebraic integer programming (IP) solvers have been proposed based on the theory of Grobner bases. The main difficulty of these solvers is the size of the Grobner bases generated. In algorithms proposed so far, large Grobner bases are generated by either introducing additional variables or by considering the generic IP problem IP/sub A, C/. Some improvements have been proposed such as Hosten and Sturmfels' method (GRIN) designed to avoid additional variables and Thomas' truncated Grobner basis method which computes the reduced Grobner basis for a specific IP problem IP/sub A, C/(b) (rather than its generalisation IPA, C). In this paper we propose a new algebraic algorithm for solving IP problems. The new algorithm, called Minimised Geometric Buchberger Algorithm, combines Hosten and Sturmfels' GRIN and Thomas' truncated Grobner basis method to compute the fundamental segments of an IP problem IP/sub A, C/ directly in its original space and also the truncated Grobner basis for a specific IP problem IP/sub A, C/ (b). We have carried out experiments to compare this algorithm with others such as the geometric Buchberger algorithm, the truncated geometric Buchberger algorithm and the algorithm in GRIN. These experiments show that the new algorithm offers significant performance improvement","recently, various algebraic integer programming (ip) solvers have been proposedbased on the theory of grobner bases. the main difficulty of these solvers is the size of the grobner bases generated. in algorithms proposed so far, large grobner bases are generated by either introducing additional variables or by considering the generic ip problem ip/sub a,c/. some improvements have been proposed such as hosten and sturmfels' method (grin) designed to avoid additional variables and thomas' truncated grobner basis method which computes the reduced grobner basis for a specific ip problem ip/sub a,c/(b) (rather than its generalisation ipa,c). in this paper we propose a new algebraic algorithm for solving ip problems. the new algorithm, called minimised geometric buchberger algorithm, combines hosten and sturmfels' grin and thomas' truncated grobner basis method to compute the fundamental segments of an ip problem ip/sub a,c/ directly in its original space and also the truncated grobner basis for a specific ip problem ip/sub a,c/ (b). we have carried out experiments to compare this algorithm with others such as the geometric buchberger algorithm, the truncated geometric buchberger algorithm and the algorithm in grin. these experiments show that the new algorithm offers significant performance improvement",195,0.5
8544,Computers and IT,"The predictor-corrector interior-point path-following algorithm is promising in solving multistage convex programming problems. Among many other general good features of this algorithm, especially attractive is that the algorithm allows the possibility to parallelise the major computations. The dynamic structure of the multistage problems specifies a block-tridiagonal system at each Newton step of the algorithm. A wrap-around permutation is then used to implement the parallel computation for this step","the predictor-corrector interior-point path-following algorithm be promising insolv multistage convex programming problem . among many other general good feature of this algorithm , especially attractive be that the algorithm allow the possibility to parallelise the major computation . the dynamic structure of the multistage problem specify a block-tridiagonal system at each newton step of the algorithm . a wrap-around permutation be then use to implement the parallel computation for this step",67,0.875
8545,Computers and IT,"In a given project network, execution of each activity in normal duration requires utilization of certain resources. If faster execution of an activity is desired then additional resources at extra cost would be required. Given a project network, the cost structure for each activity and a planning horizon, the project compression problem is concerned with the determination of optimal schedule of performing each activity while satisfying given restrictions and minimizing the total cost of project execution. The paper considers the project compression problem with time dependent cost structure for each activity. The planning horizon is divided into several regular time intervals over which the cost structure of an activity may vary. But the cost structure of the activities remains the same within a time interval. The objective is to find an optimal project schedule minimizing the total project cost. We present a mathematical model for this problem, develop some heuristics and an exact branch and bound algorithm. Using simulated problems we provide an insight into the computational performances of heuristics and the branch and bound algorithm","in a give project network , execution of each activity in normal durationrequire utilization of certain resource . if fast execution of an activity be desire then additional resource at extra cost would be require . give a project network , the cost structure for each activity and a planning horizon , the project compression problem be concern with the determination of optimal schedule of perform each activity while satisfy give restriction and minimize the total cost of project execution . the paper consider the project compression problem with time dependent cost structure for each activity . the planning horizon be divide into several regular time interval over which the cost structure of an activity may vary . but the cost structure of the activity remain the same within a time interval . the objective be to find an optimal project schedule minimize the total project cost . we present a mathematical model for this problem , develop some heuristic and an exact branch and bind algorithm . use simulated problem we provide an insight into the computational performance of heuristic and the branch and bind algorithm",175,0.75
8546,Computers and IT,"There has been a great deal of continuing discussion concerning the seemingly unbridgeable gap between so much of the research produced by business school professors and the needs of the business people who, ideally, would use it. Here, we examine this gap and suggest a model for bridging it. We sample four groups of people, business school academics (professors), deans of business schools, executive MBA students/recent graduates, and senior business executives. Each group rates 44 different (potential) properties of exemplary research. We analyze within-group differences, and more meaningfully, between-group differences. We then offer commentary on the results and use the results to develop the aforementioned suggestions for bridging the gap we find","there has been a great deal of continuing discussion concerning the seeminglyunbridgeable gap between so much of the research produced by business school professors and the needs of the business people who, ideally, would use it. here, we examine this gap and suggest a model for bridging it. we sample four groups of people, business school academics (professors), deans of business schools, executive mba students/recent graduates, and senior business executives. each group rates 44 different (potential) properties of exemplary research. we analyze within-group differences, and more meaningfully, between-group differences. we then offer commentary on the results and use the results to develop the aforementioned suggestions for bridging the gap we find",111,0.6875
8547,Computers and IT,"Studies a single machine scheduling problem to minimize the weighted number of early and tardy jobs with a common due window. There are n non-preemptive and simultaneously available jobs. Each job will incur an early (tardy) penalty if it is early (tardy) with respect to the common due window under a given schedule. The window size is a given parameter but the window location is a decision variable. The objective of the problem is to find a schedule that minimizes the weighted number of early and tardy jobs and the location penalty. We show that the problem is NP-complete in the ordinary sense and develop a dynamic programming based pseudo-polynomial algorithm. We conduct computational experiments, the results of which show that the performance of the dynamic algorithm is very good in terms of memory requirement and CPU time. We also provide polynomial time algorithms for two special cases","studies a single machine scheduling problem to minimize the weighted number of early and tardy jobs with a common due window. there are n non-preemptive and simultaneously available jobs. each job will incur an early (tardy) penalty if it is early (tardy) with respect to the common due window under a given schedule. the window size is a given parameter but the window location is a decision variable. the objective of the problem is to find a schedule that minimizes the weighted number of early and tardy jobs and the location penalty. we show that the problem is np-complete in the ordinary sense and develop a dynamic programming based pseudo-polynomial algorithm. we conduct computational experiments, the results of which show that the performance of the dynamic algorithm is very good in terms of memory requirement and cpu time. we also provide polynomial time algorithms for two special cases",148,0.777777778
8548,Computers and IT,"The one facility one commodity network design problem (OFOC) with nonnegative flow costs considers the problem of sending d units of flow from a source to a destination where arc capacity is purchased in batches of C units. The two facility problem (TFOC) is similar, but capacity can be purchased either in batches of C units or one unit. Flow costs are zero. These problems are known to be NP-hard. We describe an exact O(n/sup 3/3/sup n/) algorithm for these problems based on the repeated use of a bipartite matching algorithm. We also present a better lower bound of Omega (n/sup 2k*/) for an earlier Omega (n/sup 2k/) algorithm described in the literature where k = [d/C] and k* = min{k, [(n 2)/2]}. The matching algorithm is faster than this one for k or= [(n - 2)/2]. Finally, we provide another reformulation of the problem that is quasi integral. This property could be useful in designing a modified version of the simplex method to solve the problem using a sequence of pivots with integer extreme solutions, referred to as the integral simplex method in the literature","the one facility one commodity network design problem (ofoc) with nonnegativeflow costs considers the problem of sending d units of flow from a source to a destination where arc capacity is purchased in batches of c units. the two facility problem (tfoc) is similar, but capacity can be purchased either in batches of c units or one unit. flow costs are zero. these problems are known to be np-hard. we describe an exact o(n/sup 3/3/sup n/) algorithm for these problems based on the repeated use of a bipartite matching algorithm. we also present a better lower bound of omega (n/sup 2k*/) for an earlier omega (n/sup 2k/) algorithm described in the literature where k = [d/c] and k* = min{k, [(n 2)/2]}. the matching algorithm is faster than this one for k >or= [(n - 2)/2]. finally, we provide another reformulation of the problem that is quasi integral. this property could be useful in designing a modified version of the simplex method to solve the problem using a sequence of pivots with integer extreme solutions, referred to as the integral simplex method in the literature",185,0.454545455
8549,Computers and IT,"The use and acquisition of information is a key part of the way any business makes money. Data mining technologies provide greater insight into how this information can be better used and more effectively acquired. Steven Kudyba, an expert in the field of data mining technologies, shares his expertise in an interview","the use and acquisition of information be a key part of the way any businessmake money . data mining technology provide great insight into how this information can be better use and more effectively acquire . steven kudyba , an expert in the field of data mining technology , share his expertise in an interview",51,0.333333333
8550,Computers and IT,"I propose in this paper to show, through a number of case studies, that videoconferencing is user-friendly, cost-effective, time-effective and life-enhancing for people of all ages and abilities and that it requires only a creative and imaginative approach to unlock its potential. I believe that these benefits need not, and should not, be restricted to the education sector. My examples will range from simple storytelling, through accessing international experts, professional development and distance learning in a variety of forms, to the use of videoconferencing for virtual meetings and planning sessions. In some cases, extracts from the reactions and responses of the participants will be included to illustrate the impact of the medium","i propose in this paper to show , through a number of case study , thatvideoconferenc be user-friendly , cost-effective , time-effective and life-enhancing for people of all age and ability and that it require only a creative and imaginative approach to unlock its potential . i believe that these benefit need not , and should not , be restrict to the education sector . my example will range from simple storytelling , through access international expert , professional development and distance learning in a variety of form , to the use of videoconferencing for virtual meeting and plan session . in some case , extract from the reaction and response of the participant will be include to illustrate the impact of the medium",111,1
8551,Computers and IT,"If you know how to surf the World Wide Web, have used email before, and can learn how to send an email attachment, then learning how to interact in an online course should not be difficult at all. In a way to find out, I decided to offer two identical courses, one of which would be offered online and the other the ""traditional way"". I wanted to see how students would fare with identical material provided in each course. I wanted their anonymous feedback, when the course was over","if you know how to surf the world wide web , have use email before , and can learn how to send an email attachment , then learn how to interact in an online course should not be difficult at all . in a way to find out , i decide to offer two identical course , one of which would be offer online and the other the `` traditional way '' . i want to see how student would fare with identical material provide in each course . i want their anonymous feedback , when the course be over",89,0
8552,Computers and IT,"A concept of how the World Wide Web (WWW) and its associated technologies can be used to manage construction projects has been recognized by practitioners in the construction industry for quite sometime. This concept is often referred to as a Web-Based Project Management System (WPMS). It promises, to enhance construction project documentation and control, and to revolutionize the way construction project teams process and transmit project information. WPMS is an electronic project-management system conducted through the Internet. The system provides a centralized, commonly accessible, reliable means of transmitting and storing project information. Project information is stored on the server and a standard Web browser is used as the gateway to exchange this information, eliminating geographic and hardware platforms boundary","a concept of how the world wide web (www) and its associated technologies can be used to manage construction projects has been recognized by practitioners in the construction industry for quite sometime. this concept is often referred to as a web-based project management system (wpms). it promises, to enhance construction project documentation and control, and to revolutionize the way construction project teams process and transmit project information. wpms is an electronic project-management system conducted through the internet. the system provides a centralized, commonly accessible, reliable means of transmitting and storing project information. project information is stored on the server and a standard web browser is used as the gateway to exchange this information, eliminating geographic and hardware platforms boundary",119,0.571428571
8553,Computers and IT,"This paper examines the real constraints to the expansion of all encumbering and all pervasive information technology in our contemporary society. Perhaps the U. S. Internet infrastructure is the most appropriate to examine since it is U. S. technology that has led the world into the Internet age. In this context, this paper reviews the state of the U. S. Internet backbone that will lead us into information society of the future by facilitating massive data transmission","this paper examine the real constraint to the expansion of all encumber and all pervasive information technology in our contemporary society . perhaps the u.s. internet infrastructure be the most appropriate to examine since it be u.s. technology that have lead the world into the internet age . in this context , this paper review the state of the u.s. internet backbone that will lead us into information society of the future by facilitate massive data transmission",74,0.2
8554,Computers and IT,A recent Gartner report identifies Hummingbird in the first wave of vendors as an early example of convergence in the 'smart enterprise suite' market. We spoke to Hummingbird's Marketing Director for Northern Europe,a recent gartner report identify hummingbird in the first wave of vendor asan early example of convergence in the ` smart enterprise suite ' market . we speak to hummingbird 's marketing director for northern europe,32,0.2
8555,Computers and IT,"The ""multi-channel content delivery"" model (MCCD) might be a new concept to you, but it is already beginning to replace traditional methods of business communications, print and content delivery, argues Darren Atkinson, CTO, FormScape","the ""multi-channel content delivery"" model (mccd) might be a new concept toyou, but it is already beginning to replace traditional methods of business communications, print and content delivery, argues darren atkinson, cto, formscape",33,0.25
8556,Computers and IT,"Customization is a crucial, lengthy, and costly aspect in the successful implementation of ERP systems, and has, accordingly, become a major specialty of many vendors and consulting companies. The study examines how such companies can increase their clients' perception of engagement success through increased client trust that is brought about through responsive and dependable customization. Survey data from ERP customization clients show that, as hypothesized, clients' trust influenced their perception of engagement success with the company. The data also show that clients' trust in the customization company was increased when the company behaved in accordance with client expectations by being responsive, and decreased when the company behaved in a manner that contradicted these expectations by not being dependable. Responses to an open-ended question addendum attached to the survey corroborated the importance of responsiveness and dependability. Implications for customization companies and research on trust are discussed","customization be a crucial , lengthy , and costly aspect in the successful implementation of erp system , and have , accordingly , become a major specialty of many vendor and consult company . the study examine how such company can increase their client ' perception of engagement success through increase client trust that be bring about through responsive and dependable customization . survey data from erp customization client show that , as hypothesize , client ' trust influence their perception of engagement success with the company . the data also show that client ' trust in the customization company be increase when the company behave in accordance with client expectation by be responsive , and decrease when the company behave in a manner that contradict these expectation by not be dependable . response to an open-ended question addendum attach to the survey corroborate the importance of responsiveness and dependability . implication for customization company and research on trust be discuss",145,0.583333333
8557,Computers and IT,"Accredited imaging qualifications, hot on the heels of Microsoft, Cisco and others, are taking off in the USA. Dave Tyler looks at the CDIA+ qualification that looks likely to become the exam of choice for the DM industry","accredit imaging qualification , hot on the heel of microsoft , cisco andother , be take off in the usa . dave tyler look at the cdia + qualification that look likely to become the exam of choice for the dm industry",37,0.666666667
8558,Computers and IT,"NHS spending will rise from Pounds 65. 4bn in 2002 to Pounds 87. 2bn in 2006, and by 2008, spending will total Pounds 105. 6bn. David Tyler looks at how the health sector is already beginning to exploit IT, and particularly document management, to improve service and cut costs","nh spend will rise from pound 65.4 bn in 2002 to pound 87.2 bn in 2006 , andby 2008 , spending will total pound 105.6 bn . david tyler look at how the health sector be already begin to exploit it , and particularly document management , to improve service and cut cost",45,0.5
8559,Computers and IT,"A three-dimensional (3D) lumped-parameter model of a powered wheelchair was created to aid the development of the Rocket prototype wheelchair and to help explore the effect of innovative design features on its stability. The model was developed using simulation software, specifically Working Model 3D. The accuracy of the model was determined by comparing both its static stability angles and dynamic behavior as it passed down a 4. 8-cm (1. 9"") road curb at a heading of 45 degrees with the performance of the actual wheelchair. The model's predictions of the static stability angles in the forward, rearward, and lateral directions were within 9. 3, 7. 1, and 3. 8% of the measured values, respectively. The average absolute error in the predicted position of the wheelchair as it moved down the curb was 2. 2 cm/m (0. 9"" per 3'3"") traveled. The accuracy was limited by the inability to model soft bodies, the inherent difficulties in modeling a statically indeterminate system, and the computing time. Nevertheless, it was found to be useful in investigating the effect of eight design alterations on the lateral stability of the wheelchair. Stability was quantified by determining the static lateral stability angles and the maximum height of a road curb over which the wheelchair could successfully drive on a diagonal heading. The model predicted that the stability was more dependent on the configuration of the suspension system than on the dimensions and weight distribution of the wheelchair. Furthermore, for the situations and design alterations studied, predicted improvements in static stability were not correlated with improvements in dynamic stability","a three-dimensional (3d) lumped-parameter model of a powered wheelchair was created to aid the development of the rocket prototype wheelchair and to help explore the effect of innovative design features on its stability. the model was developed using simulation software, specifically working model 3d. the accuracy of the model was determined by comparing both its static stability angles and dynamic behavior as it passed down a 4.8-cm (1.9"") road curb at a heading of 45 degrees with the performance of the actual wheelchair. the model's predictions of the static stability angles in the forward, rearward, and lateral directions were within 9.3, 7.1, and 3.8% of the measured values, respectively. the average absolute error in the predicted position of the wheelchair as it moved down the curb was 2.2 cm/m (0.9"" per 3'3"") traveled. the accuracy was limited by the inability to model soft bodies, the inherent difficulties in modeling a statically indeterminate system, and the computing time. nevertheless, it was found to be useful in investigating the effect of eight design alterations on the lateral stability of the wheelchair. stability was quantified by determining the static lateral stability angles and the maximum height of a road curb over which the wheelchair could successfully drive on a diagonal heading. the model predicted that the stability was more dependent on the configuration of the suspension system than on the dimensions and weight distribution of the wheelchair. furthermore, for the situations and design alterations studied, predicted improvements in static stability were not correlated with improvements in dynamic stability",255,0.4
8560,Computers and IT,"The author reviews the history of medical image computing at his institute, summarizes the achievements, sketches some of the difficulties encountered, and draws conclusions that might be of interest especially to people new to the field. The origin and history section provides a chronology of this work, emphasizing the milestones reached during the past three decades. In accordance with the author's group's focus on imaging, the paper is accompanied by many pictures, some of which, he thinks, are of historical value","the author review the history of medical image computing at his institute , summarize the achievement , sketch some of the difficulty encounter , and draw conclusion that might be of interest especially to people new to the field . the origin and history section provide a chronology of this work , emphasize the milestone reach during the past three decade . in accordance with the author 's group 's focus on imaging , the paper be accompany by many picture , some of which , he think , be of historical value",81,0.285714286
8561,Computers and IT,"In this paper, errors and discrepancies in the subject paper [Cincotti et al. (2002)] are highlighted. A comment, concerning the axial resolution associated to the adopted processing procedure is also reported","in this paper, errors and discrepancies in the subject paper [cincotti et al. (2002)] are highlighted. a comment, concerning the axial resolution associated to the adopted processing procedure is also reported",31,0.2
8562,Computers and IT,"For pt. I see ibid. , vol. 21, no. 7, p. 823-8 (2002). Microwave-induced thermoacoustic tomography (TAT) in a cylindrical configuration is developed to image biological tissue. Thermoacoustic signals are acquired by scanning a flat ultrasonic transducer. Using a new expansion of a spherical wave in cylindrical coordinates, we apply the Fourier and Hankel transforms to TAT and obtain an exact frequency-domain reconstruction method. The effect of discrete spatial sampling on image quality is analyzed. An aliasing-proof reconstruction method is proposed. Numerical and experimental results are included","for pt. i see ibid., vol. 21, no. 7, p. 823-8 (2002). microwave-induced thermoacoustic tomography (tat) in a cylindrical configuration is developed to image biological tissue. thermoacoustic signals are acquired by scanning a flat ultrasonic transducer. using a new expansion of a spherical wave in cylindrical coordinates, we apply the fourier and hankel transforms to tat and obtain an exact frequency-domain reconstruction method. the effect of discrete spatial sampling on image quality is analyzed. an aliasing-proof reconstruction method is proposed. numerical and experimental results are included",86,0.5
8563,Computers and IT,We report an exact and fast Fourier-domain reconstruction algorithm for thermoacoustic tomography in a planar configuration assuming thermal confinement and constant acoustic speed. The effects of the finite size of the detector and the finite length of the excitation pulse are explicitly included in the reconstruction algorithm. The algorithm is numerically and experimentally verified. We also demonstrate that the blurring caused by the finite size of the detector surface is the primary limiting factor on the resolution and that it can be compensated for by deconvolution,we report an exact and fast fourier-domain reconstruction algorithm for thermoacoustic tomography in a planar configuration assume thermal confinement and constant acoustic speed . the effect of the finite size of the detector and the finite length of the excitation pulse be explicitly include in the reconstruction algorithm . the algorithm be numerically and experimentally verify . we also demonstrate that the blur cause by the finite size of the detector surface be the primary limit factor on the resolution and that it can be compensate for by deconvolution,86,0.642857143
8564,Computers and IT,"Reconstruction-based microwave-induced thermoacoustic tomography in a spherical configuration is presented. Thermoacoustic waves from biological tissue samples excited by microwave pulses are measured by a wide-band unfocused ultrasonic transducer, which is set on a spherical surface enclosing the sample. Sufficient data are acquired from different directions to reconstruct the microwave absorption distribution. An exact reconstruction solution is derived and approximated to a modified backprojection algorithm. Experiments demonstrate that the reconstructed images agree well with the original samples. The spatial resolution of the system reaches 0. 5 mm","reconstruction-based microwave-induced thermoacoustic tomography in a spherical configuration be present . thermoacoustic wave from biological tissue sample excite by microwave pulse be measure by a wide-band unfocused ultrasonic transducer , which be set on a spherical surface enclose the sample . sufficient data be acquire from different direction to reconstruct the microwave absorption distribution . an exact reconstruction solution be derive and approximate to a modify backprojection algorithm . experiment demonstrate that the reconstructed image agree well with the original sample . the spatial resolution of the system reach 0.5 mm",85,0.583333333
8565,Computers and IT,"One of the most difficult tasks in a job shop manufacturing environment is to balance schedule and capacity in an ongoing basis. MRP systems are commonly used for scheduling, although their inability to deal with capacity constraints adequately is a severe drawback. In this study, we show that material requirements planning can be done more effectively in a job shop environment using a resource constrained project scheduling model. The proposed model augments MRP models by incorporating capacity constraints and using variable lead time lengths. The efficacy of this approach is tested on MRP systems by comparing the inventory carrying costs and resource allocation of the solutions obtained by the proposed model to those obtained by using a traditional MRP model. In general, it is concluded that the proposed model provides improved schedules with considerable reductions in inventory carrying costs","one of the most difficult task in a job shop manufacture environment be to balance schedule and capacity in an ongoing basis . mrp system be commonly use for scheduling , although their inability to deal with capacity constraint adequately be a severe drawback . in this study , we show that material requirement plan can be do more effectively in a job shop environment use a resource constrain project scheduling model . the propose model augment mrp model by incorporate capacity constraint and use variable lead time length . the efficacy of this approach be test on mrp system by compare the inventory carry cost and resource allocation of the solution obtain by the propose model to those obtain by use a traditional mrp model . in general , it be conclude that the propose model provide improve schedule with considerable reduction in inventory carry cost",139,0.8
8566,Computers and IT,"Dual nature of mass multi-agent systems (mMAS) emerging as an internal discord of two spheres - micro (virtual) consisting of agents and their internal phenomena, and macro arising at the interface to the real world $stems the necessity of a new approach to analysis, design and utilisation of such systems. Based on the concept of VR decomposition, the problem of management of such systems is discussed. As a sub-type that makes mMAS closer to the application sphere, an evolutionary multi-agent system (EMAS) is proposed. EMAS combines features of mMAS with advantages of an evolutionary model of computation. As an illustration of this consideration two particular EMAS are presented, which allow us to obtain promising results in the fields of multiobjective optimisation and time-series prediction, and thus justify the approach","dual nature of mass multi-agent systems (mmas) emerging as an internal discordof two spheres - micro (virtual) consisting of agents and their internal phenomena, and macro arising at the interface to the real world $stems the necessity of a new approach to analysis, design and utilisation of such systems. based on the concept of vr decomposition, the problem of management of such systems is discussed. as a sub-type that makes mmas closer to the application sphere, an evolutionary multi-agent system (emas) is proposed. emas combines features of mmas with advantages of an evolutionary model of computation. as an illustration of this consideration two particular emas are presented, which allow us to obtain promising results in the fields of multiobjective optimisation and time-series prediction, and thus justify the approach",128,0.25
8567,Computers and IT,"We compute unmeasured cone-beam projections from projections measured by a third-generation helical volumetric computed tomography system by solving a characteristic problem for an ultrahyperbolic differential equation [John (1938)]. By working in the Fourier domain, we convert the second-order PDE into a family of first-order ordinary differential equations. A simple first-order integration is used to solve the ODES","we compute unmeasured cone-beam projections from projections measured by athird-generation helical volumetric computed tomography system by solving a characteristic problem for an ultrahyperbolic differential equation [john (1938)]. by working in the fourier domain, we convert the second-order pde into a family of first-order ordinary differential equations. a simple first-order integration is used to solve the odes",56,0.454545455
8568,Computers and IT,"The quantitative estimation of regional cardiac deformation from three-dimensional (3-D) image sequences has important clinical implications for the assessment of viability in the heart wall. We present here a generic methodology for estimating soft tissue deformation which integrates image-derived information with biomechanical models, and apply it to the problem of cardiac deformation estimation. The method is image modality independent. The images are segmented interactively and then initial correspondence is established using a shape-tracking approach. A dense motion field is then estimated using a transversely isotropic, linear-elastic model, which accounts for the muscle fiber directions in the left ventricle. The dense motion field is in turn used to calculate the deformation of the heart wall in terms of strain in cardiac specific directions. The strains obtained using this approach in open-chest dogs before and after coronary occlusion, exhibit a high correlation with strains produced in the same animals using implanted markers. Further, they show good agreement with previously published results in the literature. This proposed method provides quantitative regional 3-D estimates of heart deformation","the quantitative estimation of regional cardiac deformation from three-dimensional (3-d) image sequences has important clinical implications for the assessment of viability in the heart wall. we present here a generic methodology for estimating soft tissue deformation which integrates image-derived information with biomechanical models, and apply it to the problem of cardiac deformation estimation. the method is image modality independent. the images are segmented interactively and then initial correspondence is established using a shape-tracking approach. a dense motion field is then estimated using a transversely isotropic, linear-elastic model, which accounts for the muscle fiber directions in the left ventricle. the dense motion field is in turn used to calculate the deformation of the heart wall in terms of strain in cardiac specific directions. the strains obtained using this approach in open-chest dogs before and after coronary occlusion, exhibit a high correlation with strains produced in the same animals using implanted markers. further, they show good agreement with previously published results in the literature. this proposed method provides quantitative regional 3-d estimates of heart deformation",173,0.4
8569,Computers and IT,"This paper reports on the clinical application of a system for recovering the time-varying three-dimensional (3-D) left-ventricular (LV) shape from multiview X-ray cineangiocardiograms. Considering that X-ray cineangiocardiography is still commonly employed in clinical cardiology and computational costs for 3-D recovery and visualization are rapidly decreasing, it is meaningful to develop a clinically applicable system for 3-D LV shape recovery from X-ray cineangiocardiograms. The system is based on a previously reported closed-surface method of shape recovery from two-dimensional occluding contours with multiple views. To apply the method to ""real"" LV cineangiocardiograms, user-interactive systems were implemented for preprocessing, including detection of LV contours, calibration of the imaging geometry, and setting of the LV model coordinate system. The results for three real LV angiographic image sequences are presented, two with fixed multiple views (using supplementary angiography) and one with rotating views. 3-D reconstructions utilizing different numbers of views were compared and evaluated in terms of contours manually traced by an experienced radiologist. The performance of the preprocesses was also evaluated, and the effects of variations in user-specified parameters on the final 3-D reconstruction results were shown to be sufficiently small. These experimental results demonstrate the potential usefulness of combining multiple views for 3-D recovery from ""real"" LV cineangiocardiograms","this paper reports on the clinical application of a system for recovering the time-varying three-dimensional (3-d) left-ventricular (lv) shape from multiview x-ray cineangiocardiograms. considering that x-ray cineangiocardiography is still commonly employed in clinical cardiology and computational costs for 3-d recovery and visualization are rapidly decreasing, it is meaningful to develop a clinically applicable system for 3-d lv shape recovery from x-ray cineangiocardiograms. the system is based on a previously reported closed-surface method of shape recovery from two-dimensional occluding contours with multiple views. to apply the method to ""real"" lv cineangiocardiograms, user-interactive systems were implemented for preprocessing, including detection of lv contours, calibration of the imaging geometry, and setting of the lv model coordinate system. the results for three real lv angiographic image sequences are presented, two with fixed multiple views (using supplementary angiography) and one with rotating views. 3-d reconstructions utilizing different numbers of views were compared and evaluated in terms of contours manually traced by an experienced radiologist. the performance of the preprocesses was also evaluated, and the effects of variations in user-specified parameters on the final 3-d reconstruction results were shown to be sufficiently small. these experimental results demonstrate the potential usefulness of combining multiple views for 3-d recovery from ""real"" lv cineangiocardiograms",205,0.461538462
8570,Computers and IT,"For pt. I see ibid. , vol. 21, no. 7, p. 755-63 (2002). Image error analysis of a diffuse near-infrared tomography (NIR) system has been carried out on simulated data using a statistical approach described in pt. I of this paper (Pogue et al. , 2002). The methodology is used here with experimental data acquired on phantoms with a prototype imaging system intended for characterizing breast tissue. Results show that imaging performance is not limited by random measurement error, but rather by calibration issues. The image error over the entire field of view is generally not minimized when an accurate homogeneous estimate of the phantom properties is available; however, local image error over a target region of interest (ROI) is reduced. The image reconstruction process which includes a Levenberg-Marquardt style regularization provides good minimization of the objective function, yet its reduction is not always correlated with an overall image error decrease. Minimization of the bias in an ROI which contains localized changes in the optical properties can be achieved through five to nine iterations of the algorithm. Precalibration of the algorithm through statistical evaluation of phantom studies may provide a better measure of the image accuracy than that implied by minimization of the standard objective function","for pt. i see ibid., vol. 21, no. 7, p. 755-63 (2002). image error analysis of a diffuse near-infrared tomography (nir) system has been carried out on simulated data using a statistical approach described in pt. i of this paper (pogue et al., 2002). the methodology is used here with experimental data acquired on phantoms with a prototype imaging system intended for characterizing breast tissue. results show that imaging performance is not limited by random measurement error, but rather by calibration issues. the image error over the entire field of view is generally not minimized when an accurate homogeneous estimate of the phantom properties is available; however, local image error over a target region of interest (roi) is reduced. the image reconstruction process which includes a levenberg-marquardt style regularization provides good minimization of the objective function, yet its reduction is not always correlated with an overall image error decrease. minimization of the bias in an roi which contains localized changes in the optical properties can be achieved through five to nine iterations of the algorithm. precalibration of the algorithm through statistical evaluation of phantom studies may provide a better measure of the image accuracy than that implied by minimization of the standard objective function",204,0.384615385
8571,Computers and IT,"Near-infrared (NIR) diffuse tomography is an emerging method for imaging the interior of tissues to quantify concentrations of hemoglobin and exogenous chromophores noninvasively in vivo. It often exploits an optical diffusion model-based image reconstruction algorithm to estimate spatial property values from measurements of the light flux at the surface of the tissue. In this study, mean-squared error (MSE) over the image is used to evaluate methods for regularizing the ill-posed inverse image reconstruction problem in NIR tomography. Estimates of image bias and image standard deviation were calculated based upon 100 repeated reconstructions of a test image with randomly distributed noise added to the light flux measurements. It was observed that the bias error dominates at high regularization parameter values while variance dominates as the algorithm is allowed to approach the optimal solution. This optimum does not necessarily correspond to the minimum projection error solution, but typically requires further iteration with a decreasing regularization parameter to reach the lowest image error. Increasing measurement noise causes a need to constrain the minimum regularization parameter to higher values in order to achieve a minimum in the overall image MSE","near-infrared (nir) diffuse tomography is an emerging method for imaging the interior of tissues to quantify concentrations of hemoglobin and exogenous chromophores noninvasively in vivo. it often exploits an optical diffusion model-based image reconstruction algorithm to estimate spatial property values from measurements of the light flux at the surface of the tissue. in this study, mean-squared error (mse) over the image is used to evaluate methods for regularizing the ill-posed inverse image reconstruction problem in nir tomography. estimates of image bias and image standard deviation were calculated based upon 100 repeated reconstructions of a test image with randomly distributed noise added to the light flux measurements. it was observed that the bias error dominates at high regularization parameter values while variance dominates as the algorithm is allowed to approach the optimal solution. this optimum does not necessarily correspond to the minimum projection error solution, but typically requires further iteration with a decreasing regularization parameter to reach the lowest image error. increasing measurement noise causes a need to constrain the minimum regularization parameter to higher values in order to achieve a minimum in the overall image mse",186,0
8572,Computers and IT,"This paper presents a new method of knowledge gathering for decision support in image understanding based on information extracted from the dynamics of saccadic eye movements. The framework involves the construction of a generic image feature extraction library, from which the feature extractors that are most relevant to the visual assessment by domain experts are determined automatically through factor analysis. The dynamics of the visual search are analyzed by using the Markov model for providing training information to novices on how and where to look for image features. The validity of the framework has been evaluated in a clinical scenario whereby the pulmonary vascular distribution on Computed Tomography images was assessed by experienced radiologists as a potential indicator of heart failure. The performance of the system has been demonstrated by training four novices to follow the visual assessment behavior of two experienced observers. In all cases, the accuracy of the students improved from near random decision making (33%) to accuracies ranging from 50% to 68%","this paper presents a new method of knowledge gathering for decision support inimage understanding based on information extracted from the dynamics of saccadic eye movements. the framework involves the construction of a generic image feature extraction library, from which the feature extractors that are most relevant to the visual assessment by domain experts are determined automatically through factor analysis. the dynamics of the visual search are analyzed by using the markov model for providing training information to novices on how and where to look for image features. the validity of the framework has been evaluated in a clinical scenario whereby the pulmonary vascular distribution on computed tomography images was assessed by experienced radiologists as a potential indicator of heart failure. the performance of the system has been demonstrated by training four novices to follow the visual assessment behavior of two experienced observers. in all cases, the accuracy of the students improved from near random decision making (33%) to accuracies ranging from 50% to 68%",164,0
8573,Computers and IT,"Traditional quantitative coronary angiography is performed on two-dimensional (2-D) projection views. These views are chosen by the angiographer to minimize vessel overlap and foreshortening. With 2-D projection views that are acquired in this nonstandardized fashion, however, there is no way to know or estimate how much error occurs in the QCA process. Furthermore, coronary arteries possess a curvilinear shape and undergo a cyclical deformation due to their attachment to the myocardium. Therefore, it is necessary to obtain three-dimensional (3-D) information to best describe and quantify the dynamic curvilinear nature of the human coronary artery. Using a patient-specific 3-D coronary reconstruction algorithm and routine angiographic images, a new technique is proposed to describe: (1) the curvilinear nature of 3-D coronary arteries and intracoronary devices; (2) the magnitude of the arterial deformation caused by intracoronary devices and due to heart motion; and (3) optimal view(s) with respect to the desired ""pathway"" for delivering intracoronary devices","traditional quantitative coronary angiography is performed on two-dimensional (2-d) projection views. these views are chosen by the angiographer to minimize vessel overlap and foreshortening. with 2-d projection views that are acquired in this nonstandardized fashion, however, there is no way to know or estimate how much error occurs in the qca process. furthermore, coronary arteries possess a curvilinear shape and undergo a cyclical deformation due to their attachment to the myocardium. therefore, it is necessary to obtain three-dimensional (3-d) information to best describe and quantify the dynamic curvilinear nature of the human coronary artery. using a patient-specific 3-d coronary reconstruction algorithm and routine angiographic images, a new technique is proposed to describe: (1) the curvilinear nature of 3-d coronary arteries and intracoronary devices; (2) the magnitude of the arterial deformation caused by intracoronary devices and due to heart motion; and (3) optimal view(s) with respect to the desired ""pathway"" for delivering intracoronary devices",153,0
8574,Computers and IT,"Multispectral imaging technologies are satisfying the need for a ""persistent"" look at the battlefield. We highlight the need to persistently monitor a battlefield to determine exactly who and what is there. For example, infrared imaging can be used to expose the fuel status of an aircraft on the runway. A daytime, visible-spectrum image of the same aircraft would offer information about external details, such as the plane's markings and paint scheme. A dual-band camera enables precision image registration by fusion and frequently yields more information than is possible by evaluating the images separately","multispectral imaging technology be satisfy the need for a `` persistent `` look at the battlefield . we highlight the need to persistently monitor a battlefield to determine exactly who and what be there . for example , infrared imaging can be use to expose the fuel status of an aircraft on the runway . a daytime , visible-spectrum image of the same aircraft would offer information about external detail , such as the plane 's marking and paint scheme . a dual-band camera enable precision image registration by fusion and frequently yield more information than be possible by evaluate the image separately",92,0
8575,Computers and IT,"The purpose of this study was to examine the subject relevance of information technology (IT) in hospitality and tourism management programs with skills deployed in the workplace. This study aimed at investigating graduates' transition from education to employment, and to determine how well they appear to be equipped to meet the needs of the hospitality and tourism industry. One hundred and seventeen graduates responded to a mail survey. These graduates rated the importance of IT skills in the workplace, the level of IT teaching in hotel and tourism management programs, and the self-competence level in IT. This study concluded that a gap exists between the IT skills required at work and those acquired at university","the purpose of this study was to examine the subject relevance of information technology (it) in hospitality and tourism management programs with skills deployed in the workplace. this study aimed at investigating graduates' transition from education to employment, and to determine how well they appear to be equipped to meet the needs of the hospitality and tourism industry. one hundred and seventeen graduates responded to a mail survey. these graduates rated the importance of it skills in the workplace, the level of it teaching in hotel and tourism management programs, and the self-competence level in it. this study concluded that a gap exists between the it skills required at work and those acquired at university",115,0
8576,Computers and IT,"This article presents a qualitative account of the development of computer-mediated tourism distance learning resources. A distance learning model was developed at the Centre for Tourism, University of Otago (New Zealand) in 1998-1999. The article reviews the development of this Internet-based learning resource explaining the design and development of programme links (providing study information for students) and paper links (course material and learning features). The design of course material is reviewed with emphasis given to consistency of presentation between papers. The template for course material is described and illustrated and the article concludes with an overview of important design considerations","this article presents a qualitative account of the development of computer-mediated tourism distance learning resources. a distance learning model was developed at the centre for tourism, university of otago (new zealand) in 1998-1999. the article reviews the development of this internet-based learning resource explaining the design and development of programme links (providing study information for students) and paper links (course material and learning features). the design of course material is reviewed with emphasis given to consistency of presentation between papers. the template for course material is described and illustrated and the article concludes with an overview of important design considerations",100,0.1
8577,Computers and IT,"Product-mix flexibility is one of the major types of manufacturing flexibility, referring to the ability to produce a broad range of products or variants with presumed low changeover costs. The value of such a capability is important to establish for an industrial firm in order to ensure that the flexibility provided will be at the right level and used profitably rather than in excess of market requirements and consequently costly. We use option-pricing theory to analyse the impact of various product-mix issues on the value of flexibility. The real options model we use incorporates multiple products, capacity constraints as well as set-up costs. The issues treated here include the number of products, demand variability, correlation between products, and the relative demand distribution within the product mix. Thus, we are interested in the nature of the input data to analyse its effect on the value of flexibility. We also check the impact at different capacity levels. The results suggest that the value of flexibility (i) increases with an increasing number of products, (ii) decreases with increasing volatility of product demand, (iii) decreases the more positively correlated the demand is, and (iv) reduces for marginal capacity with increasing levels of capacity. Of these, the impact of positively correlated demand seems to be a major issue. However, the joint impact of the number of products and demand correlation showed some non-intuitive results","product-mix flexibility is one of the major types of manufacturing flexibility,referring to the ability to produce a broad range of products or variants with presumed low changeover costs. the value of such a capability is important to establish for an industrial firm in order to ensure that the flexibility provided will be at the right level and used profitably rather than in excess of market requirements and consequently costly. we use option-pricing theory to analyse the impact of various product-mix issues on the value of flexibility. the real options model we use incorporates multiple products, capacity constraints as well as set-up costs. the issues treated here include the number of products, demand variability, correlation between products, and the relative demand distribution within the product mix. thus, we are interested in the nature of the input data to analyse its effect on the value of flexibility. we also check the impact at different capacity levels. the results suggest that the value of flexibility (i) increases with an increasing number of products, (ii) decreases with increasing volatility of product demand, (iii) decreases the more positively correlated the demand is, and (iv) reduces for marginal capacity with increasing levels of capacity. of these, the impact of positively correlated demand seems to be a major issue. however, the joint impact of the number of products and demand correlation showed some non-intuitive results",228,0
8578,Computers and IT,"In this paper, a new learning package, VONNA(HBP), which provides an interactive and online environment for novices to study and practice hotel budget planning, is introduced. Its design philosophy will be discussed thoughtfully with special focus on how to make use of the multimedia and Internet. According to literatures, learning packages are faced to be more effective in delivering teaching material. Researchers indicate that students using a self-paced learning package score higher than in a traditional classroom setting. Moreover, the learning package provides different scenarios for students to explore themselves in a practical environment and is more cost effective and systematic than lectures. Currently, most learning packages in hotel education are not implemented using multimedia with Internet access. Our paper describes a new learning package that fills the gaps. VONNA(HBP) requires participants to investigate operational budgets on various areas such as sales levels, payroll, inventory level, promotion strategies, and facilities planning, etc. Eventually, the students/novices are required to practice their skills in a comprehensive case about a hypothetical hotel. They need to solve managerial problems by a combination of budgetary planning on human resources, staff training programmes, facilities' maintenance and replacement, or promotion schemes. Analytical tools are available for students/novices to judge an appropriate decision in handling constrained resources","in this paper, a new learning package, vonna(hbp), which provides aninteractive and online environment for novices to study and practice hotel budget planning, is introduced. its design philosophy will be discussed thoughtfully with special focus on how to make use of the multimedia and internet. according to literatures, learning packages are faced to be more effective in delivering teaching material. researchers indicate that students using a self-paced learning package score higher than in a traditional classroom setting. moreover, the learning package provides different scenarios for students to explore themselves in a practical environment and is more cost effective and systematic than lectures. currently, most learning packages in hotel education are not implemented using multimedia with internet access. our paper describes a new learning package that fills the gaps. vonna(hbp) requires participants to investigate operational budgets on various areas such as sales levels, payroll, inventory level, promotion strategies, and facilities planning, etc. eventually, the students/novices are required to practice their skills in a comprehensive case about a hypothetical hotel. they need to solve managerial problems by a combination of budgetary planning on human resources, staff training programmes, facilities' maintenance and replacement, or promotion schemes. analytical tools are available for students/novices to judge an appropriate decision in handling constrained resources",208,0.526315789
8579,Computers and IT,"Food and beverage management is the traditional core of hospitality education but, in its laboratory manifestation, has come under increasing pressure in recent years. It is an area that, arguably, presents the greatest challenges in adaptation to contemporary learning technologies but, at the same time, stands to benefit most from the potential of the Web. This paper addresses the design and development of a CD-ROM learning resource for food and beverage. It is a learning resource which is designed to integrate with rather than to replace existing conventional classroom and laboratory learning methods and, thus, compensate for the decline in the resource base faced in food and beverage education in recent years. The paper includes illustrative material drawn from the CD-ROM which demonstrates its use in teaching and learning","food and beverage management be the traditional core of hospitality education but , in its laboratory manifestation , have come under increase pressure in recent year . it be an area that , arguably , present the great challenge in adaptation to contemporary learn technology but , at the same time , stand to benefit most from the potential of the web . this paper address the design and development of a cd-rom learning resource for food and beverage . it be a learn resource which be design to integrate with rather than to replace exist conventional classroom and laboratory learning method and , thus , compensate for the decline in the resource base face in food and beverage education in recent year . the paper include illustrative material draw from the cd-rom which demonstrate its use in teaching and learning",129,0.6
8580,Computers and IT,"This paper presents a method to increase the accountability of certificate management by making it intractable for the certification authority (CA) to create contradictory statements about the validity of a certificate. The core of the method is a new primitive, undeniable attester, that allows someone to commit to some set S of bitstrings by publishing a short digest of S and to give attestations for any x that it is or is not a member of S. Such an attestation can be verified by obtaining in an authenticated way the published digest and applying a verification algorithm to the triple of the bitstring, the attestation and the digest. The most important feature of this primitive is the intractability of creating two contradictory proofs for the same candidate element x and digest. We give an efficient construction for undeniable attesters based on authenticated search trees. We show that the construction also applies to sets of more structured elements. We also show that undeniable attesters exist iff collision-resistant hash functions exist","this paper presents a method to increase the accountability of certificate management by making it intractable for the certification authority (ca) to create contradictory statements about the validity of a certificate. the core of the method is a new primitive, undeniable attester, that allows someone to commit to some set s of bitstrings by publishing a short digest of s and to give attestations for any x that it is or is not a member of s. such an attestation can be verified by obtaining in an authenticated way the published digest and applying a verification algorithm to the triple of the bitstring, the attestation and the digest. the most important feature of this primitive is the intractability of creating two contradictory proofs for the same candidate element x and digest. we give an efficient construction for undeniable attesters based on authenticated search trees. we show that the construction also applies to sets of more structured elements. we also show that undeniable attesters exist iff collision-resistant hash functions exist",169,0.470588235
8581,Computers and IT,"The widespread use of Internet-based services is increasing the amount of information (such as user profiles) that clients are required to disclose. This information demand is necessary for regulating access to services, and functionally convenient (e. g. , to support service customization), but it has raised privacy-related concerns which, if not addressed, may affect the users disposition to use network services. At the same time, servers need to regulate service access without disclosing entirely the details of their access control policy. There is therefore a pressing need for privacy-aware techniques to regulate access to services open to the network. We propose an approach for regulating service access and information disclosure on the Web. The approach consists of a uniform formal framework to formulate - and reason about - both service access and information disclosure constraints. It also provides a means for parties to communicate their requirements while ensuring that no private information be disclosed and that the communicated requirements are correct with respect to the constraints","the widespread use of internet-based services is increasing the amount of information (such as user profiles) that clients are required to disclose. this information demand is necessary for regulating access to services, and functionally convenient (e.g., to support service customization), but it has raised privacy-related concerns which, if not addressed, may affect the users disposition to use network services. at the same time, servers need to regulate service access without disclosing entirely the details of their access control policy. there is therefore a pressing need for privacy-aware techniques to regulate access to services open to the network. we propose an approach for regulating service access and information disclosure on the web. the approach consists of a uniform formal framework to formulate - and reason about - both service access and information disclosure constraints. it also provides a means for parties to communicate their requirements while ensuring that no private information be disclosed and that the communicated requirements are correct with respect to the constraints",164,0.615384615
8582,Computers and IT,"With widespread acceptance of the Internet as a public medium for communication and information retrieval, there has been rising concern that the personal privacy of users can be eroded by cooperating network entities. A technical solution to maintaining privacy is to provide anonymity. We present a protocol for initiator anonymity called Hordes, which uses forwarding mechanisms similar to those used in previous protocols for sending data, but is the first protocol to make use of multicast routing to anonymously receive data. We show this results in shorter transmission latencies and requires less work of the protocol participants, in terms of the messages processed. We also present a comparison of the security and anonymity of Hordes with previous protocols, using the first quantitative definition of anonymity and unlinkability. Our analysis shows that Hordes provides anonymity in a degree similar to that of Crowds and Onion Routing, but also that Hordes has numerous performance advantages","with widespread acceptance of the internet as a public medium for communicationand information retrieval , there have be rise concern that the personal privacy of user can be erode by cooperate network entity . a technical solution to maintain privacy be to provide anonymity . we present a protocol for initiator anonymity call horde , which use forwarding mechanism similar to those use in previous protocol for send data , but be the first protocol to make use of multicast rout to anonymously receive data . we show this result in short transmission latency and require less work of the protocol participant , in term of the message process . we also present a comparison of the security and anonymity of horde with previous protocol , use the first quantitative definition of anonymity and unlinkability . our analysis show that horde provide anonymity in a degree similar to that of crowd and onion rout , but also that horde have numerous performance advantage",152,0.923076923
8583,Computers and IT,"The development and commissioning of information-processing and computing systems (IPCSs) at four power units, each of 500 MW capacity at the thermal power stations Tszisyan' and Imin' in China, are considered. The functional structure and the characteristics of the functions of the IPCSs are presented as is information on the technology of development and experience in adjustments. Ways of using the experience gained in creating a comprehensive functional firmware system are shown","the development and commissioning of information-processing and computingsystems (ipcss) at four power units, each of 500 mw capacity at the thermal power stations tszisyan' and imin' in china, are considered. the functional structure and the characteristics of the functions of the ipcss are presented as is information on the technology of development and experience in adjustments. ways of using the experience gained in creating a comprehensive functional firmware system are shown",71,0.7
8584,Computers and IT,The problem of selecting the shape of the actuator valve (the final control valve) itself is discussed; the solution to this problem will permit appreciable dynamic loads to be eliminated from the moving elements of the steam distribution system of steam turbines under all operating conditions,the problem of selecting the shape of the actuator valve (the final controlvalve) itself is discussed; the solution to this problem will permit appreciable dynamic loads to be eliminated from the moving elements of the steam distribution system of steam turbines under all operating conditions,45,0.25
8585,Computers and IT,"Technology and technological advances have always been a part of healthcare, but often it's advances in treatment machinery and materials that get the attention. However, technology gains also occur behind the scenes in operations. One of the less glamorous but powerful technological advances available today is predictive segmentation, a phrase that means ""a new way to assess and view individuals in the market based on their health status and health needs. "" Sophisticated databases, data mining, neural networks and statistical capabilities have enabled the development of predictive segmentation techniques. These predictive models for healthcare can identify who is likely to need certain services and who is likely to become ill. They are a significant departure from various geographical and attitudinal segmentation methods that healthcare strategists have used in the past to gain a better understanding of their customers","technology and technological advance have always be a part of healthcare , but often it ' advance in treatment machinery and material that get the attention . however , technology gain also occur behind the scene in operation . one of the less glamorous but powerful technological advance available today be predictive segmentation , a phrase that mean `` a new way to assess and view individual in the market base on their health status and health need . '' sophisticated database , data mining , neural network and statistical capability have enable the development of predictive segmentation technique . these predictive model for healthcare can identify who be likely to need certain service and who be likely to become ill. they be a significant departure from various geographical and attitudinal segmentation method that healthcare strategist have use in the past to gain a better understanding of their customer",136,0.8
8586,Computers and IT,"Uninterrupted power supplies (UPS), or battery backup systems, once provided a relatively limited, although important, function-continual battery support to connected equipment in the event of a power failure. However, yesterday's ""battery in a box"" has evolved into a sophisticated network power management tool that can monitor and actively correct many of the problems that might plague a healthy network. This new breed of UPS system provides such features as automatic voltage regulation, generous runtimes and unattended system shutdown, and now also monitors and automatically restarts critical services and operating systems if they lock up or otherwise fail","uninterrupted power supplies (ups), or battery backup systems, once provided arelatively limited, although important, function-continual battery support to connected equipment in the event of a power failure. however, yesterday's ""battery in a box"" has evolved into a sophisticated network power management tool that can monitor and actively correct many of the problems that might plague a healthy network. this new breed of ups system provides such features as automatic voltage regulation, generous runtimes and unattended system shutdown, and now also monitors and automatically restarts critical services and operating systems if they lock up or otherwise fail",96,0.75
8587,Computers and IT,"This paper sets out to examine the relationship between training and firm performance in middle-sized UK companies. It recognises that there is evidence that ""high performance work practices"" appear to be associated with better performance in large US companies, but argues that this relationship is less likely to be present in middle-sized companies. The paper's key contribution is to justify the wider concept of education, training and development (ETD) as applicable to such companies. It then finds that clusters of some ETD variables do appear to be associated with better middle-sized company performance","this paper sets out to examine the relationship between training and firm performance in middle-sized uk companies. it recognises that there is evidence that ""high performance work practices"" appear to be associated with better performance in large us companies, but argues that this relationship is less likely to be present in middle-sized companies. the paper's key contribution is to justify the wider concept of education, training and development (etd) as applicable to such companies. it then finds that clusters of some etd variables do appear to be associated with better middle-sized company performance",93,0.5
8588,Computers and IT,"Many large hospitals and healthcare systems have grown accustomed to the reliability of mainframe architecture, although tighter operating budgets, coupled with advances in client/server technology, have led to more office and clinical applications being moved off mainframes. But Evanston Northwestern Healthcare wasn't ready to get rid of its IBM OS 390 mainframe just yet. While a number of new clinical applications are being installed on two brand new IBM servers, Evanston Northwestern Healthcare will retain its favored hospital billing system and let it reside on the organization's mainframe, as it has since 1982","many large hospital and healthcare system have grow accustomed to thereliability of mainframe architecture , although tight operating budget , couple with advance in client/server technology , have lead to more office and clinical application be move off mainframe . but evanston northwestern healthcare be n't ready to get rid of its ibm o 390 mainframe just yet . while a number of new clinical application be be instal on two brand new ibm server , evanston northwestern healthcare will retain its favored hospital billing system and let it reside on the organization 's mainframe , as it have since 1982",92,0.4
8589,Computers and IT,"Workflow software provides the right communication solution for hospital specialists, and delivers an unexpected financial boost too","workflow software provide the right communication solution for hospitalspecialist , and deliver an unexpected financial boost too",16,0.25
8590,Computers and IT,"Three years ago, the Institute of Medicine (IOM) reported that medical errors result in at least 44, 000 deaths each year-more than deaths from highway accidents, breast cancer or AIDS. That report, and others which placed serious errors as high as 98, 000 annually, served as a wake-up call for healthcare providers such as the CareGroup Healthcare System Inc. , a Boston-area healthcare network that is the second largest integrated delivery system in the northeastern United States. With annual revenues of $1. 2B, CareGroup provides primary care and specialty services to more than 1, 000, 000 patients. CareGroup combined wireless technology with the Web to create a provider order entry (POE) system designed to reduce the frequency of costly medical mistakes. The POE infrastructure includes InterSystems Corporation's CACHE database, Dell Computer C600 laptops and Cisco Systems' Aironet 350 wireless networks","three years ago, the institute of medicine (iom) reported that medical errorsresult in at least 44,000 deaths each year-more than deaths from highway accidents, breast cancer or aids. that report, and others which placed serious errors as high as 98,000 annually, served as a wake-up call for healthcare providers such as the caregroup healthcare system inc., a boston-area healthcare network that is the second largest integrated delivery system in the northeastern united states. with annual revenues of $1.2b, caregroup provides primary care and specialty services to more than 1,000,000 patients. caregroup combined wireless technology with the web to create a provider order entry (poe) system designed to reduce the frequency of costly medical mistakes. the poe infrastructure includes intersystems corporation's cache database, dell computer c600 laptops and cisco systems' aironet 350 wireless networks",133,0.75
8591,Computers and IT,"More and more healthcare executives are electing to cut the cord to their existing computer systems by implementing mobile technology. The allure of information anywhere, anytime is intoxicating, demonstrated by the cell phones and personal digital assistants (PDAs) that adorn today's professionals. The utility and convenience of these devices is undeniable. But what is the best strategy for implementing a mobile solution within a healthcare enterprise, be it large or small-and under what circumstances? What types of healthcare workers benefit most from mobile technology? And how state-of-the-art is security for wireless applications and devices? These are the questions that healthcare executives are asking-and should be asking-as they evaluate mobile solutions","more and more healthcare executives are electing to cut the cord to theirexisting computer systems by implementing mobile technology. the allure of information anywhere, anytime is intoxicating, demonstrated by the cell phones and personal digital assistants (pdas) that adorn today's professionals. the utility and convenience of these devices is undeniable. but what is the best strategy for implementing a mobile solution within a healthcare enterprise, be it large or small-and under what circumstances? what types of healthcare workers benefit most from mobile technology? and how state-of-the-art is security for wireless applications and devices? these are the questions that healthcare executives are asking-and should be asking-as they evaluate mobile solutions",109,0.5
8592,Computers and IT,Procedures for the formation of states of the hidden Markov models are described. Formant amplitudes and frequencies are used as state features. The training strategy is presented that allows one to calculate the parameters of conditional probabilities of the generation of a given formant set by a given hidden state with the help of the maximum likelihood method,procedure for the formation of state of the hidden markov model aredescrib . formant amplitude and frequency be use as state feature . the training strategy be present that allow one to calculate the parameter of conditional probability of the generation of a give formant set by a give hidden state with the help of the maximum likelihood method,57,0.857142857
8593,Computers and IT,"Modern language studies are characterized by a variety of forms, ways, and methods of their development. In this connection, it is necessary to specify the problem of the development of their internal differentiation and classification, which lead to the formation of specific areas knowledge. An example of such an area is speechology-a field of science belonging to fundamental, theoretical, and applied linguistics","modern language study be characterize by a variety of form , way , andmethod of their development . in this connection , it be necessary to specify the problem of the development of their internal differentiation and classification , which lead to the formation of specific area knowledge . an example of such an area be speechology-a field of science belong to fundamental , theoretical , and apply linguistics",61,0.5
8594,Computers and IT,"In this paper we generalize a characterization property of generalized Pareto distributions, which is known for ordinary order statistics, to arbitrary schemes of progressive type-II censored order statistics. Various goodness-of-fit tests for generalized Pareto distributions based on progressively censored data statistics are discussed","in this paper we generalize a characterization property of generalize pareto distribution , which be know for ordinary order statistic , to arbitrary scheme of progressive type-ii censor order statistic . various goodness-of-fit test for generalize pareto distribution base on progressively censor data statistic be discuss",43,0.8
8595,Computers and IT,E-commerce over the Web has created a relatively new type of information system. So it is hardly surprising that little attention has been given to the maintenance of such systems-and even less to attempting to develop them with future maintenance in mind. But there are various ways e-commerce systems can be developed to reduce future maintenance,e-commerce over the web have create a relatively new type of informationsystem . so it be hardly surprising that little attention have be give to the maintenance of such systems-and even less to attempt to develop them with future maintenance in mind . but there be various way e-commerce system can be develop to reduce future maintenance,55,0
8596,Computers and IT,"MUSE (MUltiplicity SElector) is the trigger and control system of CHIMERA, a 4 pi charged particle detector. Initialization of MUSE can be performed via VMEbus. This paper describes the design of VMEbus interface and functional module in MUSE, and briefly discusses an application of MUSE","muse (multiplicity selector) is the trigger and control system of chimera, a 4pi charged particle detector. initialization of muse can be performed via vmebus. this paper describes the design of vmebus interface and functional module in muse, and briefly discusses an application of muse",44,0.6
8597,Computers and IT,"Technical specialists need to think about their role in IT projects and how they communicate with end-users and other participants to ensure they contribute fully as team members. It is especially important to communicate and document trade-offs that may have to be made, including the rationale behind them, so that if requirements change, the impact and decisions can be readily communicated to the stakeholders","technical specialist need to think about their role in it project and howthey communicate with end-user and other participant to ensure they contribute fully as team member . it be especially important to communicate and document trade-off that may have to be make , include the rationale behind them , so that if requirement change , the impact and decision can be readily communicate to the stakeholder",63,0.75
8598,Computers and IT,"Going freelance offers the potential of higher earnings, variety and independence - but also removes the benefits of permanent employment and can mean long distance travel and periods out of work. The author looks at the benefits and drawbacks - and how to get started as an IT contractor","go freelance offer the potential of high earnings , variety andindependence - but also remove the benefit of permanent employment and can mean long distance travel and period out of work . the author look at the benefit and drawback - and how to get start as an it contractor",48,0.5
8599,Computers and IT,"IT companies that contribute volunteers, resources or funding to charities and local groups not only make a real difference to their communities but also add value to their businesses. So says a new coalition of IT industry bodies formed to raise awareness of the options for community involvement, promote the business case, and publicise examples of best practice. The BCS, Intellect (formed from the merger of the Computing Services and Software Association and the Federation of the Electronics Industry) and the Worshipful Company of Information Technologists plan to run advisory seminars and provide guidelines on how companies of all sizes can transform their local communities using their specialist IT skills and resources while reaping business benefits","it companies that contribute volunteers, resources or funding to charities andlocal groups not only make a real difference to their communities but also add value to their businesses. so says a new coalition of it industry bodies formed to raise awareness of the options for community involvement, promote the business case, and publicise examples of best practice. the bcs, intellect (formed from the merger of the computing services and software association and the federation of the electronics industry) and the worshipful company of information technologists plan to run advisory seminars and provide guidelines on how companies of all sizes can transform their local communities using their specialist it skills and resources while reaping business benefits",115,0.25
8600,Computers and IT,"Under the UK government's spending review in 2000 the Office of Science and Technology was allocated Pounds 98m to establish a three year e-science research and development programme. The programme has a bold vision: to change the dynamic of the way science is undertaken. The term 'e-science' was introduced by John Taylor, director general of research councils in the Office of Science and Technology. He saw many areas of science becoming increasingly reliant on new ways of collaborative, multidisciplinary, interorganisation working. E-science is intended to capture these new modes of working. There are two major components to the programme: the science, and the infrastructure to support that science. The infrastructure is generally referred to as the Grid. The choice of name resonates with the idea of a future in which computing resources and storage, as well as expensive scientific facilities and software, can be accessed on demand, like electricity. Open source prototypes of the middleware are available and under development as part of the e-science programme and other international efforts","under the uk government 's spending review in 2000 the office of science andtechnology be allocate pound 98m to establish a three year e-science research and development program . the program have a bold vision : to change the dynamic of the way science be undertake . the term ` e-science ' be introduce by john taylor , director general of research council in the office of science and technology . he saw many area of science become increasingly reliant on new way of collaborative , multidisciplinary , interorganisation working . e-science be intend to capture these new mode of work . there be two major component to the program : the science , and the infrastructure to support that science . the infrastructure be generally refer to as the grid . the choice of name resonate with the idea of a future in which computing resource and storage , as well as expensive scientific facility and software , can be access on demand , like electricity . open source prototype of the middleware be available and under development as part of the e-science program and other international effort",169,0.444444444
8601,Computers and IT,A study group examines the issues auditors face in gathering electronic information as evidence and its impact on the audit,a study group examine the issue auditor face in gather electronicinformation as evidence and its impact on the audit,19,0
8602,Computers and IT,"Welcome to our fourth annual survey of accounting systems and enterprise resource planning (ERP) systems. Last September, we concentrated on financial and distribution systems for medium-sized businesses (mid market) and included 22 products in our charts. This year, we extended the products to include manufacturing and added 34 products to the list","welcome to our fourth annual survey of accounting systems and enterpriseresource planning (erp) systems. last september, we concentrated on financial and distribution systems for medium-sized businesses (mid market) and included 22 products in our charts. this year, we extended the products to include manufacturing and added 34 products to the list",51,0.5
8603,Computers and IT,"From business intelligence to wireless networking to service providers, here is what you need to know to keep up to speed with a changing landscape","from business intelligence to wireless networking to service provider , here iswhat you need to know to keep up to speed with a change landscape",24,0.75
8604,Computers and IT,"September 11 stripped us of our innocence, forcing corporations to recognize that disaster planning is a business necessity","september 11 strip us of our innocence , force corporation to recognizethat disaster planning be a business necessity",17,0.333333333
8605,Computers and IT,"When chartered accountants focus on IT, it's not simply because we think technology is neat. We keep on top of tech trends and issues because it helps us do our jobs well. We need to know how to best manage and implement the wealth of technology systems within out client base or employer, as well as to determine on an ongoing basis how evolving technologies might affect business strategies, threats and opportunities. One way to stay current with technology is by monitoring the online drumbeat. Imagine the Internet as an endless conversation of millions of chattering voices, each focusing on a multitude of topics and issues. It's not surprising that a great deal of the information relates to technology itself, and if you learn how to tune in to the drumbeat, you can keep yourself informed","when charter accountant focus on it , it ' not simply because we thinktechnology be neat . we keep on top of tech trend and issue because it help us do our job well . we need to know how to best manage and implement the wealth of technology system within out client base or employer , as well as to determine on an ongoing basis how evolve technology might affect business strategy , threat and opportunity . one way to stay current with technology be by monitor the online drumbeat . imagine the internet as an endless conversation of million of chatter voice , each focus on a multitude of topic and issue . it ' not surprising that a great deal of the information relate to technology itself , and if you learn how to tune in to the drumbeat , you can keep yourself inform",135,0.090909091
8606,Computers and IT,"The paper presents the way of combining two decision problems concerning a single (or a common) dimension, so that an effective fuzzy decision rule can be obtained. Normality of the possibility distribution is assumed, leading to possibility of fusing the respective functions related to the two decision problems and their characteristics (decisions, states of nature, utility functions, etc. ). The approach proposed can be applied in cases when the statement of the problem requires making of more refined distinctions rather than considering simply a bi-criterion or bi-utility two-decision problem","the paper presents the way of combining two decision problems concerning asingle (or a common) dimension, so that an effective fuzzy decision rule can be obtained. normality of the possibility distribution is assumed, leading to possibility of fusing the respective functions related to the two decision problems and their characteristics (decisions, states of nature, utility functions, etc.). the approach proposed can be applied in cases when the statement of the problem requires making of more refined distinctions rather than considering simply a bi-criterion or bi-utility two-decision problem",87,0.125
8607,Computers and IT,"In this case study in knowledge engineering and data mining, we implement a recognizer for two variations of the 'bull flag' technical charting heuristic and use this recognizer to discover trading rules on the NYSE Composite Index. Out-of-sample results indicate that these rules are effective","in this case study in knowledge engineering and data mining , we implement arecognizer for two variation of the ` bull flag ' technical charting heuristic and use this recognizer to discover trading rule on the nyse composite index . out-of-sample result indicate that these rule be effective",44,0.666666667
8608,Computers and IT,"The paper presents a statistical method of verifying ideological classifications of votes. Parliamentary votes, preclassified by an expert (on a chosen subset), are verified at an assumed significance level by seeking the most likely match with the actual vote results. Classifications that do not meet the requirements defined are rejected. The results obtained can be applied in the ideological dimensioning algorithms, enabling ideological identification of dimensions obtained","the paper presents a statistical method of verifying ideologicalclassifications of votes. parliamentary votes, preclassified by an expert (on a chosen subset), are verified at an assumed significance level by seeking the most likely match with the actual vote results. classifications that do not meet the requirements defined are rejected. the results obtained can be applied in the ideological dimensioning algorithms, enabling ideological identification of dimensions obtained",66,0.428571429
8609,Computers and IT,"We study axiomatic properties of the top cycle and uncovered solutions for weak tournaments. Subsequently, we establish its connection with the rational choice theory","we study axiomatic property of the top cycle and uncovered solution for weaktournament . subsequently , we establish its connection with the rational choice theory",23,0.8
8610,Computers and IT,A sufficient optimality condition is established for a nonlinear programming problem without differentiability assumption on the data wherein Clarke's (1975) generalized gradient is used to define invexity,a sufficient optimality condition is established for a nonlinear programmingproblem without differentiability assumption on the data wherein clarke's (1975) generalized gradient is used to define invexity,26,0.285714286
8611,Computers and IT,"In the paper non-identifier-based adaptive stabilization of undamped flexible structures is considered in the case of collocated input and output operators. The systems have poles and zeros on the imaginary axis. In the case where velocity feedback is available, the adaptive stabilizer is constructed by an adaptive PD-controller (proportional plus derivative controller). In the case where only position feedback is available, the adaptive stabilizer is constructed by an adaptive P-controller for the augmented system which consists of the controlled system and a parallel compensator. Numerical examples are given to illustrate the effectiveness of the proposed controllers","in the paper non-identifier-based adaptive stabilization of undamped flexiblestructures is considered in the case of collocated input and output operators. the systems have poles and zeros on the imaginary axis. in the case where velocity feedback is available, the adaptive stabilizer is constructed by an adaptive pd-controller (proportional plus derivative controller). in the case where only position feedback is available, the adaptive stabilizer is constructed by an adaptive p-controller for the augmented system which consists of the controlled system and a parallel compensator. numerical examples are given to illustrate the effectiveness of the proposed controllers",95,0.818181818
8612,Computers and IT,"We prove that if a sequence (f/sub n/)/sub n/ of DC functions (difference of two convex functions) converges to a DC function f in some appropriate way and if u/sub n/ is a critical point of f/sub n/, in the sense described by Toland (1978, 1979), and is such that (u/sub n/)/sub n/ converges to u, then u is a critical point of f, still in Toland's sense. We also build a new algorithm which searches for this critical point u and then apply it in order to compute the solution of a semilinear elliptic equation","we prove that if a sequence (f/sub n/)/sub n/ of dc functions (difference of two convex functions) converges to a dc function f in some appropriate way and if u/sub n/ is a critical point of f/sub n/, in the sense described by toland (1978, 1979), and is such that (u/sub n/)/sub n/ converges to u, then u is a critical point of f, still in toland's sense. we also build a new algorithm which searches for this critical point u and then apply it in order to compute the solution of a semilinear elliptic equation",96,0.2
8613,Computers and IT,"We consider an optimal control problem for systems governed by ordinary differential equations with control constraints. Since no convexity assumptions are made on the data, the problem is reformulated in relaxed form. The relaxed state equation is discretized by the implicit trapezoidal scheme and the relaxed controls are approximated by piecewise constant relaxed controls. We then propose a combined descent and discretization method that generates sequences of discrete relaxed controls and progressively refines the discretization. Since here the adjoint of the discrete state equation is not defined, we use, at each iteration, an approximate derivative of the cost functional defined by discretizing the continuous adjoint equation and the integral involved by appropriate trapezoidal schemes. It is proved that accumulation points of sequences constructed by this method satisfy the strong relaxed necessary conditions for optimality for the continuous problem. Finally, the computed relaxed controls can be easily approximated by piecewise constant classical controls","we consider an optimal control problem for system govern by ordinarydifferential equation with control constraint . since no convexity assumption be make on the data , the problem be reformulate in relax form . the relax state equation be discretiz by the implicit trapezoidal scheme and the relax control be approximate by piecewise constant relax control . we then propose a combine descent and discretization method that generate sequence of discrete relax control and progressively refine the discretization . since here the adjoint of the discrete state equation be not define , we use , at each iteration , an approximate derivative of the cost functional define by discretiz the continuous adjoint equation and the integral involve by appropriate trapezoidal scheme . it be prove that accumulation point of sequence construct by this method satisfy the strong relax necessary condition for optimality for the continuous problem . finally , the computed relax control can be easily approximate by piecewise constant classical control",151,0.416666667
8614,Computers and IT,"Exact solutions for static analysis of thermoelectroelastic laminated plates are presented. In this analysis, a new concise procedure for the analytical solution of composite laminated plates with piezoelectric layers is developed. A simple eigenvalue formula in real number form is directly developed from the basic coupled piezoelectric differential equations and the difficulty of treating imaginary eigenvalues is avoided. The solution is defined in the trigonometric series and can be applied to thin and thick plates. Numerical studies are conducted on a five-layer piezoelectric plate and the complexity of stresses and deformations under combined loading is illustrated. The results could be used as a benchmark for assessing any numerical solution by approximate approaches such as the finite element method while also providing useful physical insight into the behavior of piezoelectric plates in a thermal environment","exact solution for static analysis of thermoelectroelastic laminate plate be present . in this analysis , a new concise procedure for the analytical solution of composite laminate plate with piezoelectric layer be develop . a simple eigenvalue formula in real number form be directly develop from the basic couple piezoelectric differential equation and the difficulty of treat imaginary eigenvalue be avoid . the solution be define in the trigonometric series and can be apply to thin and thick plate . numerical study be conduct on a five-lay piezoelectric plate and the complexity of stress and deformation under combine load be illustrate . the result could be use as a benchmark for assess any numerical solution by approximate approach such as the finite element method while also provide useful physical insight into the behavior of piezoelectric plate in a thermal environment",134,0.85
8615,Computers and IT,"For pt. I see ibid. , pp. 1159-76. The two papers report systematically a constitutive description for the inelastic and strength behavior of laminated composites reinforced with various fiber preforms. The constitutive relationship is established micromechanically, through layer-by-layer analysis. Namely, only the properties of the constituent fiber and matrix materials of the composites are required as input data. In the previous part lamina theory was presented. Three fundamental quantities of the laminae, i. e. the internal stresses generated in the constituent fiber and matrix materials and the instantaneous compliance matrix, with different fiber preform (including woven, braided, and knitted fabric) reinforcements were explicitly obtained by virtue of the bridging micromechanics model. In this paper, the laminate stress analysis is shown. The purpose of this analysis is to determine the load shared by each lamina in the laminate, so that the lamina theory can be applied. Incorporation of the constitutive equations into an FEM software package is illustrated. A number of application examples are given to demonstrate the efficiency of the constitutive theory. The predictions made include: failure envelopes of multidirectional laminates subjected to biaxial in-plane loads, thermomechanical cycling stress-strain curves of a titanium metal matrix composite laminate, S-N curves of multilayer knitted fabric reinforced laminates under tensile fatigue, and bending load-deflection plots and ultimate bending strengths of laminated braided fabric reinforced beams subjected to lateral loads","for pt. i see ibid., pp. 1159-76. the two papers report systematically a constitutive description for the inelastic and strength behavior of laminated composites reinforced with various fiber preforms. the constitutive relationship is established micromechanically, through layer-by-layer analysis. namely, only the properties of the constituent fiber and matrix materials of the composites are required as input data. in the previous part lamina theory was presented. three fundamental quantities of the laminae, i.e. the internal stresses generated in the constituent fiber and matrix materials and the instantaneous compliance matrix, with different fiber preform (including woven, braided, and knitted fabric) reinforcements were explicitly obtained by virtue of the bridging micromechanics model. in this paper, the laminate stress analysis is shown. the purpose of this analysis is to determine the load shared by each lamina in the laminate, so that the lamina theory can be applied. incorporation of the constitutive equations into an fem software package is illustrated. a number of application examples are given to demonstrate the efficiency of the constitutive theory. the predictions made include: failure envelopes of multidirectional laminates subjected to biaxial in-plane loads, thermomechanical cycling stress-strain curves of a titanium metal matrix composite laminate, s-n curves of multilayer knitted fabric reinforced laminates under tensile fatigue, and bending load-deflection plots and ultimate bending strengths of laminated braided fabric reinforced beams subjected to lateral loads",224,0.607142857
8616,Computers and IT,"It is well known that a structural design with isotropic materials can only be accomplished based on a stress failure criterion. This is, however, generally not true with laminated composites. Only when the laminate is subjected to an in-plane load, can the ultimate failure of the laminate correspond to its last-ply failure, and hence a stress failure criterion may be sufficient to detect the maximum load that can be sustained by the laminate. Even in such a case, the load shared by each lamina in the laminate cannot be correctly determined if the lamina instantaneous stiffness matrix is inaccurately provided, since the lamina is always statically indeterminate in the laminate. If, however, the laminate is subjected to a lateral load, its ultimate failure occurs before last-ply failure and use of the stress failure criterion is no longer sufficient; an additional critical deflection or curvature condition must also be employed. This necessitates development of an efficient constitutive relationship for laminated composites in order that the laminate strains/deflections up to ultimate failure can be accurately calculated. A general constitutive description for the thermomechanical response of a fibrous laminate up to ultimate failure with applications to various fibrous laminates is presented in the two papers. The constitutive relationship is obtained by combining classical lamination theory with a recently developed bridging micromechanics model, through a layer-by-layer analysis. This paper focuses on lamina analysis","it be well know that a structural design with isotropic material can only be accomplish base on a stress failure criterion . this be , however , generally not true with laminated composite . only when the laminate be subject to an in-plane load , can the ultimate failure of the laminate correspond to its last-ply failure , and hence a stress failure criterion may be sufficient to detect the maximum load that can be sustain by the laminate . even in such a case , the load share by each lamina in the laminate can not be correctly determine if the lamina instantaneous stiffness matrix be inaccurately provide , since the lamina be always statically indeterminate in the laminate . if , however , the laminate be subject to a lateral load , its ultimate failure occur before last-ply failure and use of the stress failure criterion be no longer sufficient ; an additional critical deflection or curvature condition must also be employ . this necessitate development of an efficient constitutive relationship for laminate composite in order that the laminate strains/deflection up to ultimate failure can be accurately calculate . a general constitutive description for the thermomechanical response of a fibrous laminate up to ultimate failure with application to various fibrous laminate be present in the two paper . the constitutive relationship be obtain by combine classical lamination theory with a recently develop bridge micromechanic model , through a layer-by-lay analysis . this paper focus on lamina analysis",229,0.538461538
8617,Computers and IT,"Based on the concept of limit point instability, an advanced nonlinear finite-element method that can be used to analyze the aerostatic stability of cable-stayed bridges is proposed. Both geometric nonlinearity and three components of wind loads are considered in this method. The example bridge is the second Santou Bay cable-stayed bridge with a main span length of 518 m built in China. Aerostatic stability of the example bridge is investigated using linear and proposed methods. The effect of pitch moment coefficient on the aerostatic stability of the bridge has been studied. The results show that the aerostatic instability analyses of cable-stayed bridges based on the linear method considerably overestimate the wind-resisting capacity of cable-stayed bridges. The proposed method is highly accurate and efficient. Pitch moment coefficient has a major effect on the aerostatic stability of cable-stayed bridges. Finally, the aerostatic failure mechanism of cable-stayed bridges is explained by tracing the aerostatic instability path","base on the concept of limit point instability , an advanced nonlinear finite-element method that can be use to analyze the aerostatic stability of cable-stayed bridge be propose . both geometric nonlinearity and three component of wind load be consider in this method . the example bridge be the second santou bay cable-stayed bridge with a main span length of 518 m build in china . aerostatic stability of the example bridge be investigate use linear and propose method . the effect of pitch moment coefficient on the aerostatic stability of the bridge have be study . the result show that the aerostatic instability analysis of cable-stayed bridge base on the linear method considerably overestimate the wind-resisting capacity of cable-stayed bridge . the propose method be highly accurate and efficient . pitch moment coefficient have a major effect on the aerostatic stability of cable-stayed bridge . finally , the aerostatic failure mechanism of cable-stayed bridge be explain by trace the aerostatic instability path",153,0.8
8618,Computers and IT,"The application of knowledge-based control systems in the area of biotechnological processes has become increasingly popular over the past decade. This paper outlines the structure of the advanced knowledge-based part of the BIOGENES Copyright control system for the control of bioprocesses such as the fed-batch Saccharomyces cerevisiae cultivation. First, a brief overview of all the tasks implemented in the knowledge-based level including process data classification, qualitative process state identification and supervisory process control is given. The procedures performing the on-line identification of metabolic states and supervisory process control (setpoint calculation and control strategy selection) are described in more detail. Finally, the performance of the system is discussed using results obtained from a number of experimental cultivation runs in a laboratory unit","the application of knowledge-based control systems in the area ofbiotechnological processes has become increasingly popular over the past decade. this paper outlines the structure of the advanced knowledge-based part of the biogenes copyright control system for the control of bioprocesses such as the fed-batch saccharomyces cerevisiae cultivation. first, a brief overview of all the tasks implemented in the knowledge-based level including process data classification, qualitative process state identification and supervisory process control is given. the procedures performing the on-line identification of metabolic states and supervisory process control (setpoint calculation and control strategy selection) are described in more detail. finally, the performance of the system is discussed using results obtained from a number of experimental cultivation runs in a laboratory unit",120,0.7
8619,Computers and IT,"A new method for solving the weighted linear least squares problems with full rank is proposed. Based on the theory of Liapunov's stability, the method associates a dynamic system with a weighted linear least squares problem, whose solution we are interested in and integrates the former numerically by an A-stable numerical method. The numerical tests suggest that the new method is more than comparative with current conventional techniques based on the normal equations","a new method for solve the weighted linear least square problem with fullrank be propose . base on the theory of liapunov 's stability , the method associate a dynamic system with a weighted linear least square problem , whose solution we be interested in and integrate the former numerically by an a-stable numerical method . the numerical test suggest that the new method be more than comparative with current conventional technique base on the normal equation",72,0.5
8620,Computers and IT,"In this paper we introduce and analyze a fully discrete approximation for a parabolic problem with a nonlinear boundary condition which implies that the solutions blow up in finite time. We use standard linear elements with mass lumping for the space variable. For the time discretization we write the problem in an equivalent form which is obtained by introducing an appropriate time re-scaling and then, we use explicit Runge-Kutta methods for this equivalent problem. In order to motivate our procedure we present it first in the case of a simple ordinary differential equation and show how the blow up time is approximated in this case. We obtain necessary and sufficient conditions for the blowup of the numerical solution and prove that the numerical blow-up time converges to the continuous one. We also study, for the explicit Euler approximation, the localization of blow-up points for the numerical scheme","in this paper we introduce and analyze a fully discrete approximation for aparabolic problem with a nonlinear boundary condition which imply that the solution blow up in finite time . we use standard linear element with mass lump for the space variable . for the time discretization we write the problem in an equivalent form which be obtain by introduce an appropriate time re-scaling and then , we use explicit runge-kutta method for this equivalent problem . in order to motivate our procedure we present it first in the case of a simple ordinary differential equation and show how the blow up time be approximate in this case . we obtain necessary and sufficient condition for the blowup of the numerical solution and prove that the numerical blow-up time converge to the continuous one . we also study , for the explicit euler approximation , the localization of blow-up point for the numerical scheme",146,0.857142857
8621,Computers and IT,"We consider a general framework for analysing the convergence of multi-grid solvers applied to finite element discretisations of mixed problems, both of conforming and nonconforming type. As a basic new feature. our approach allows to use different finite element discretisations on each level of the multi-grid hierarchy. Thus, in our multi-level approach, accurate higher order finite element discretisations can be combined with fast multi-level solvers based on lower order (nonconforming) finite element discretisations. This leads to the design of efficient multi-level solvers for higher order finite element discretisations","we consider a general framework for analysing the convergence of multi-grid solvers applied to finite element discretisations of mixed problems, both of conforming and nonconforming type. as a basic new feature. our approach allows to use different finite element discretisations on each level of the multi-grid hierarchy. thus, in our multi-level approach, accurate higher order finite element discretisations can be combined with fast multi-level solvers based on lower order (nonconforming) finite element discretisations. this leads to the design of efficient multi-level solvers for higher order finite element discretisations",88,0.666666667
8622,Computers and IT,"In the framework of classical risk theory we investigate a surplus process in the presence of a nonlinear dividend barrier and derive equations for two characteristics of such a process, the probability of survival and the expected sum of discounted dividend payments. Number-theoretic solution techniques are developed for approximating these quantities and numerical illustrations are given for exponential claim sizes and a parabolic dividend barrier","in the framework of classical risk theory we investigate a surplus process inthe presence of a nonlinear dividend barrier and derive equation for two characteristic of such a process , the probability of survival and the expect sum of discounted dividend payment . number-theoretic solution technique be develop for approximate these quantity and numerical illustration be give for exponential claim size and a parabolic dividend barrier",64,0.888888889
8623,Computers and IT,"Communications during a crisis, both internal and external, set the tone during response and carry a message through recovery. The authors describe how to set up a system for information coordination to make sure the right people get the right message, and the organization stays in control","communication during a crisis , both internal and external , set the tone duringresponse and carry a message through recovery . the author describe how to set up a system for information coordination to make sure the right people get the right message , and the organization stay in control",46,0.333333333
8624,Computers and IT,"We investigate theta functions attached to quadratic forms over a number field K. We establish a functional equation by regarding the theta functions as specializations of symplectic theta functions. By applying a differential operator to the functional equation, we show how theta functions with harmonic coefficients over K behave under modular transformations","we investigate theta function attach to quadratic form over a number fieldk . we establish a functional equation by regard the theta function as specialization of symplectic theta function . by apply a differential operator to the functional equation , we show how theta function with harmonic coefficient over k behave under modular transformation",51,1
8625,Computers and IT,"In this paper it has been proved that if q is an odd prime, q not=7 (mod 8), n is an odd integer or=5, n is not a multiple of 3 and (h, n)=1, where h is the class number of the filed Q( square root (-q)), then the diophantine equation x/sup 2/+q/sup 2k+1/=y/sup n/ has exactly two families of solutions (q, n, k, x, y)","in this paper it has been proved that if q is an odd prime, q not=7 (mod 8), nis an odd integer >or=5, n is not a multiple of 3 and (h, n)=1, where h is the class number of the filed q( square root (-q)), then the diophantine equation x/sup 2/+q/sup 2k+1/=y/sup n/ has exactly two families of solutions (q, n, k, x, y)",65,0.6
8626,Computers and IT,"We classify even unimodular Gaussian lattices of rank 12, that is, even unimodular integral lattices of rank 12 over the ring of Gaussian integers. This is equivalent to the classification of the automorphisms tau with tau /sup 2/=-1 in the automorphism groups of all the Niemeier lattices, which are even unimodular (real) integral lattices of rank 24. There are 28 even unimodular Gaussian lattices of rank 12 up to equivalence","we classify even unimodular gaussian lattices of rank 12, that is, evenunimodular integral lattices of rank 12 over the ring of gaussian integers. this is equivalent to the classification of the automorphisms tau with tau /sup 2/=-1 in the automorphism groups of all the niemeier lattices, which are even unimodular (real) integral lattices of rank 24. there are 28 even unimodular gaussian lattices of rank 12 up to equivalence",69,0.6
8627,Computers and IT,"D. Weisser (1981) proved that there are exactly four Galois cubic number fields with Hilbert modular threefolds of arithmetic genus one. In this paper, we extend Weisser's work to cover all cubic number fields. Our main result is that there are exactly 33 fields with Hilbert modular threefolds of arithmetic genus one. These fields are enumerated explicitly","d. weisser (1981) proved that there are exactly four galois cubic number fieldswith hilbert modular threefolds of arithmetic genus one. in this paper, we extend weisser's work to cover all cubic number fields. our main result is that there are exactly 33 fields with hilbert modular threefolds of arithmetic genus one. these fields are enumerated explicitly",56,0.666666667
8628,Computers and IT,"In this paper, we examine the Iwasawa theory of elliptic curves E with additive reduction at an odd prime p. By extending Perrin-Riou's theory to certain nonsemistable representations, we are able to convert Kato's zeta-elements into p-adic L-functions. This allows us to deduce the cotorsion of the Selmer group over the cyclotomic Z/sub p/-extension of Q, and thus prove an inequality in the p-adic Birch and Swinnerton-Dyer conjecture at primes p whose square divides the conductor of E","in this paper , we examine the iwasawa theory of elliptic curve e with additivereduction at an odd prime p. by extend perrin-riou 's theory to certain nonsemistable representation , we be able to convert kato 's zeta-element into p-adic l-function . this allow us to deduce the cotorsion of the selmer group over the cyclotomic z/sub p / - extension of q , and thus prove an inequality in the p-adic birch and swinnerton-dyer conjecture at prime p whose square divide the conductor of e",77,0.727272727
8629,Computers and IT,The objective of this work is to develop an expert system for cucumber disorder diagnosis using non-monotonic reasoning to handle the situation when the system cannot reach a conclusion. One reason for this situation is when the information is incomplete. Another reason is when the domain knowledge itself is incomplete. Another reason is when the information is inconsistent. This method maintains the truth of the system in case of changing a piece of information. The proposed method uses two types of non-monotonic reasoning namely: default reasoning and reasoning in the presence of inconsistent information to achieve its goal,the objective of this work be to develop an expert system for cucumber disorderdiagnosi use non-monotonic reasoning to handle the situation when the system can not reach a conclusion . one reason for this situation be when the information be incomplete . another reason be when the domain knowledge itself be incomplete . another reason be when the information be inconsistent . this method maintain the truth of the system in case of change a piece of information . the propose method use two type of non-monotonic reasoning namely : default reasoning and reasoning in the presence of inconsistent information to achieve its goal,97,0.25
8630,Computers and IT,We show that for a real quadratic field F the dihedral congruence primes with respect to F for cusp forms of weight k and quadratic nebentypus are essentially the primes dividing expressions of the form epsilon /sub +//sup k-1/+or-1 where epsilon /sub +/ is a totally positive fundamental unit of F. This extends work of Hida. Our results allow us to identify a family of (ray) class fields of F which are generated by torsion points on modular abelian varieties,we show that for a real quadratic field f the dihedral congruence primes withrespect to f for cusp forms of weight k and quadratic nebentypus are essentially the primes dividing expressions of the form epsilon /sub +//sup k-1/+or-1 where epsilon /sub +/ is a totally positive fundamental unit of f. this extends work of hida. our results allow us to identify a family of (ray) class fields of f which are generated by torsion points on modular abelian varieties,79,0.714285714
8631,Computers and IT,"For the first order nonstationary hyperbolic equation taking the piecewise linear discontinuous Galerkin solver, we prove that under the uniform rectangular partition, such a discontinuous solver, after postprocessing, can have two and half approximative order which is half order higher than the optimal estimate by P. Lesaint and P. Raviart (1974) under the rectangular partition","for the first order nonstationary hyperbolic equation taking the piecewise linear discontinuous galerkin solver, we prove that under the uniform rectangular partition, such a discontinuous solver, after postprocessing, can have two and half approximative order which is half order higher than the optimal estimate by p. lesaint and p. raviart (1974) under the rectangular partition",55,0.8
8632,Computers and IT,"The Laguerre Gauss-Radau interpolation is investigated. Some approximation results are obtained. As an example, the Laguerre pseudospectral scheme is constructed for the BBM equation. The stability and the convergence of proposed scheme are proved. The numerical results show the high accuracy of this approach","the laguerre gauss-radau interpolation be investigate . some approximationresult be obtain . as an example , the laguerre pseudospectral scheme be construct for the bbm equation . the stability and the convergence of propose scheme be prove . the numerical result show the high accuracy of this approach",43,0.5
8633,Computers and IT,"The motion of surface waves under the effect of bottom is a very interesting and challenging phenomenon in the nature. we use boundary integral method to compute and analyze this problem. In the linear analysis, the linearized equations have bounded error increase under some compatible conditions. This contributes to the cancellation of instable Kelvin-Helmholtz terms. Under the effect of bottom, the existence of equations is hard to determine, but given some limitations it proves true. These limitations are that the swing of interfaces should be small enough, and the distance between surface and bottom should be large enough. In order to maintain the stability of computation, some compatible relationship must be satisfied. In the numerical examples, the simulation of standing waves and breaking waves are calculated. And in the case of shallow bottom, we found that the behavior of waves are rather singular","the motion of surface wave under the effect of bottom be a very interestingand challenging phenomenon in the nature . we use boundary integral method to compute and analyze this problem . in the linear analysis , the linearize equation have bound error increase under some compatible condition . this contribute to the cancellation of instable kelvin-helmholtz term . under the effect of bottom , the existence of equation be hard to determine , but give some limitation it prove true . these limitation be that the swing of interface should be small enough , and the distance between surface and bottom should be large enough . in order to maintain the stability of computation , some compatible relationship must be satisfy . in the numerical example , the simulation of stand wave and break wave be calculate . and in the case of shallow bottom , we find that the behavior of wave be rather singular",142,0.666666667
8634,Computers and IT,This paper is concerned with the second order elliptic problems with small periodic coefficients on a bounded domain with a curved boundary. A two-scale curved element method which couples linear elements and isoparametric elements is proposed. The error estimate is obtained over the given smooth domain. Furthermore an additive Schwarz method is provided for the isoparametric element method,this paper be concern with the second order elliptic problem with small periodic coefficient on a bounded domain with a curved boundary . a two-scale curved element method which couple linear element and isoparametric element be propose . the error estimate be obtain over the give smooth domain . furthermore an additive schwarz method be provide for the isoparametric element method,58,0.909090909
8635,Computers and IT,"This paper discusses band-limited scaling function, especially the single interval band case and three interval band cases. Their relationship to oversampling property and weakly translation invariance are also studied. At the end, we propose an open problem","this paper discuss band-limited scaling function , especially the singleinterval band case and three interval band case . their relationship to oversampl property and weakly translation invariance be also study . at the end , we propose an open problem",36,0.75
8636,Computers and IT,"The focus of this paper is on the relationship between accuracy of multivariate refinable vector and vector cascade algorithm. We show that, if the vector cascade algorithm (1. 5) with isotropic dilation converges to a vector-valued function with regularity, then the initial function must satisfy the Strang-Fix conditions","the focus of this paper is on the relationship between accuracy of multivariaterefinable vector and vector cascade algorithm. we show that, if the vector cascade algorithm (1.5) with isotropic dilation converges to a vector-valued function with regularity, then the initial function must satisfy the strang-fix conditions",46,0.666666667
8637,Computers and IT,Non-tensor product bivariate fractal interpolation functions defined on gridded rectangular domains are constructed. Linear spaces consisting of these functions are introduced. The relevant Lagrange interpolation problem is discussed. A negative result about the existence of affine fractal interpolation functions defined on such domains is obtained,non-tensor product bivariate fractal interpolation function define on griddedrectangular domain be construct . linear space consist of these function be introduce . the relevant lagrange interpolation problem be discuss . a negative result about the existence of affine fractal interpolation function define on such domain be obtain,44,0.833333333
8638,Computers and IT,"An elliptic curve is a pair (E, O), where E is a smooth projective curve of genus 1 and O is a point of E, called the point at infinity. Every elliptic curve can be given by a Weierstrass equation E : y/sup 2/ + a/sub 1/xy + a/sub 3/y = x/sup 3/ + a/sub 2/x/sup 2/ + a/sub 4/x + a/sub 6/. Let Q be the set of rationals. E is said to be defined over Q if the coefficients a/sub i/, i = 1, 2, 3, 4, 6 are rationals and O is defined over Q. Let E/Q be an elliptic curve and let E(Q)/sub tors/ be the torsion group of points of E defined over Q. The theorem of Mazur asserts that E(Q)/sub tors/ is one of the following 15 groups E(Q)/sub tors/ {Z/mZ, Z/mZ * Z/2mZ, m, = 1, 2, . . . , 10, 12, m = 1, 2, 3, 4. We say that an elliptic curve E'/Q is isogenous to the elliptic curve E if there is an isogeny, i. e. a morphism phi : E to E' such that phi (O) = O, where O is the point at infinity. We give an explicit model of all elliptic curves for which E(Q)/sub tors/ is in the form Z/mZ where m = 9, 10, 12 or Z/2Z * Z/2mZ where m = 4, according to Mazur's theorem. Moreover, for every family of such elliptic curves, we give an explicit model of all their isogenous curves with cyclic kernels consisting of rational points","an elliptic curve is a pair (e, o), where e is a smooth projective curve ofgenus 1 and o is a point of e, called the point at infinity. every elliptic curve can be given by a weierstrass equation e : y/sup 2/ + a/sub 1/xy + a/sub 3/y = x/sup 3/ + a/sub 2/x/sup 2/ + a/sub 4/x + a/sub 6/. let q be the set of rationals. e is said to be defined over q if the coefficients a/sub i/, i = 1, 2, 3, 4, 6 are rationals and o is defined over q. let e/q be an elliptic curve and let e(q)/sub tors/ be the torsion group of points of e defined over q. the theorem of mazur asserts that e(q)/sub tors/ is one of the following 15 groups e(q)/sub tors/ {z/mz, z/mz * z/2mz, m, = 1, 2, ..., 10, 12, m = 1, 2, 3, 4. we say that an elliptic curve e'/q is isogenous to the elliptic curve e if there is an isogeny, i.e. a morphism phi : e to e' such that phi (o) = o, where o is the point at infinity. we give an explicit model of all elliptic curves for which e(q)/sub tors/ is in the form z/mz where m = 9,10,12 or z/2z * z/2mz where m = 4, according to mazur's theorem. moreover, for every family of such elliptic curves, we give an explicit model of all their isogenous curves with cyclic kernels consisting of rational points",251,0.714285714
8639,Computers and IT,This paper abstracts the contents of a PhD dissertation entitled 'Transformation Rules and Strategies for Functional-Logic Programs' which has been recently defended. These techniques are based on fold/unfold transformations and they can be used to optimize integrated (functional-logic) programs for a wide class of applications. Experimental results show that typical examples in the field of artificial intelligence are successfully enhanced by our transformation system SYNTH. The thesis presents the first approach of these methods for declarative languages that integrate the best features from functional and logic programming,this paper abstracts the contents of a phd dissertation entitled'transformation rules and strategies for functional-logic programs' which has been recently defended. these techniques are based on fold/unfold transformations and they can be used to optimize integrated (functional-logic) programs for a wide class of applications. experimental results show that typical examples in the field of artificial intelligence are successfully enhanced by our transformation system synth. the thesis presents the first approach of these methods for declarative languages that integrate the best features from functional and logic programming,86,0.666666667
8640,Computers and IT,"We present the definition of the therapy decision task and its associated Heuristic Multi-Attribute (HM) solving method, in the form of a KADS-style specification. The goal of the therapy decision task is to identify the ideal therapy, for a given patient, in accordance with a set of objectives of a diverse nature constituting a global therapy-evaluation framework in which considerations such as patient preferences and quality-of-life results are integrated. We give a high-level overview of this task as a specialisation of the generic decision task, and additional decomposition methods for the subtasks involved. These subtasks possess some reflective capabilities for reasoning about self-models, particularly the learning subtask, which incrementally corrects and refines the model used to assess the effects of the therapies. This work illustrates the process of reuse in the framework of AI software development methodologies such as KADS-CommonKADS in order to obtain new (more specialised but still generic) components for the analysis libraries developed in this context. In order to maximise reuse benefits, where possible, the therapy decision task and HM method have been defined in terms of regular components from the earlier-mentioned libraries. To emphasise the importance of using a rigorous approach to the modelling of domain and method ontologies, we make extensive use of the semi-formal object-oriented analysis notation UML, together with its associated constraint language OCL, to illustrate the ontology of the decision method and the corresponding specific one of the therapy decision domain, the latter being a refinement via inheritance of the former","we present the definition of the therapy decision task and its associated heuristic multi-attribute (hm) solving method, in the form of a kads-style specification. the goal of the therapy decision task is to identify the ideal therapy, for a given patient, in accordance with a set of objectives of a diverse nature constituting a global therapy-evaluation framework in which considerations such as patient preferences and quality-of-life results are integrated. we give a high-level overview of this task as a specialisation of the generic decision task, and additional decomposition methods for the subtasks involved. these subtasks possess some reflective capabilities for reasoning about self-models, particularly the learning subtask, which incrementally corrects and refines the model used to assess the effects of the therapies. this work illustrates the process of reuse in the framework of ai software development methodologies such as kads-commonkads in order to obtain new (more specialised but still generic) components for the analysis libraries developed in this context. in order to maximise reuse benefits, where possible, the therapy decision task and hm method have been defined in terms of regular components from the earlier-mentioned libraries. to emphasise the importance of using a rigorous approach to the modelling of domain and method ontologies, we make extensive use of the semi-formal object-oriented analysis notation uml, together with its associated constraint language ocl, to illustrate the ontology of the decision method and the corresponding specific one of the therapy decision domain, the latter being a refinement via inheritance of the former",249,0.6875
8641,Computers and IT,"Any intelligent system, whether natural or artificial, must have three characteristics: knowledge, reasoning, and learning. Artificial intelligence (AI) studies these three aspects in artificial systems. Briefly, we could say that knowledge refers to the system's world model, and reasoning to the manipulation of this knowledge. Learning is slightly more complex; the system interacts with the world and as a consequence it builds onto and modifies its knowledge. This process of self-building and self-modifying is known as learning. This thesis is set within the field of artificial intelligence and focuses on learning. More specifically, it deals with the inductive learning of decision trees","any intelligent system, whether natural or artificial, must have threecharacteristics: knowledge, reasoning, and learning. artificial intelligence (ai) studies these three aspects in artificial systems. briefly, we could say that knowledge refers to the system's world model, and reasoning to the manipulation of this knowledge. learning is slightly more complex; the system interacts with the world and as a consequence it builds onto and modifies its knowledge. this process of self-building and self-modifying is known as learning. this thesis is set within the field of artificial intelligence and focuses on learning. more specifically, it deals with the inductive learning of decision trees",101,0.857142857
8642,Computers and IT,This article explicitly determines the quadratic Gauss sum over finite commutative rings,this article explicitly determine the quadratic gauss sum over finitecommutative ring,11,0.5
8643,Computers and IT,"This article investigates technology competency requirements in the library profession. Using the position advertisements in American Libraries in five-year increments over a twenty-year period (1970-1990), the article examines and evaluates the advertised qualifications of positions and attempts to see if midcareer librarians-especially those who have achieved their degree prior to the change in MLS curriculum that currently emphasizes technology-are ""effective"" librarians in the present and future job market","this article investigates technology competency requirements in the libraryprofession. using the position advertisements in american libraries in five-year increments over a twenty-year period (1970-1990), the article examines and evaluates the advertised qualifications of positions and attempts to see if midcareer librarians-especially those who have achieved their degree prior to the change in mls curriculum that currently emphasizes technology-are ""effective"" librarians in the present and future job market",67,0.75
8644,Computers and IT,"Little did I know when I attended Judith Bardwick's presentation on plateauing at the ALA annual convention in 1988 that it would turn out to be one of the most valuable sessions I would attend at any library conference, since it has enabled me to understand the phenomenon of plateauing and to use the strategies she suggested to rejuvenate my career and personal life continually. Key concepts and solutions from her book and from other literature on plateauing are summarized and examples given as to how I incorporated them into my life","little do i know when i attend judith bardwick 's presentation on plateauingat the ala annual convention in 1988 that it would turn out to be one of the most valuable session i would attend at any library conference , since it have enable me to understand the phenomenon of plateauing and to use the strategy she suggest to rejuvenate my career and personal life continually . key concept and solution from her book and from other literature on plateauing be summarize and example give as to how i incorporate them into my life",91,0.5
8645,Computers and IT,"Keeping a work journal can be useful in exploring one's thoughts and feelings about work challenges and work decisions. It can help bring about greater fulfillment in one's work life by facilitating self-renewal, change, the search for new meaning, and job satisfaction. One example of a work journal which I kept in 1998 is considered. It touches on several issues of potential interest to midlife career librarians including the challenge of technology, returning to work at midlife after raising a family, further education, professional writing, and job exchange","keep a work journal can be useful in explore one 's thought and feelingsabout work challenge and work decision . it can help bring about great fulfillment in one 's work life by facilitate self-renewal , change , the search for new meaning , and job satisfaction . one example of a work journal which i keep in 1998 be consider . it touch on several issue of potential interest to midlife career librarian include the challenge of technology , return to work at midlife after raise a family , further education , professional writing , and job exchange",87,0.909090909
8646,Computers and IT,"This essay is a personal reflection on entering librarianship in middle age at a time when the profession, like society in general, is experiencing rapidly accelerating change. Much of this change is due to the increased use of computers and information technologies in the library setting. These aids in the production, collection, storage, retrieval, and dissemination of the collective information, knowledge, and sometimes wisdom of the past and the contemporary world can exhilarate or burden depending on one's worldview, the organization, and the flexibility of the workplace. This writer finds herself working in a library where everyone is expected continually to explore and use new ways of working and providing library service to a campus and a wider community. No time is spent in reflecting on what was, but all efforts are to anticipate and prepare for what will be","this essay be a personal reflection on enter librarianship in middle age at a time when the profession , like society in general , be experience rapidly accelerate change . much of this change be due to the increase use of computer and information technology in the library setting . these aid in the production , collection , storage , retrieval , and dissemination of the collective information , knowledge , and sometimes wisdom of the past and the contemporary world can exhilarate or burden depend on one 's worldview , the organization , and the flexibility of the workplace . this writer find herself work in a library where everyone be expect continually to explore and use new way of work and provide library service to a campus and a wide community . no time be spend in reflect on what be , but all effort be to anticipate and prepare for what will be",140,0.888888889
8647,Computers and IT,"This article considers job rotation-the systematic movement of employees from one job to another-as one of the many tools within the organizational development tool kit. There is a brief consideration of useful print and Internet literature on the subject as well as a discussion of the pros and cons of job rotation. The application of job rotation methods in Ryerson University Library, a small academic library, concludes the article in order to illustrate process and insights through example","this article consider job rotation-the systematic movement of employee fromone job to another-as one of the many tool within the organizational development tool kit . there be a brief consideration of useful print and internet literature on the subject as well as a discussion of the pro and con of job rotation . the application of job rotation method in ryerson university library , a small academic library , conclude the article in order to illustrate process and insight through example",77,0.8
8648,Computers and IT,"It was 1963 when Candy Start began working in libraries. Libraries seemed to be a refuge from change, a dependable environment devoted primarily to preservation. She was mistaken. Technological changes in every decade of her experience have affected how and where she used her MLS. Far from a static refuge, libraries have proven to be spaceships loaded with precious cargo hurtling into the unknown. The historian in the author says that perhaps libraries have always been like this. This paper looks at a midlife decision point and the choice that this librarian made to move from a point of lessening productivity and interest to one of increasing challenge and contribution. It is a personal narrative of midlife experience from one librarian's point of view. Since writing this article, Candy's career has followed more changes. After selling the WINGS TM system, she has taken her experiences and vision to another library vendor, Gaylord Information Systems, where she serves as a senior product strategist","it be 1963 when candy start begin work in library . library seem to bea refuge from change , a dependable environment devote primarily to preservation . she be mistake . technological change in every decade of her experience have affect how and where she use her ml . far from a static refuge , library have prove to be spaceship load with precious cargo hurtle into the unknown . the historian in the author say that perhaps library have always be like this . this paper look at a midlife decision point and the choice that this librarian make to move from a point of lessen productivity and interest to one of increase challenge and contribution . it be a personal narrative of midlife experience from one librarian 's point of view . since write this article , candy 's career have follow more change . after sell the wing tm system , she have take her experience and vision to another library vendor , gaylord information system , where she serve as a senior product strategist",161,0.75
8649,Computers and IT,"The health care libraries unit coordinates, facilitates, and promotes continuing personal development for all staff in the Health Libraries and Information Network (HeLIN) of the Oxford Deanery (UK). It supports the development of a culture of lifelong learning and recognizes that CPD should help deliver organizational objectives, as well as enabling all staff to expand and fulfill their potential. A major emphasis for 2000 was to investigate ways of improving support for individual learning within the workplace. The group identified a need to build on existing informal support networks in order to provide additional learning opportunities and decided to investigate the feasibility of piloting a mentoring scheme. The objectives of the pilot were to increase understanding and knowledge of mentoring as a tool for CPD; to investigate existing mentoring schemes and their applicability for HeLIN; to develop a pilot mentoring scheme for HeLIN incorporating a program for accreditation of mentors; and to evaluate the scheme and disseminate the results. In order to identify current practice in this area, a literature review was carried out, and colleagues with an interest in or existing knowledge of mentoring schemes were contacted where possible. In the absence of clearly defined appraisal tools, all abstracts were read, and articles that met the following criteria were obtained and distributed to the group for review","the health care libraries unit coordinates, facilitates, and promotescontinuing personal development for all staff in the health libraries and information network (helin) of the oxford deanery (uk). it supports the development of a culture of lifelong learning and recognizes that cpd should help deliver organizational objectives, as well as enabling all staff to expand and fulfill their potential. a major emphasis for 2000 was to investigate ways of improving support for individual learning within the workplace. the group identified a need to build on existing informal support networks in order to provide additional learning opportunities and decided to investigate the feasibility of piloting a mentoring scheme. the objectives of the pilot were to increase understanding and knowledge of mentoring as a tool for cpd; to investigate existing mentoring schemes and their applicability for helin; to develop a pilot mentoring scheme for helin incorporating a program for accreditation of mentors; and to evaluate the scheme and disseminate the results. in order to identify current practice in this area, a literature review was carried out, and colleagues with an interest in or existing knowledge of mentoring schemes were contacted where possible. in the absence of clearly defined appraisal tools, all abstracts were read, and articles that met the following criteria were obtained and distributed to the group for review",217,0.5
8650,Computers and IT,"Midcareer librarians looking for career management help on the bookshelf face thousands of choices. This article reviews thirteen popular career self-help books. The reviewed books cover various aspects of career management and provide information on which might be best suited for particular goals, including career change, career tune-up, and personal and professional self-evaluation. The comments reflect issues of interest to midcareer professionals","midcareer librarian look for career management help on the bookshelf facethousand of choice . this article review thirteen popular career self-help book . the review book cover various aspect of career management and provide information on which might be best suit for particular goal , include career change , career tune-up , and personal and professional self-evaluation . the comment reflect issue of interest to midcareer professional",61,0.571428571
8651,Computers and IT,"As deregulation, new technologies, and new competitors open up the mobile telecommunications industry, churn prediction and management has become of great concern to mobile service providers. A mobile service provider wishing to retain its subscribers needs to be able to predict which of them may be at-risk of changing services and will make those subscribers the focus of customer retention efforts. In response to the limitations of existing churn-prediction systems and the unavailability of customer demographics in the mobile telecommunications provider investigated, we propose, design, and experimentally evaluate a churn-prediction technique that predicts churning from subscriber contractual information and call pattern changes extracted from call details. This proposed technique is capable of identifying potential churners at the contract level for a specific prediction time-period. In addition, the proposed technique incorporates the multi-classifier class-combiner approach to address the challenge of a highly skewed class distribution between churners and non-churners. The empirical evaluation results suggest that the proposed call-behavior-based churn-prediction technique exhibits satisfactory predictive effectiveness when more recent call details are employed for the churn prediction model construction. Furthermore, the proposed technique is able to demonstrate satisfactory or reasonable predictive power within the one-month interval between model construction and churn prediction. Using a previous demographics-based churn-prediction system as a reference, the lift factors attained by our proposed technique appear largely satisfactory","as deregulation , new technology , and new competitor open up the mobile telecommunication industry , churn prediction and management have become of great concern to mobile service provider . a mobile service provider wish to retain its subscriber need to be able to predict which of them may be at-risk of change service and will make those subscriber the focus of customer retention effort . in response to the limitation of exist churn-prediction system and the unavailability of customer demographic in the mobile telecommunication provider investigate , we propose , design , and experimentally evaluate a churn-prediction technique that predict churning from subscriber contractual information and call pattern change extract from call detail . this propose technique be capable of identify potential churner at the contract level for a specific prediction time-period . in addition , the propose technique incorporate the multi-classifi class-combin approach to address the challenge of a highly skewed class distribution between churner and non-churner . the empirical evaluation result suggest that the propose call-behavior-based churn-prediction technique exhibit satisfactory predictive effectiveness when more recent call detail be employ for the churn prediction model construction . furthermore , the propose technique be able to demonstrate satisfactory or reasonable predictive power within the one-month interval between model construction and churn prediction . use a previous demographics-based churn-prediction system as a reference , the lift factor attain by our propose technique appear largely satisfactory",219,0.833333333
8652,Computers and IT,"Career change can be a difficult, time-consuming, and anxiety-laden process for anyone contemplating this important decision. The challenges faced by librarians considering the move from academic to public librarianship can be equally and significantly demanding. To most outsiders, at least on the surface, it may appear to be a quick and easy transition to make, but some professional librarians recognize the distinct differences between these areas of librarianship. Although the ubiquitous nature of technology has brought the various work responsibilities of academic and public librarians closer together during the last decade, there remain key differences in job-related duties and the work environments. These dissimilarities pose meaningful hurdles to leap for academic librarians wishing to migrate to the public sector. The paper considers the variations between academic and public librarianship","career change can be a difficult , time-consuming , and anxiety-laden process foranyone contemplate this important decision . the challenge face by librarian consider the move from academic to public librarianship can be equally and significantly demand . to most outsider , at least on the surface , it may appear to be a quick and easy transition to make , but some professional librarian recognize the distinct difference between these area of librarianship . although the ubiquitous nature of technology have bring the various work responsibility of academic and public librarian closer together during the last decade , there remain key difference in job-related duty and the work environment . these dissimilarity pose meaningful hurdle to leap for academic librarian wish to migrate to the public sector . the paper consider the variation between academic and public librarianship",128,0.666666667
8653,Computers and IT,"Issues of career movement and change are examined between library and archival fields and from small colleges to large universities. Issues examined include professional education and training, initial career-planning and placement, continuing education, scouting and mentoring, job market conditions, work experience and personal skills, professional involvement, and professional association self-interest. This examination leads to five observations: 1. It is easier, in terms of career transitions, for a librarian to become an archivist than it is for an archivist to become a librarian; 2. The progression from a small college venue to a large research university is very manageable with the proper planning and experience; 3. At least three of the career elements-professional education, career-planning, and professional association self-interest-in their best moments provide a foundation that enables a future consideration of change between institutional types and professional areas and in their worst moments conspire against the midcareer professional in terms of change; 4. The elements of scouting, continuing education, work experience, and professional involvement offer the greatest assistance in career transitions; 5. The job market is the wildcard that either stymies or stimulates occupational development","issue of career movement and change be examine between library and archival field and from small college to large university . issue examine include professional education and training , initial career-planning and placement , continue education , scout and mentor , job market condition , work experience and personal skill , professional involvement , and professional association self-interest . this examination lead to five observation : 1 . it be easy , in term of career transition , for a librarian to become an archivist than it be for an archivist to become a librarian ; 2 . the progression from a small college venue to a large research university be very manageable with the proper planning and experience ; 3 . at least three of the career elements-professional education , career-planning , and professional association self-interest-in their best moment provide a foundation that enable a future consideration of change between institutional type and professional area and in their bad moment conspire against the midcareer professional in term of change ; 4 . the element of scout , continue education , work experience , and professional involvement offer the great assistance in career transition ; 5 . the job market be the wildcard that either stymie or stimulate occupational development",184,0.714285714
8654,Computers and IT,"This article challenges librarians to create leaves that will not only inspire professional growth but also renewal. It presents a framework for developing a successful leave, incorporating useful advice from librarians at Concordia University (Montreal). As food for thought, the article offers examples of specific options meant to encourage professionals to explore their own creative ideas. Finally, a central theme of this article is that a midlife leave provides one with the perfect opportunity to take stock of oneself in order to define future career directions. Midlife is a time when rebel forces, feisty protestors from within, often insist on being heard. It is a time, in other words, when professionals often long to break loose from the stress ""to do far more, in less time"" (Barner, 1994). Escaping from current job constraints into a world of creative endeavor, when well-executed, is a superb means of invigorating a career stuck in gear and discovering a fresh perspective from which to view one's profession. To ignite renewal, midcareer is the perfect time to grant one's imagination free reign","this article challenges librarians to create leaves that will not only inspireprofessional growth but also renewal. it presents a framework for developing a successful leave, incorporating useful advice from librarians at concordia university (montreal). as food for thought, the article offers examples of specific options meant to encourage professionals to explore their own creative ideas. finally, a central theme of this article is that a midlife leave provides one with the perfect opportunity to take stock of oneself in order to define future career directions. midlife is a time when rebel forces, feisty protestors from within, often insist on being heard. it is a time, in other words, when professionals often long to break loose from the stress ""to do far more, in less time"" (barner, 1994). escaping from current job constraints into a world of creative endeavor, when well-executed, is a superb means of invigorating a career stuck in gear and discovering a fresh perspective from which to view one's profession. to ignite renewal, midcareer is the perfect time to grant one's imagination free reign",176,0.666666667
8655,Computers and IT,"The author explores how the four-part model of transition cycles identified by Nicholson and West (1988) applies to becoming a chief librarian of an academic library. The four stages: preparation, encounter, adjustment, and stabilization, are considered from the micro-, mezzo-, and macrolevels of the organization, as well as for their psychological and social impact on the new job incumbent. An instrument for assessment of transitional success which could be administered in the adjustment or stabilization stage is considered","the author explores how the four-part model of transition cycles identified by nicholson and west (1988) applies to becoming a chief librarian of an academic library. the four stages: preparation, encounter, adjustment, and stabilization, are considered from the micro-, mezzo-, and macrolevels of the organization, as well as for their psychological and social impact on the new job incumbent. an instrument for assessment of transitional success which could be administered in the adjustment or stabilization stage is considered",78,0.5
8656,Computers and IT,"Included in the 2002 annual bibliography update is a select list of recent books and conference proceedings that have been published since 2000. Also included is a select list of special issues of journals and periodicals that were recently published. For additional lists of recently published books and articles, see ibid. (June 2000, June 2001)","included in the 2002 annual bibliography update is a select list of recentbooks and conference proceedings that have been published since 2000. also included is a select list of special issues of journals and periodicals that were recently published. for additional lists of recently published books and articles, see ibid. (june 2000, june 2001)",54,0.714285714
8657,Computers and IT,"If careers in the computer industry were viewed, it would be evident that there is a conspicuous gender gap between the number of male and female employees. The same gap can be observed at the college level where males are dominating females as to those who pursue and obtain a degree in computer science. The question that this research paper intends to show is: why are males so dominant when it comes to computer related matters? The author has traced this question back to the computer game. Computer games are a fun medium and provide the means for an individual to become computer literate through the engagement of spatial learning and cognitive processing abilities. Since such games are marketed almost exclusively to males, females have a distinct disadvantage. Males are more computer literate through the playing of computer games, and are provided with an easy lead-in to more advanced utilization of computers such as programming. Females tend to be turned off due to the male stereotypes and marketing associated with games and thus begins the gender gap","if career in the computer industry be view , it would be evident that there be a conspicuous gender gap between the number of male and female employee . the same gap can be observe at the college level where male be dominate female as to those who pursue and obtain a degree in computer science . the question that this research paper intend to show be : why be male so dominant when it come to computer relate matter ? the author have trace this question back to the computer game . computer game be a fun medium and provide the mean for an individual to become computer literate through the engagement of spatial learning and cognitive processing ability . since such game be market almost exclusively to male , female have a distinct disadvantage . male be more computer literate through the playing of computer game , and be provide with an easy lead-in to more advanced utilization of computer such as programming . female tend to be turn off due to the male stereotype and marketing associate with game and thus begin the gender gap",177,0.818181818
8658,Computers and IT,"Over the past few years, the computer science department faculty at Baylor has observed that some students who perform adequately during the freshman and sophomore years have substantial difficulty during the junior and senior years of study. Baylor University is an institution committed to being caring of its students. The objective for this study grew out of these two realities. There are three objectives of this research. One objective is to identify students, no later than the sophomore year, who are less likely to succeed as computer science majors. A second objective is to accomplish this identification by using data from seniors majoring in computer science. A third objective is to begin to use this information at the end of their sophomore year when meeting with a computer science faculty advisor. A regression study is conducted on the data from all students classified as seniors, majoring in computer science in May 2001, showing grades in six freshman and sophomore courses, and showing grades for at least five junior or senior level computer science courses. These students and their course performance data constituted the study sample","over the past few year , the computer science department faculty at baylor have observe that some student who perform adequately during the freshman and sophomore year have substantial difficulty during the junior and senior year of study . baylor university be an institution commit to be caring of its student . the objective for this study grow out of these two reality . there be three objective of this research . one objective be to identify student , no later than the sophomore year , who be less likely to succeed as computer science major . a second objective be to accomplish this identification by use data from senior major in computer science . a third objective be to begin to use this information at the end of their sophomore year when meeting with a computer science faculty advisor . a regression study be conduct on the data from all student classify as senior , major in computer science in may 2001 , show grade in six freshman and sophomore course , and show grade for at least five junior or senior level computer science course . these student and their course performance data constitute the study sample",185,0.5
8659,Computers and IT,"The end of the industrial age coincides with the advent of the information society as the next model of social and economic organization, which brings about significant changes in the way modern man conceives work and the social environment. The functional basis of the new model is pivoted upon the effort to formulate the theory on the violent reversal of the basic relationship between man and information, and isolate it as one of the components for the creation of the new electronic reality. The objective of the theory of reversal is to effectively contribute to the formulation of a new definition consideration in regards to the concept of the emerging information society. In order to empirically apply the theory of reversal, we examine a case study based on the example of the digital library","the end of the industrial age coincide with the advent of the informationsociety as the next model of social and economic organization , which bring about significant change in the way modern man conceive work and the social environment . the functional basis of the new model be pivot upon the effort to formulate the theory on the violent reversal of the basic relationship between man and information , and isolate it as one of the component for the creation of the new electronic reality . the objective of the theory of reversal be to effectively contribute to the formulation of a new definition consideration in regard to the concept of the emerge information society . in order to empirically apply the theory of reversal , we examine a case study base on the example of the digital library",133,0.625
8660,Computers and IT,"Identity authentication systems and procedures are rapidly becoming central issues in the practice and study of information systems development and security. Requirements for Web transaction security (WTS) include strong authentication of a user, non-repudiation and encryption of all traffic. In this paper, we present an effective mechanism involving two different channels, which addresses the prime concerns involved in the security of electronic commerce transactions (ECT) viz. user authentication and non-repudiation. Although the product is primarily targeted to provide a fillip to transactions carried out over the Web, this product can also be effectively used for non-Internet transactions that are carried out where user authentication is required","identity authentication systems and procedures are rapidly becoming centralissues in the practice and study of information systems development and security. requirements for web transaction security (wts) include strong authentication of a user, non-repudiation and encryption of all traffic. in this paper, we present an effective mechanism involving two different channels, which addresses the prime concerns involved in the security of electronic commerce transactions (ect) viz. user authentication and non-repudiation. although the product is primarily targeted to provide a fillip to transactions carried out over the web, this product can also be effectively used for non-internet transactions that are carried out where user authentication is required",105,0.625
8661,Computers and IT,"The government is tightening its grip on terrorist money flows. But as the banking industry continues to expand its Patriot Act compliance activities, it is with the realization that a great deal of work remains to be done before the American financial system can become truly airtight. Identification instruments, especially drivers licenses, represent a significant weak spot","the government be tighten its grip on terrorist money flow . but as thebank industry continue to expand its patriot act compliance activity , it be with the realization that a great deal of work remain to be do before the american financial system can become truly airtight . identification instrument , especially driver license , represent a significant weak spot",56,0.5
8662,Computers and IT,"Case-based reasoning (CBR) is a problem solving methodology commonly seen in artificial intelligence. It can correctly take advantage of the situations and methods in former cases to find out suitable solutions for new problems. CBR must accurately retrieve similar prior cases for getting a good performance. In the past, many researchers proposed useful technologies to handle this problem. However, the performance of retrieving similar cases may be greatly influenced by the number of cases. In this paper, the performance issue of large-scale CBR is discussed and a parallelized indexing architecture is then proposed for efficiently retrieving similar cases in large-scale CBR. Several algorithms for implementing the proposed architecture are also described. Some experiments are made and the results show the efficiency of the proposed method","case-based reasoning (cbr) is a problem solving methodology commonly seen inartificial intelligence. it can correctly take advantage of the situations and methods in former cases to find out suitable solutions for new problems. cbr must accurately retrieve similar prior cases for getting a good performance. in the past, many researchers proposed useful technologies to handle this problem. however, the performance of retrieving similar cases may be greatly influenced by the number of cases. in this paper, the performance issue of large-scale cbr is discussed and a parallelized indexing architecture is then proposed for efficiently retrieving similar cases in large-scale cbr. several algorithms for implementing the proposed architecture are also described. some experiments are made and the results show the efficiency of the proposed method",124,0.375
8663,Computers and IT,"With the spreading of the Internet and the wide use of computers, electronic publishing is becoming an indispensable measure to gain knowledge and skills. Meanwhile, copyright is facing much more infringement than ever in this electronic environment. So, it is a key factor to effectively protect copyright of electronic publishing to foster the new publication fields. The paper analyzes the importance of copyright, the main causes for copyright infringement in electronic publishing, and presents viewpoints on the definition and application of fair use of a copyrighted work and thinking of some means to combat breach of copyright","with the spread of the internet and the wide use of computer , electronicpublish be become an indispensable measure to gain knowledge and skill . meanwhile , copyright be face much more infringement than ever in this electronic environment . so , it be a key factor to effectively protect copyright of electronic publishing to foster the new publication field . the paper analyze the importance of copyright , the main cause for copyright infringement in electronic publishing , and present viewpoint on the definition and application of fair use of a copyright work and thinking of some mean to combat breach of copyright",96,0.714285714
8664,Computers and IT,"Authoritative sources concerned with computer-aided learning, resource-based learning and on-line learning and teaching are generally agreed that, in addition to subject matter expertise and technical support, the quality of the learning materials and the learning experiences of students are critically dependent on the application of pedagogically sound theories of learning and teaching and principles of course design. The University of the Highlands and Islands Project (UHIMI) is developing ""on-line learning"" on a large scale. These developments have been accompanied by a comprehensive programme of staff development. A major emphasis of the programme is concerned with ensuring that course developers and tutors are pedagogically aware. This paper reviews (i) what is meant by ""on-line learning"" in the UHIMI context (ii) the theories of learning and teaching and principles of course design that inform the staff development programme and (iii) a review of progress to date","authoritative sources concerned with computer-aided learning, resource-based learning and on-line learning and teaching are generally agreed that, in addition to subject matter expertise and technical support, the quality of the learning materials and the learning experiences of students are critically dependent on the application of pedagogically sound theories of learning and teaching and principles of course design. the university of the highlands and islands project (uhimi) is developing ""on-line learning"" on a large scale. these developments have been accompanied by a comprehensive programme of staff development. a major emphasis of the programme is concerned with ensuring that course developers and tutors are pedagogically aware. this paper reviews (i) what is meant by ""on-line learning"" in the uhimi context (ii) the theories of learning and teaching and principles of course design that inform the staff development programme and (iii) a review of progress to date",144,0.454545455
8665,Computers and IT,"Database technology advancements have provided many opportunities for libraries. These advancements can bring the world closer together through information accessibility. Digital library projects have been established worldwide to, ultimately, fulfil the needs of end users through more efficiency and convenience. Resource sharing will continue to be the trend for libraries. Changes often create issues which need to be addressed. Issues relating to database technology and digital libraries are reviewed. Some of the major challenges in digital libraries and managerial issues are identified as well","database technology advancement have provide many opportunity forlibrarie . these advancement can bring the world closer together through information accessibility . digital library project have be establish worldwide to , ultimately , fulfil the need of end user through more efficiency and convenience . resource sharing will continue to be the trend for library . change often create issue which need to be address . issue relate to database technology and digital library be review . some of the major challenge in digital library and managerial issue be identify as well",83,0.583333333
8666,Computers and IT,"The IIW Institute of Information Management (www. IIW. de) is dealing with commercial applications of digital technologies, such as the Internet, digital printing, and many more. A study which has been carried out by the institute, identifies viral messages as a new paradigm of communication, mostly found in the area of Direct Marketing, and - who wonders - mainly within the USA. Viral messages underlie certain principles: (1) prospects and customers of the idea are offered a technology platform providing a possibility to send a message to a majority of persons; (2) there is an emotional or pecuniary incentive to participate. Ideally, niches of needs and market vacua are filled with funny ideas; (3) also, the recipients are facing emotional or pecuniary incentives to contact a majority of further recipients - this induces a snowball effect and the message is spread virally; and (4) the customer is activated as an ""ambassador"" of the piece of information, for instance promoting a product or a company. It is evident that there has been a long lasting history of what we call ""word-of-mouth"" ever since, however bundles of digital technologies empower the viral communication paradigm","the iiw institute of information management (www.iiw.de) is dealing withcommercial applications of digital technologies, such as the internet, digital printing, and many more. a study which has been carried out by the institute, identifies viral messages as a new paradigm of communication, mostly found in the area of direct marketing, and - who wonders - mainly within the usa. viral messages underlie certain principles: (1) prospects and customers of the idea are offered a technology platform providing a possibility to send a message to a majority of persons; (2) there is an emotional or pecuniary incentive to participate. ideally, niches of needs and market vacua are filled with funny ideas; (3) also, the recipients are facing emotional or pecuniary incentives to contact a majority of further recipients - this induces a snowball effect and the message is spread virally; and (4) the customer is activated as an ""ambassador"" of the piece of information, for instance promoting a product or a company. it is evident that there has been a long lasting history of what we call ""word-of-mouth"" ever since, however bundles of digital technologies empower the viral communication paradigm",189,0.625
8667,Computers and IT,"A nonlinear iterative smoothing filter based on a second-order partial differential equation is introduced. It smooths out the image according to an anisotropic diffusion process. The approach is based on a smooth approximation of the total variation (TV) functional which overcomes the non-differentiability of the TV functional at the origin. In particular, the authors perform linear smoothing over smooth areas but selective smoothing over candidate edges. By relating the smoothing parameter to the time step, they arrive at a CFL condition which guarantees the causality of the discrete scheme. This allows the adoption of higher time discretisation steps, while ensuring the absence of artefacts deriving from the non-smooth behaviour of the TV functional at the origin. In particular, it is shown that the proposed approach avoids the typical staircase effects in smooth areas which occur in the standard time-marching TV scheme","a nonlinear iterative smoothing filter based on a second-order partialdifferential equation is introduced. it smooths out the image according to an anisotropic diffusion process. the approach is based on a smooth approximation of the total variation (tv) functional which overcomes the non-differentiability of the tv functional at the origin. in particular, the authors perform linear smoothing over smooth areas but selective smoothing over candidate edges. by relating the smoothing parameter to the time step, they arrive at a cfl condition which guarantees the causality of the discrete scheme. this allows the adoption of higher time discretisation steps, while ensuring the absence of artefacts deriving from the non-smooth behaviour of the tv functional at the origin. in particular, it is shown that the proposed approach avoids the typical staircase effects in smooth areas which occur in the standard time-marching tv scheme",140,0.461538462
8668,Computers and IT,"The authors propose a voice over Internet protocol (VoIP) technique with a new hierarchical data security protection (HDSP) scheme. The proposed HDSP scheme can maintain the voice quality degraded from packet loss and preserve high data security. It performs both the data inter-leaving on the inter-frame of voice for achieving better error recovery of voices suffering from continuous packet loss, and the data encryption on the intra-frame of voice for achieving high data security, which are controlled by a random bit-string sequence generated from a chaotic system. To demonstrate the performance of the proposed HDSP scheme, we have successfully verified and analysed the proposed approach through software simulation and statistical measures on several test voices","the authors propose a voice over internet protocol (voip) technique with a new hierarchical data security protection (hdsp) scheme. the proposed hdsp scheme can maintain the voice quality degraded from packet loss and preserve high data security. it performs both the data inter-leaving on the inter-frame of voice for achieving better error recovery of voices suffering from continuous packet loss, and the data encryption on the intra-frame of voice for achieving high data security, which are controlled by a random bit-string sequence generated from a chaotic system. to demonstrate the performance of the proposed hdsp scheme, we have successfully verified and analysed the proposed approach through software simulation and statistical measures on several test voices",115,0.846153846
8669,Computers and IT,"A new type of Taylor series based 2-D finite difference approximation is presented, and it is shown that the coefficients of these approximations are not unique. Explicit formulas are presented for one of the possible sets of coefficients for an arbitrary order, by extending the previously presented 1-D approximations. These coefficients are implemented as maximally linear 2-D FIR digital differentiators, and their formulas are modified to narrow the inaccuracy regions on the resultant frequency responses, close to the Nyquist frequencies","a new type of taylor series base 2-d finite difference approximation ispresent , and it be show that the coefficient of these approximation be not unique . explicit formula be present for one of the possible set of coefficient for an arbitrary order , by extend the previously present 1-d approximation . these coefficient be implement as maximally linear 2-d fir digital differentiator , and their formula be modify to narrow the inaccuracy region on the resultant frequency response , close to the nyquist frequency",79,0.333333333
8670,Computers and IT,"The authors investigate the convergence and pruning performance of multilayer feedforward neural networks with different types of neuronal activation functions in solving various problems. Three types of activation functions are adopted in the network, namely, the traditional sigmoid function, the sinusoidal function and a periodic function that can be considered as a combination of the first two functions. To speed up the learning, as well as to reduce the network size, the extended Kalman filter (EKF) algorithm conjunct with a pruning method is used to train the network. The corresponding networks are applied to solve five typical problems, namely, 4-point XOR logic function, parity generation, handwritten digit recognition, piecewise linear function approximation and sunspot series prediction. Simulation results show that periodic activation functions perform better than monotonic ones in solving multicluster classification problems. Moreover, the combined periodic activation function is found to possess the fast convergence and multicluster classification capabilities of the sinusoidal activation function while keeping the robustness property of the sigmoid function required in the modelling of unknown systems","the authors investigate the convergence and pruning performance of multilayer feedforward neural networks with different types of neuronal activation functions in solving various problems. three types of activation functions are adopted in the network, namely, the traditional sigmoid function, the sinusoidal function and a periodic function that can be considered as a combination of the first two functions. to speed up the learning, as well as to reduce the network size, the extended kalman filter (ekf) algorithm conjunct with a pruning method is used to train the network. the corresponding networks are applied to solve five typical problems, namely, 4-point xor logic function, parity generation, handwritten digit recognition, piecewise linear function approximation and sunspot series prediction. simulation results show that periodic activation functions perform better than monotonic ones in solving multicluster classification problems. moreover, the combined periodic activation function is found to possess the fast convergence and multicluster classification capabilities of the sinusoidal activation function while keeping the robustness property of the sigmoid function required in the modelling of unknown systems",171,0.6875
8671,Computers and IT,"The authors present a performance-robustness evaluation of the recently developed minimal resource allocation network (MRAN) for equalisation in highly nonlinear magnetic recording channels in disc storage systems. Unlike communication systems, equalisation of signals in these channels is a difficult problem, as they are corrupted by data-dependent noise and highly nonlinear distortions. Nair and Moon (1997) have proposed a maximum signal to distortion ratio (MSDR) equaliser for data storage channels, which uses a specially designed neural network, where all the parameters of the neural network are determined theoretically, based on the exact knowledge of the channel model parameters. In the present paper, the performance of the MSDR equaliser is compared with that of the MRAN equaliser using a magnetic recording channel model, under Conditions that include variations in partial erasure, jitter, width and noise power, as well as model mismatch. Results from the study indicate that the less complex MRAN equaliser gives consistently better performance robustness than the MSDR equaliser in terms of signal to distortion ratios (SDRs)","the authors present a performance-robustness evaluation of the recently developed minimal resource allocation network (mran) for equalisation in highly nonlinear magnetic recording channels in disc storage systems. unlike communication systems, equalisation of signals in these channels is a difficult problem, as they are corrupted by data-dependent noise and highly nonlinear distortions. nair and moon (1997) have proposed a maximum signal to distortion ratio (msdr) equaliser for data storage channels, which uses a specially designed neural network, where all the parameters of the neural network are determined theoretically, based on the exact knowledge of the channel model parameters. in the present paper, the performance of the msdr equaliser is compared with that of the mran equaliser using a magnetic recording channel model, under conditions that include variations in partial erasure, jitter, width and noise power, as well as model mismatch. results from the study indicate that the less complex mran equaliser gives consistently better performance robustness than the msdr equaliser in terms of signal to distortion ratios (sdrs)",167,0.615384615
8672,Computers and IT,"The cross-correlation and constant modulus algorithm (CC-CMA) has been proven to be an effective approach in the problem of joint blind equalisation and source separation in a multi-input and multi-output system. In the paper, the steady-state mean-square error performance of CC-CMA in a noise-free environment is studied, and a new expression is derived based on the energy preservation approach of Mai and Sayed (2000). Simulation studies are undertaken to support the analysis","the cross-correlation and constant modulus algorithm (cc-cma) has been proven to be an effective approach in the problem of joint blind equalisation and source separation in a multi-input and multi-output system. in the paper, the steady-state mean-square error performance of cc-cma in a noise-free environment is studied, and a new expression is derived based on the energy preservation approach of mai and sayed (2000). simulation studies are undertaken to support the analysis",72,0.7
8673,Computers and IT,"A workflow model is useful for business process analysis. A well-built workflow can help a company streamline its internal processes by reducing overhead. The results of workflow modeling need to be managed as information assets in a systematic fashion. Reusing these results is likely to enhance the quality of the modeling. Therefore, this paper proposes a document-based workflow modeling mechanism, which employs a case-based reasoning (CBR) technique for the effective reuse of design outputs. A repository is proposed to support this CBR process. A real-life case is illustrated to demonstrate the usefulness of our approach","a workflow model is useful for business process analysis. a well-built workflowcan help a company streamline its internal processes by reducing overhead. the results of workflow modeling need to be managed as information assets in a systematic fashion. reusing these results is likely to enhance the quality of the modeling. therefore, this paper proposes a document-based workflow modeling mechanism, which employs a case-based reasoning (cbr) technique for the effective reuse of design outputs. a repository is proposed to support this cbr process. a real-life case is illustrated to demonstrate the usefulness of our approach",94,0.833333333
8674,Computers and IT,"This paper presents an integrated robust fault detection and isolation (FDI) and fault tolerant control (FTC) scheme for a fault in actuators or sensors of linear stochastic systems subjected to unknown inputs (disturbances). As usual in this kind of works, it is assumed that single fault occurs at a time and the fault treated is of random bias type. The FDI module is constructed using banks of robust two-stage Kalman filters, which simultaneously estimate the state and the fault bias, and generate residual sets decoupled from unknown disturbances. All elements of residual sets are evaluated by using a hypothesis statistical test, and the fault is declared according to the prepared decision logic. The FTC module is activated based on the fault indicator, and additive compensation signal is computed using the fault bias estimate and combined to the nominal control law for compensating the fault's effect on the system. Simulation results for the simplified longitudinal flight control system with parameter variations, process and measurement noises demonstrate the effectiveness of the approach proposed","this paper presents an integrated robust fault detection and isolation (fdi) and fault tolerant control (ftc) scheme for a fault in actuators or sensors of linear stochastic systems subjected to unknown inputs (disturbances). as usual in this kind of works, it is assumed that single fault occurs at a time and the fault treated is of random bias type. the fdi module is constructed using banks of robust two-stage kalman filters, which simultaneously estimate the state and the fault bias, and generate residual sets decoupled from unknown disturbances. all elements of residual sets are evaluated by using a hypothesis statistical test, and the fault is declared according to the prepared decision logic. the ftc module is activated based on the fault indicator, and additive compensation signal is computed using the fault bias estimate and combined to the nominal control law for compensating the fault's effect on the system. simulation results for the simplified longitudinal flight control system with parameter variations, process and measurement noises demonstrate the effectiveness of the approach proposed",171,0.5
8675,Computers and IT,"The paper considers data modelling using multi-output regression models. A locally regularised orthogonal least-squares (LROLS) algorithm is proposed for constructing sparse multi-output regression models that generalise well. By associating each regressor in the regression model with an individual regularisation parameter, the ability of the multi-output orthogonal least-squares (OLS) model selection to produce a parsimonious model with a good generalisation performance is greatly enhanced","the paper considers data modelling using multi-output regression models. a locally regularised orthogonal least-squares (lrols) algorithm is proposed for constructing sparse multi-output regression models that generalise well. by associating each regressor in the regression model with an individual regularisation parameter, the ability of the multi-output orthogonal least-squares (ols) model selection to produce a parsimonious model with a good generalisation performance is greatly enhanced",63,0.571428571
8676,Computers and IT,"A least load dispatching algorithm for distributing requests to parallel Web server nodes is described. In this algorithm, the load offered to a node by a request is estimated based on the expected transfer time of the corresponding reply through the Internet. This loading information is then used by the algorithm to identify the least load node of the Web site. By using this algorithm, each request will always be sent for service at the earliest possible time. Performance comparison using NASA and ClarkNet access logs between the proposed algorithm and commonly used dispatching algorithms is performed. The results show that the proposed algorithm gives 10% higher throughput than that of the commonly used random and round-robin dispatching algorithms","a least load dispatch algorithm for distribute request to parallel webserver node be describe . in this algorithm , the load offer to a node by a request be estimate base on the expect transfer time of the corresponding reply through the internet . this load information be then use by the algorithm to identify the least load node of the web site . by use this algorithm , each request will always be send for service at the earliest possible time . performance comparison use nasa and clarknet access log between the propose algorithm and commonly use dispatch algorithm be perform . the result show that the propose algorithm give 10 % high throughput than that of the commonly use random and round-robin dispatch algorithm",118,0.6
8677,Computers and IT,"A novel self-tuning proportional and derivative (ST-PD) control based TCP congestion control scheme is proposed. The new scheme approaches the congestion control problem from a control-theoretical perspective and overcomes several Important limitations associated with existing TCP congestion control schemes, which are heuristic based. In the proposed scheme, a PD controller is employed to keep the buffer occupancy of the bottleneck node on the connection path at an ideal operating level, and it adjusts the TCP window accordingly. The control gains of the PD controller are tuned online by a fuzzy logic controller based on the perceived bandwidth-delay product of the TCP connection. This scheme gives ST-PD TCP several advantages over current TCP implementations. These include rapid response to bandwidth variations, insensitivity to buffer sizes, and significant improvement of TCP throughput over lossy links by decoupling congestion control and error control functions of TCP","a novel self-tuning proportional and derivative (st-pd) control based tcpcongestion control scheme is proposed. the new scheme approaches the congestion control problem from a control-theoretical perspective and overcomes several important limitations associated with existing tcp congestion control schemes, which are heuristic based. in the proposed scheme, a pd controller is employed to keep the buffer occupancy of the bottleneck node on the connection path at an ideal operating level, and it adjusts the tcp window accordingly. the control gains of the pd controller are tuned online by a fuzzy logic controller based on the perceived bandwidth-delay product of the tcp connection. this scheme gives st-pd tcp several advantages over current tcp implementations. these include rapid response to bandwidth variations, insensitivity to buffer sizes, and significant improvement of tcp throughput over lossy links by decoupling congestion control and error control functions of tcp",142,0
8678,Computers and IT,"The well known family of binary Legendre or quadratic residue sequences can be generalised to the multiple-valued case by employing a polyphase representation. These p-phase sequences, with p prime, also have prime length L, and can be constructed from the index sequence of length L or, equivalently, from the cosets of pth power residues and non-residues modulo-L. The linear complexity of these polyphase sequences is derived and shown to fall into four classes depending on the value assigned to b/sub 0/, the initial digit of the sequence, and on whether p belongs to the set of pth power residues or not. The characteristic polynomials of the linear feedback shift registers that generate these sequences are also derived","the well know family of binary legendre or quadratic residue sequence can begeneralis to the multiple-valued case by employ a polyphase representation . these p-phase sequence , with p prime , also have prime length l , and can be construct from the index sequence of length l or , equivalently , from the coset of pth power residue and non-residue modulo-l . the linear complexity of these polyphase sequence be derive and show to fall into four class depend on the value assign to b/sub 0 / , the initial digit of the sequence , and on whether p belong to the set of pth power residue or not . the characteristic polynomial of the linear feedback shift register that generate these sequence be also derive",116,0
8679,Computers and IT,"The module placement problem is to determine the co-ordinates of logic modules in a chip such that no two modules overlap and some cost (e. g. silicon area, interconnection length, etc. ) is optimised. To shorten connections between inputs and outputs and/or make related modules adjacent, it is desired to place some modules along the specific boundaries of a chip. To deal with such boundary constraints, we explore the feasibility conditions of a B*-tree with boundary constraints and develop a simulated annealing-based algorithm using B*-trees. Unlike most previous work, the proposed algorithm guarantees a feasible B*-tree with boundary constraints for each perturbation. Experimental results show that the algorithm can obtain a smaller silicon area than the most recent work based on sequence pairs","the module placement problem is to determine the co-ordinates of logic modulesin a chip such that no two modules overlap and some cost (e.g. silicon area, interconnection length, etc.) is optimised. to shorten connections between inputs and outputs and/or make related modules adjacent, it is desired to place some modules along the specific boundaries of a chip. to deal with such boundary constraints, we explore the feasibility conditions of a b*-tree with boundary constraints and develop a simulated annealing-based algorithm using b*-trees. unlike most previous work, the proposed algorithm guarantees a feasible b*-tree with boundary constraints for each perturbation. experimental results show that the algorithm can obtain a smaller silicon area than the most recent work based on sequence pairs",120,0
8680,Computers and IT,"An exact analytical model for the aliasing error probability (AEP) in the signature analysis of control signals using a modified signature analyser is presented. The signature analyser used comprises a general-structure two-input compacting module (TICM), which simplifies the motherboard VLSI design by providing a flexible geometry, which could be easily integrated with neighbouring structures. The use of the modified data probe eliminates the ambiguity introduced by the high-impedance state and at the same time retains the same signature of the binary stream. The model specifies algebraically the effects of the TICM architecture, the test pattern length, and the control stream error probabilities. It is proved that the (hardware) criterion used for calculating the AEP for the internal- and external exclusive-OR two-input shift registers is not valid for the general case and a new criterion is provided. The results obtained are augmented by two special cases, a case study, and associated simulation","an exact analytical model for the aliasing error probability (aep) in thesignature analysis of control signals using a modified signature analyser is presented. the signature analyser used comprises a general-structure two-input compacting module (ticm), which simplifies the motherboard vlsi design by providing a flexible geometry, which could be easily integrated with neighbouring structures. the use of the modified data probe eliminates the ambiguity introduced by the high-impedance state and at the same time retains the same signature of the binary stream. the model specifies algebraically the effects of the ticm architecture, the test pattern length, and the control stream error probabilities. it is proved that the (hardware) criterion used for calculating the aep for the internal- and external exclusive-or two-input shift registers is not valid for the general case and a new criterion is provided. the results obtained are augmented by two special cases, a case study, and associated simulation",150,0
8681,Computers and IT,"A fast algorithm having a pseudopolynomial run-time and memory requirement in the worst case is developed to generate multiplierless architectures at all wordlengths for constant multiplications in linear DSP transforms. It is also re-emphasised that indefinitely reducing operators for multiplierless architectures is not sufficient to reduce the final chip area. For a major reduction, techniques like resource folding must be used. Simple techniques for improving the results are also presented","a fast algorithm have a pseudopolynomial run-time and memory requirement in the bad case be develop to generate multiplierless architecture at all wordlength for constant multiplication in linear dsp transform . it be also re-emphasise that indefinitely reduce operator for multiplierless architecture be not sufficient to reduce the final chip area . for a major reduction , technique like resource folding must be use . simple technique for improve the result be also present",70,0
8682,Computers and IT,An extension of a gradient controllability problem to the case where the target subregion is a part of the boundary of a parabolic system domain is discussed. A definition and some properties adapted to this case are presented. The focus is on the characterisation of the control achieving a regional boundary gradient target with minimum energy. An approach is developed that leads to a numerical algorithm for the computation of optimal control. Numerical illustrations show the efficiency of the approach and lead to conjectures,an extension of a gradient controllability problem to the case where the targetsubregion be a part of the boundary of a parabolic system domain be discuss . a definition and some property adapt to this case be present . the focus be on the characterisation of the control achieve a regional boundary gradient target with minimum energy . an approach be develop that lead to a numerical algorithm for the computation of optimal control . numerical illustration show the efficiency of the approach and lead to conjecture,83,0
8683,Computers and IT,It is shown how nonlinearities are mapped in NARX polynomial models. General expressions are derived for the gain and eigenvalue functions in terms of the regressors and coefficients of NARX models. Such relationships are useful in grey-box identification problems. The results are illustrated using simulated and real data,it be show how nonlinearitie be map in narx polynomial model . generalexpression be derive for the gain and eigenvalue function in term of the regressor and coefficient of narx model . such relationship be useful in grey-box identification problem . the result be illustrate use simulated and real data,47,0
8684,Computers and IT,"An approach is investigated for the adaptive neural net-based H/sub infinity / control design of a class of nonlinear uncertain systems. In the proposed framework, two multilayer feedforward neural networks are constructed as an alternative to approximate the nonlinear system. The neural networks are piecewisely interpolated to generate a linear differential inclusion model by which a linear state feedback H/sub infinity / control law can be applied. An adaptive weight adjustment mechanism for the multilayer feedforward neural networks is developed to ensure H/sub infinity / regulation performance. It is shown that finding the control gain matrices can be transformed into a standard linear matrix inequality problem and solved via a developed recurrent neural network","an approach be investigate for the adaptive neural net-based h/sub infinity / control design of a class of nonlinear uncertain system . in the propose framework , two multilay feedforward neural network be construct as an alternative to approximate the nonlinear system . the neural network be piecewisely interpolate to generate a linear differential inclusion model by which a linear state feedback h/sub infinity / control law can be apply . an adaptive weight adjustment mechanism for the multilay feedforward neural network be develop to ensure h/sub infinity / regulation performance . it be show that find the control gain matrix can be transform into a standard linear matrix inequality problem and solve via a developed recurrent neural network",113,0
8685,Computers and IT,"Tom Berry has lost his PDA, and now he has an even better understanding of the risks and benefits of working on the move","tom berry have lose his pda , and now he have an even better understanding of therisk and benefit of work on the move",23,0
8686,Computers and IT,"A controller structure valid for SISO plants involving both internal and external point delays is presented. The control signal is based only on the input and output plant signals. The controller allows finite or infinite spectrum assignment. The most important feature of the proposed controller is that it only involves the use of a class of point-delayed signals. Thus the controller synthesis involves less computational cost than former methods. Since the plant control input is generated by filtering the input and output plant signals, this controller structure is potentially applicable to the adaptive case of unknown plant parameters","a controller structure valid for siso plant involve both internal andexternal point delay be present . the control signal be base only on the input and output plant signal . the controller allow finite or infinite spectrum assignment . the most important feature of the propose controller be that it only involve the use of a class of point-delayed signal . thus the controller synthesis involve less computational cost than former method . since the plant control input be generate by filter the input and output plant signal , this controller structure be potentially applicable to the adaptive case of unknown plant parameter",97,0.583333333
8687,Computers and IT,"An adaptive tracking control design for robotic systems using Gaussian wavelet networks is proposed. A Gaussian wavelet network with accurate approximation capability is employed to approximate the unknown dynamics of robotic systems by using an adaptive learning algorithm that can learn the parameters of the dilation and translation of Gaussian wavelet functions. Depending on the finite number of wavelet basis functions which result in inevitable approximation errors, a robust control law is provided to guarantee the stability of the closed-loop robotic system that can be proved by Lyapunov theory. Finally, the effectiveness of the Gaussian wavelet network-based control approach is illustrated through comparative simulations on a six-link robot manipulator","an adaptive tracking control design for robotic system use gaussian wavelet network be propose . a gaussian wavelet network with accurate approximation capability be employ to approximate the unknown dynamic of robotic system by use an adaptive learning algorithm that can learn the parameter of the dilation and translation of gaussian wavelet function . depend on the finite number of wavelet basis function which result in inevitable approximation error , a robust control law be provide to guarantee the stability of the closed-loop robotic system that can be prove by lyapunov theory . finally , the effectiveness of the gaussian wavelet network-based control approach be illustrate through comparative simulation on a six-link robot manipulator",109,0.727272727
8688,Computers and IT,"The computation of the frequency response of systems depending affinely on uncertain parameters can be reduced to that of all its one-dimensional edge plants while the image of such an edge plant at a fixed frequency is an arc or a line segment in the complex plane. Based on this conclusion, four computational formulas of the maximal and minimal (maxi-mini) magnitudes and phases of an edge plant at a fixed frequency are given. The formulas, besides sharing a simpler form of expression, concretely display how the extrema of the frequency response of the edge plant relate to the typical characteristics of the arc and line segment such as the centre, radius and tangent points of the arc, the distance from the origin to the line segment etc. The direct application of the results is to compute the Bode-, Nichols- and Nyquist-plot collections of the systems which are needed in robustness analysis and design","the computation of the frequency response of systems depending affinely on uncertain parameters can be reduced to that of all its one-dimensional edge plants while the image of such an edge plant at a fixed frequency is an arc or a line segment in the complex plane. based on this conclusion, four computational formulas of the maximal and minimal (maxi-mini) magnitudes and phases of an edge plant at a fixed frequency are given. the formulas, besides sharing a simpler form of expression, concretely display how the extrema of the frequency response of the edge plant relate to the typical characteristics of the arc and line segment such as the centre, radius and tangent points of the arc, the distance from the origin to the line segment etc. the direct application of the results is to compute the bode-, nichols- and nyquist-plot collections of the systems which are needed in robustness analysis and design",153,0.583333333
8689,Computers and IT,"A simple design methodology for the digital redesign of static state feedback controllers by using linear matrix inequalities is presented. The proposed method provides close matching of the states between the original continuous-time system and those of the digitally redesigned system with a guaranteed stability. Specifically, the digital redesign problem is reformulated as linear matrix inequalities (LMIs) and solved by a numerical optimisation technique. The main feature of the proposed method is that the closed-loop stability of the digitally redesigned system is explicitly guaranteed within the design procedure using the LMI-based approach. A numerical example of the position control of a simple crane system is presented","a simple design methodology for the digital redesign of static state feedbackcontrollers by using linear matrix inequalities is presented. the proposed method provides close matching of the states between the original continuous-time system and those of the digitally redesigned system with a guaranteed stability. specifically, the digital redesign problem is reformulated as linear matrix inequalities (lmis) and solved by a numerical optimisation technique. the main feature of the proposed method is that the closed-loop stability of the digitally redesigned system is explicitly guaranteed within the design procedure using the lmi-based approach. a numerical example of the position control of a simple crane system is presented",105,0.636363636
8690,Computers and IT,"For part 1, see ibid. , p. 285-90, (2002). Several different control schemes for integral processes with dead time resulted in the same disturbance response. It has already been shown that such a response is subideal. Hence, it is necessary to quantitatively analyse the achievable specifications and the robust stability regions. The control parameter can be quantitatively determined with a compromise between the disturbance response and the robustness. Four specifications: (normalised) maximum dynamic error, maximum decay rate, (normalised) control action bound and approximate recovery time are used to characterise the step-disturbance response. It is shown that any attempt to obtain a (normalised) dynamic error less than tau /sub m/ is impossible and a sufficient condition on the (relative) gain-uncertainty bound is square root (3)/2","for part 1, see ibid., p.285-90, (2002). several different control schemes forintegral processes with dead time resulted in the same disturbance response. it has already been shown that such a response is subideal. hence, it is necessary to quantitatively analyse the achievable specifications and the robust stability regions. the control parameter can be quantitatively determined with a compromise between the disturbance response and the robustness. four specifications: (normalised) maximum dynamic error, maximum decay rate, (normalised) control action bound and approximate recovery time are used to characterise the step-disturbance response. it is shown that any attempt to obtain a (normalised) dynamic error less than tau /sub m/ is impossible and a sufficient condition on the (relative) gain-uncertainty bound is square root (3)/2",121,0.692307692
8691,Computers and IT,"A disturbance observer-based control scheme (a version of 2 DOF internal model control) which is very effective in controlling integral processes with dead time is presented. The controller can be designed to reject ramp disturbances as well as step disturbances and even arbitrary disturbances. When the plant model is available only two parameters are left to tune. One is the time constant of the set-point response and the other is the time constant of the disturbance response. The latter is tuned according to the compromise between disturbance response and robustness. This control scheme has a simple, clear, easy-to-design, easy-to-implement structure and good performance. It is compared to the best results (so far) using some simulation examples","a disturbance observer-based control scheme (a version of 2 dof internal model control) which is very effective in controlling integral processes with dead time is presented. the controller can be designed to reject ramp disturbances as well as step disturbances and even arbitrary disturbances. when the plant model is available only two parameters are left to tune. one is the time constant of the set-point response and the other is the time constant of the disturbance response. the latter is tuned according to the compromise between disturbance response and robustness. this control scheme has a simple, clear, easy-to-design, easy-to-implement structure and good performance. it is compared to the best results (so far) using some simulation examples",116,0.666666667
8692,Computers and IT,"To tackle the obstacle of applying passivity-based control (PBC) to power systems, an affine non-linear system widely existing in power systems is formulated as a standard Hamiltonian system using a pre-feedback method. The port controlled Hamiltonian with dissipation (PCHD) model of a thyristor controlled serial compensator (TCSC) is then established corresponding with a revised Hamiltonian function. Furthermore, employing the modified Hamiltonian function directly as the storage function, a non-linear adaptive L/sub 2/ gain control method is proposed to solve the problem of L/sub 2/ gain disturbance attenuation for this Hamiltonian system with parametric perturbations. Finally, simulation results are presented to verify the validity of the proposed controller","to tackle the obstacle of applying passivity-based control (pbc) to power systems, an affine non-linear system widely existing in power systems is formulated as a standard hamiltonian system using a pre-feedback method. the port controlled hamiltonian with dissipation (pchd) model of a thyristor controlled serial compensator (tcsc) is then established corresponding with a revised hamiltonian function. furthermore, employing the modified hamiltonian function directly as the storage function, a non-linear adaptive l/sub 2/ gain control method is proposed to solve the problem of l/sub 2/ gain disturbance attenuation for this hamiltonian system with parametric perturbations. finally, simulation results are presented to verify the validity of the proposed controller",107,0.416666667
8693,Computers and IT,"The paper presents a nonlinear adaptive controller (NAC) for single-input single-output feedback linearisable nonlinear systems. A sliding-mode state and perturbation observer is designed to estimate the system states and perturbation which includes the combined effect of system nonlinearities, uncertainties and external disturbances. The NAC design does not require the details of the nonlinear system model and full system states. It possesses an adaptation capability to deal with system parameter uncertainties, unmodelled system dynamics and external disturbances. The convergence of the observer and the stability analysis of the controller/observer system are given. The proposed control scheme is applied for control of a synchronous generator, in comparison with a state-feedback linearising controller (FLC). Simulation study is carried out based on a single-generator infinite-bus power system to show the performance of the controller/observer system","the paper presents a nonlinear adaptive controller (nac) for single-inputsingle-output feedback linearisable nonlinear systems. a sliding-mode state and perturbation observer is designed to estimate the system states and perturbation which includes the combined effect of system nonlinearities, uncertainties and external disturbances. the nac design does not require the details of the nonlinear system model and full system states. it possesses an adaptation capability to deal with system parameter uncertainties, unmodelled system dynamics and external disturbances. the convergence of the observer and the stability analysis of the controller/observer system are given. the proposed control scheme is applied for control of a synchronous generator, in comparison with a state-feedback linearising controller (flc). simulation study is carried out based on a single-generator infinite-bus power system to show the performance of the controller/observer system",130,0.615384615
8694,Computers and IT,"The synthesis of a robust control law for regulation control of a class of relative-degree-one nonlinear systems is presented. The control design is based on a sliding-mode uncertainty estimator, developed under a framework of algebraic-differential concepts. The closed-loop stability for the underlying closed-loop system is achieved via averaging techniques. Robustness of the proposed control scheme is proved in the face of noise measurements, model uncertainties and sustained disturbances. The performance of the proposed control law is illustrated with numerical simulations, comparing the proposed controller with a well tuned PI controller","the synthesis of a robust control law for regulation control of a class ofrelative-degree-one nonlinear system be present . the control design be base on a sliding-mode uncertainty estimator , develop under a framework of algebraic-differential concept . the closed-loop stability for the underlie closed-loop system be achieve via average technique . robustness of the propose control scheme be prove in the face of noise measurement , model uncertainty and sustain disturbance . the performance of the propose control law be illustrate with numerical simulation , compare the propose controller with a well tune pi controller",89,0.75
8695,Computers and IT,"Deals with matched pole-zero discretisation, which has been used in practice for hand calculations in the digital redesign of continuous-time systems but available only in the transfer-function form. Since this form is inconvenient for characterising the time-domain properties of sampled-data loops and for computerising the design of such systems, a state-space formulation is developed. Under the new interpretation, the matched pole-zero model is shown to be structurally identical to a hold-equivalent discrete-time model, where the generalised hold takes integral part, thus unifying the most widely used discretisation approaches. An algorithm for obtaining the generalised hold function is presented. The hold-equivalent structure of the matched pole-zero model clarifies several discrete-time system properties, such as controllability and observability, and their preservation or loss with a matched pole-zero discretisation. With the proposed formulation, the matched pole-zero, hold-equivalent, and mapping models can now all be constructed with a single schematic model","deal with match pole-zero discretisation , which have be use in practicefor hand calculation in the digital redesign of continuous-time system but available only in the transfer-function form . since this form be inconvenient for characterise the time-domain property of sampled-data loop and for computerise the design of such system , a state-space formulation be develop . under the new interpretation , the match pole-zero model be show to be structurally identical to a hold-equivalent discrete-time model , where the generalise hold take integral part , thus unifying the most widely use discretisation approach . an algorithm for obtain the generalised hold function be present . the hold-equivalent structure of the match pole-zero model clarify several discrete-time system property , such as controllability and observability , and their preservation or loss with a match pole-zero discretisation . with the propose formulation , the match pole-zero , hold-equivalent , and mapping model can now all be construct with a single schematic model",146,0.727272727
8696,Computers and IT,"The computational solution of large-scale linear systems of equations necessitates the use of fast algorithms but is also greatly enhanced by employing parallelization techniques. The objective of this work is to demonstrate the speedup achieved by the MPI (message passing interface) parallel implementation of the steepest descent fast multipole method (SDFMM). Although this algorithm has already been optimized to take advantage of the structure of the physics of scattering problems, there is still the opportunity to speed up the calculation by dividing tasks into components using multiple processors and solve them in parallel. The SDFMM has three bottlenecks ordered as (1) filling the sparse impedance matrix associated with the near-field method of moments interactions (MoM), (2) the matrix vector multiplications associated with this sparse matrix and (3) the far field interactions associated with the fast multipole method. The parallel implementation task is accomplished using a thirty-one node Intel Pentium Beowulf cluster and is also validated on a 4-processor Alpha workstation. The Beowulf cluster consists of thirty-one nodes of 350 MHz Intel Pentium IIs with 256 MB of RAM and one node of a 4*450 MHz Intel Pentium II Xeon shared memory processor with 2 GB of RAM with all nodes connected to a 100 BaseTX Ethernet network. The Alpha workstation has a maximum of four 667 MHz processors. Our numerical results show significant linear speedup in filling the sparse impedance matrix. Using the 32-processors on the Beowulf cluster lead to a 7. 2 overall speedup while a 2. 5 overall speedup is gained using the 4-processors on the Alpha workstation","the computational solution of large-scale linear systems of equations necessitates the use of fast algorithms but is also greatly enhanced by employing parallelization techniques. the objective of this work is to demonstrate the speedup achieved by the mpi (message passing interface) parallel implementation of the steepest descent fast multipole method (sdfmm). although this algorithm has already been optimized to take advantage of the structure of the physics of scattering problems, there is still the opportunity to speed up the calculation by dividing tasks into components using multiple processors and solve them in parallel. the sdfmm has three bottlenecks ordered as (1) filling the sparse impedance matrix associated with the near-field method of moments interactions (mom), (2) the matrix vector multiplications associated with this sparse matrix and (3) the far field interactions associated with the fast multipole method. the parallel implementation task is accomplished using a thirty-one node intel pentium beowulf cluster and is also validated on a 4-processor alpha workstation. the beowulf cluster consists of thirty-one nodes of 350 mhz intel pentium iis with 256 mb of ram and one node of a 4*450 mhz intel pentium ii xeon shared memory processor with 2 gb of ram with all nodes connected to a 100 basetx ethernet network. the alpha workstation has a maximum of four 667 mhz processors. our numerical results show significant linear speedup in filling the sparse impedance matrix. using the 32-processors on the beowulf cluster lead to a 7.2 overall speedup while a 2.5 overall speedup is gained using the 4-processors on the alpha workstation",258,0.576923077
8697,Computers and IT,"Simulation of power electronic circuits remains a problem due to the high level of stiffness brought about by the modelling of switches as biresistors i. e. very low turn-on resistance and very high turn-off resistance. The merits and drawbacks of two modelling methods that address this problem are discussed. A modelling solution for ensuring numerically stable, accurate and fast simulation of power electronic systems is proposed. The solution enables easy connectivity between power electronic elements in the simulation model. It involves the modelling of virtual capacitance at switching nodes to soften voltage discontinuity due to the switch current suddenly going to zero. Undesirable ringing effects that may arise due to the interaction between the virtual capacitance and circuit inductance are eliminated by modelling virtual damping resistors in parallel to inductors that are adjacent to switching elements. A midpoint configuration method is also introduced for modelling shunt capacitors. A DC traction system is simulated using this modelling strategy and the results are included. Simulation results obtained using this modelling strategy are validated by comparison with the established mesh analysis technique of modelling. The simulation performance is also compared with the Power System Blockset commercial software","simulation of power electronic circuit remain a problem due to the high level of stiffness bring about by the modelling of switch as biresistor i.e. very low turn-on resistance and very high turn-off resistance . the merit and drawback of two modelling method that address this problem be discuss . a model solution for ensure numerically stable , accurate and fast simulation of power electronic system be propose . the solution enable easy connectivity between power electronic element in the simulation model . it involve the modelling of virtual capacitance at switch node to soften voltage discontinuity due to the switch current suddenly go to zero . undesirable ringing effect that may arise due to the interaction between the virtual capacitance and circuit inductance be eliminate by model virtual damp resistor in parallel to inductor that be adjacent to switch element . a midpoint configuration method be also introduce for model shunt capacitor . a dc traction system be simulated use this modelling strategy and the result be include . simulation result obtain use this modelling strategy be validate by comparison with the establish mesh analysis technique of modelling . the simulation performance be also compare with the power system blockset commercial software",193,0.384615385
8698,Computers and IT,"The three-element resonant network has various topological alternatives, one of which, a prospective compound topology, is investigated in detail. The converter uses one capacitor (C) and two inductors (L/sup 2/), to form a compound type CL/sup 2/ network. Various advantages and limitations of the converter are detailed, and a new design procedure for such converters is also introduced. The converter may be controlled by varying the switching frequency or by pulse-width modulation. An experimental prototype has been produced and an excellent performance in the lagging power-factor mode has been confirmed","the three-element resonant network has various topological alternatives, one of which, a prospective compound topology, is investigated in detail. the converter uses one capacitor (c) and two inductors (l/sup 2/), to form a compound type cl/sup 2/ network. various advantages and limitations of the converter are detailed, and a new design procedure for such converters is also introduced. the converter may be controlled by varying the switching frequency or by pulse-width modulation. an experimental prototype has been produced and an excellent performance in the lagging power-factor mode has been confirmed",90,0.666666667
8699,Computers and IT,"A high power factor half-bridge rectifier with neutral point switch clamped scheme is proposed. Three power switches are employed in the proposed rectifier. Two PWM control schemes are used to draw a sinusoidal line current with low current distortion. The control signals of the power switches are derived from the DC link voltage balance compensator, line current controller and DC link voltage regulator. The hysteresis current control scheme is employed to track the line current command. The proposed control scheme and the circuit configuration can be applied to the active power filter to eliminate the harmonic currents and compensate the reactive power generated from the nonlinear load. Analytical and experimental results are included to illustrate the validity and effectiveness of the proposed control scheme","a high power factor half-bridge rectifier with neutral point switch clampedscheme be propose . three power switch be employ in the propose rectifier . two pwm control scheme be use to draw a sinusoidal line current with low current distortion . the control signal of the power switch be derive from the dc link voltage balance compensator , line current controller and dc link voltage regulator . the hysteresis current control scheme be employ to track the line current command . the propose control scheme and the circuit configuration can be apply to the active power filter to eliminate the harmonic current and compensate the reactive power generate from the nonlinear load . analytical and experimental result be include to illustrate the validity and effectiveness of the propose control scheme",123,0.571428571
8700,Computers and IT,An advanced active power filter for the compensation of instantaneous harmonic current components in nonlinear current loads is presented. A signal processing technique using an adaptive neural network algorithm is applied for the detection of harmonic components generated by nonlinear current loads and it can efficiently determine the instantaneous harmonic components in real time. The validity of this active filtering processing system to compensate current harmonics is substantiated by simulation results,an advanced active power filter for the compensation of instantaneous harmoniccurrent component in nonlinear current load be present . a signal processing technique use an adaptive neural network algorithm be apply for the detection of harmonic component generate by nonlinear current load and it can efficiently determine the instantaneous harmonic component in real time . the validity of this active filter processing system to compensate current harmonic be substantiate by simulation result,70,0.625
8701,Computers and IT,"Three-phase four-wire active power filters (APFs) are presented that can be paralleled to enlarge the system capacity and reliability. The APF employs the PWM four-leg voltage-source inverter. A decoupling control approach for the leg connected to the neutral line is proposed such that the switching of all legs has no interaction. Functions of the proposed APF include compensation of reactive power, harmonic current, unbalanced power and zero-sequence current of the load. The objective is to achieve unity power factor, balanced line current and zero neutral-line current. Compensation of all components is capacity-limited, co-operating with the cascaded load current sensing scheme. Multiple APFs can be paralleled to share the load power without requiring any control interconnection. In addition to providing the theoretic bases and detailed design of the APFs, two 6 kVA APFs are implemented. The effectiveness of the proposed method is validated with experimental results","three-phase four-wire active power filters (apfs) are presented that can be paralleled to enlarge the system capacity and reliability. the apf employs the pwm four-leg voltage-source inverter. a decoupling control approach for the leg connected to the neutral line is proposed such that the switching of all legs has no interaction. functions of the proposed apf include compensation of reactive power, harmonic current, unbalanced power and zero-sequence current of the load. the objective is to achieve unity power factor, balanced line current and zero neutral-line current. compensation of all components is capacity-limited, co-operating with the cascaded load current sensing scheme. multiple apfs can be paralleled to share the load power without requiring any control interconnection. in addition to providing the theoretic bases and detailed design of the apfs, two 6 kva apfs are implemented. the effectiveness of the proposed method is validated with experimental results",145,0.3125
8702,Computers and IT,"The simulation and experimental study of a fuzzy logic controlled, three-phase shunt active power filter to improve power quality by compensating harmonics and reactive power required by a nonlinear load is presented. The advantage of fuzzy control is that it is based on a linguistic description and does not require a mathematical model of the system. The fuzzy control scheme is realised on an inexpensive dedicated micro-controller (INTEL 8031) based system. The compensation process is based on sensing line currents only, an approach different from conventional methods, which require harmonics or reactive volt-ampere requirement of the load. The performance of the fuzzy logic controller is compared with a conventional PI controller. The dynamic behavior of the fuzzy controller is found to be better than the conventional PI controller. PWM pattern generation is based on carrierless hysteresis based current control to obtain the switching signals. Various simulation and experimental results are presented under steady state and transient conditions","the simulation and experimental study of a fuzzy logic controlled, three-phaseshunt active power filter to improve power quality by compensating harmonics and reactive power required by a nonlinear load is presented. the advantage of fuzzy control is that it is based on a linguistic description and does not require a mathematical model of the system. the fuzzy control scheme is realised on an inexpensive dedicated micro-controller (intel 8031) based system. the compensation process is based on sensing line currents only, an approach different from conventional methods, which require harmonics or reactive volt-ampere requirement of the load. the performance of the fuzzy logic controller is compared with a conventional pi controller. the dynamic behavior of the fuzzy controller is found to be better than the conventional pi controller. pwm pattern generation is based on carrierless hysteresis based current control to obtain the switching signals. various simulation and experimental results are presented under steady state and transient conditions",156,0.272727273
8703,Computers and IT,"We develop an efficient way of representing the geometry and topology of volumetric datasets of biological structures from medium to low resolution, aiming at storing and querying them in a database framework. We make use of a new vector quantization algorithm to select the points within the macromolecule that best approximate the probability density function of the original volume data. Connectivity among points is obtained with the use of the alpha shapes theory. This novel data representation has a number of interesting characteristics, such as (1) it allows us to automatically segment and quantify a number of important structural features from low-resolution maps, such as cavities and channels, opening the possibility of querying large collections of maps on the basis of these quantitative structural features; (2) it provides a compact representation in terms of size; (3) it contains a subset of three-dimensional points that optimally quantify the densities of medium resolution data; and (4) a general model of the geometry and topology of the macromolecule (as opposite to a spatially unrelated bunch of voxels) is easily obtained by the use of the alpha shapes theory","we develop an efficient way of representing the geometry and topology of volumetric datasets of biological structures from medium to low resolution, aiming at storing and querying them in a database framework. we make use of a new vector quantization algorithm to select the points within the macromolecule that best approximate the probability density function of the original volume data. connectivity among points is obtained with the use of the alpha shapes theory. this novel data representation has a number of interesting characteristics, such as (1) it allows us to automatically segment and quantify a number of important structural features from low-resolution maps, such as cavities and channels, opening the possibility of querying large collections of maps on the basis of these quantitative structural features; (2) it provides a compact representation in terms of size; (3) it contains a subset of three-dimensional points that optimally quantify the densities of medium resolution data; and (4) a general model of the geometry and topology of the macromolecule (as opposite to a spatially unrelated bunch of voxels) is easily obtained by the use of the alpha shapes theory",185,0.761904762
8704,Computers and IT,"The syntenic distance between two genomes is given by the minimum number of fusions, fissions, and translocations required to transform one into the other, ignoring the order of genes within chromosomes. Computing this distance is NP-hard. In the present work, we give a tight connection between syntenic distance and the incomplete gossip problem, a novel generalization of the classical gossip problem. In this problem, there are n gossipers, each with a unique piece of initial information; they communicate by phone calls in which the two participants exchange all their information. The goal is to minimize the total number of phone calls necessary to inform each gossiper of his set of relevant gossip which he desires to learn. As an application of the connection between syntenic distance and incomplete gossip, we derive an O(2/sup O(n log n)/) algorithm to exactly compute the syntenic distance between two genomes with at most n chromosomes each. Our algorithm requires O(n/sup 2/+2/sup O(d log d)/) time when this distance is d, improving the O(n/sup 2/+2(O(d//sup 2/))) running time of the best previous exact algorithm","the syntenic distance between two genomes is given by the minimum number offusions, fissions, and translocations required to transform one into the other, ignoring the order of genes within chromosomes. computing this distance is np-hard. in the present work, we give a tight connection between syntenic distance and the incomplete gossip problem, a novel generalization of the classical gossip problem. in this problem, there are n gossipers, each with a unique piece of initial information; they communicate by phone calls in which the two participants exchange all their information. the goal is to minimize the total number of phone calls necessary to inform each gossiper of his set of relevant gossip which he desires to learn. as an application of the connection between syntenic distance and incomplete gossip, we derive an o(2/sup o(n log n)/) algorithm to exactly compute the syntenic distance between two genomes with at most n chromosomes each. our algorithm requires o(n/sup 2/+2/sup o(d log d)/) time when this distance is d, improving the o(n/sup 2/+2(o(d//sup 2/))) running time of the best previous exact algorithm",178,0.714285714
8705,Computers and IT,"We study the computational problem ""find the value of the quantified formula obtained by quantifying the variables in a sum of terms. "" The ""sum"" can be based on any commutative monoid, the ""quantifiers"" need only satisfy two simple conditions, and the variables can have any finite domain. This problem is a generalization of the problem ""given a sum-of-products of terms, find the value of the sum"" studied by R. E. Stearns and H. B. Hunt III (1996). A data structure called a ""structure tree"" is defined which displays information about ""subproblems"" that can be solved independently during the process of evaluating the formula. Some formulas have ""good"" structure trees which enable certain generic algorithms to evaluate the formulas in significantly less time than by brute force evaluation. By ""generic algorithm, "" we mean an algorithm constructed from uninterpreted function symbols, quantifier symbols, and monoid operations. The algebraic nature of the model facilitates a formal treatment of ""local reductions"" based on the ""local replacement"" of terms. Such local reductions ""preserve formula structure"" in the sense that structure trees with nice properties transform into structure trees with similar properties. These local reductions can also be used to transform hierarchical specified problems with useful structure into hierarchically specified problems having similar structure","we study the computational problem ""find the value of the quantified formulaobtained by quantifying the variables in a sum of terms."" the ""sum"" can be based on any commutative monoid, the ""quantifiers"" need only satisfy two simple conditions, and the variables can have any finite domain. this problem is a generalization of the problem ""given a sum-of-products of terms, find the value of the sum"" studied by r.e. stearns and h.b. hunt iii (1996). a data structure called a ""structure tree"" is defined which displays information about ""subproblems"" that can be solved independently during the process of evaluating the formula. some formulas have ""good"" structure trees which enable certain generic algorithms to evaluate the formulas in significantly less time than by brute force evaluation. by ""generic algorithm,"" we mean an algorithm constructed from uninterpreted function symbols, quantifier symbols, and monoid operations. the algebraic nature of the model facilitates a formal treatment of ""local reductions"" based on the ""local replacement"" of terms. such local reductions ""preserve formula structure"" in the sense that structure trees with nice properties transform into structure trees with similar properties. these local reductions can also be used to transform hierarchical specified problems with useful structure into hierarchically specified problems having similar structure",205,0.428571429
8706,Computers and IT,"Let alpha approximately=0. 87856 denote the best approximation ratio currently known for the Max-Cut problem on general graphs. We consider a semidefinite relaxation of the Max-Cut problem, round it using the random hyperplane rounding technique of M. X. Goemans and D. P. Williamson (1995), and then add a local improvement step. We show that for graphs of degree at most Delta , our algorithm achieves an approximation ratio of at least alpha + epsilon , where epsilon 0 is a constant that depends only on Delta . . Using computer assisted analysis, we show that for graphs of maximal degree 3 our algorithm obtains an approximation ratio of at least 0. 921, and for 3-regular graphs the approximation ratio is at least 0. 924. We note that for the semidefinite relaxation of Max-Cut used by Goemans and Williamson the integrality gap is at least 1/0. 885, even for 2-regular graphs","let alpha approximately=0.87856 denote the best approximation ratio currentlyknown for the max-cut problem on general graphs. we consider a semidefinite relaxation of the max-cut problem, round it using the random hyperplane rounding technique of m.x. goemans and d.p. williamson (1995), and then add a local improvement step. we show that for graphs of degree at most delta , our algorithm achieves an approximation ratio of at least alpha + epsilon , where epsilon >0 is a constant that depends only on delta .. using computer assisted analysis, we show that for graphs of maximal degree 3 our algorithm obtains an approximation ratio of at least 0.921, and for 3-regular graphs the approximation ratio is at least 0.924. we note that for the semidefinite relaxation of max-cut used by goemans and williamson the integrality gap is at least 1/0.885, even for 2-regular graphs",142,0.571428571
8707,Computers and IT,"In this paper, we consider hashing with linear probing for a hashing table with m places, n items (nm), and l=m-n empty places. For a noncomputer science-minded reader, we shall use the metaphore of n cars parking on m places: each car c/sub i/ chooses a place p/sub i/ at random, and if p/sub i/ is occupied, c/sub i/ tries successively p/sub i/+1, p/sub i/+2, until it finds an empty place. Pittel [42] proves that when l/m goes to some positive limit beta 1, the size B/sub 1//sup m, l/ of the largest block of consecutive cars satisfies 2( beta -1-log beta ) B/sub 1//sup m, l/=2 log m-3 log log m+ Xi /sub m/, where Xi /sub m/ converges weakly to an extreme-value distribution. In this paper we examine at which level for n a phase transition occurs between B/sub 1//sup m, l/=o(m) and m-B/sub 1//sup m, l/=o(m). The intermediate case reveals an interesting behavior of sizes of blocks, related to the standard additive coalescent in the same way as the sizes of connected components of the random graph are related to the multiplicative coalescent","in this paper, we consider hashing with linear probing for a hashing table withm places, n items (n<m), and l=m-n empty places. for a noncomputer science-minded reader, we shall use the metaphore of n cars parking on m places: each car c/sub i/ chooses a place p/sub i/ at random, and if p/sub i/ is occupied, c/sub i/ tries successively p/sub i/+1, p/sub i/+2, until it finds an empty place. pittel [42] proves that when l/m goes to some positive limit beta <1, the size b/sub 1//sup m,l/ of the largest block of consecutive cars satisfies 2( beta -1-log beta ) b/sub 1//sup m,l/=2 log m-3 log log m+ xi /sub m/, where xi /sub m/ converges weakly to an extreme-value distribution. in this paper we examine at which level for n a phase transition occurs between b/sub 1//sup m,l/=o(m) and m-b/sub 1//sup m,l/=o(m). the intermediate case reveals an interesting behavior of sizes of blocks, related to the standard additive coalescent in the same way as the sizes of connected components of the random graph are related to the multiplicative coalescent",181,0.166666667
8708,Computers and IT,"There exists a polynomial time algorithm to compute the pathwidth of outerplanar graphs, but the large exponent makes this algorithm impractical. In this paper, we give an algorithm that, given a biconnected outerplanar graph G, finds a path decomposition of G of pathwidth at most twice the pathwidth of G plus one. To obtain the result, several relations between the pathwidth of a biconnected outerplanar graph and its dual are established","there exist a polynomial time algorithm to compute the pathwidth ofouterplanar graph , but the large exponent make this algorithm impractical . in this paper , we give an algorithm that , give a biconnected outerplanar graph g , find a path decomposition of g of pathwidth at most twice the pathwidth of g plus one . to obtain the result , several relation between the pathwidth of a biconnected outerplanar graph and its dual be establish",70,0.8
8709,Computers and IT,"We establish an O(n log/sup 2/ n) upper bound on the time for deterministic distributed broadcasting in multi-hop radio networks with unknown topology. This nearly matches the known lower bound of Omega (n log n). The fastest previously known algorithm for this problem works in time O(n/sup 3/2/). Using our broadcasting algorithm, we develop an O(n/sup 3/2/ log/sup 2/ n) algorithm for gossiping in the same network model","we establish an o(n log/sup 2/ n) upper bound on the time for deterministicdistributed broadcasting in multi-hop radio networks with unknown topology. this nearly matches the known lower bound of omega (n log n). the fastest previously known algorithm for this problem works in time o(n/sup 3/2/). using our broadcasting algorithm, we develop an o(n/sup 3/2/ log/sup 2/ n) algorithm for gossiping in the same network model",67,0.4
8710,Computers and IT,"A positive (or monotone) Boolean function is regular if its variables are naturally ordered, left to fight, by decreasing strength, so that shifting the nonzero component of any true vector to the left always yields another true vector. This paper considers the problem of recognizing whether a positive function f is regular, where f is given by min T(f) (the set of all minimal true vectors of f). We propose a simple linear time (i. e. , O(n|min T(f)|)-time) algorithm for it. This improves upon the previous algorithm by J. S. Provan and M. O. Ball (1988) which requires O(n/sup 2/|min T(f)|) time. As a corollary, we also present an O(n(n+|min T(f)|))-time algorithm for the recognition problem of 2-monotonic functions","a positive (or monotone) boolean function is regular if its variables arenaturally ordered, left to fight, by decreasing strength, so that shifting the nonzero component of any true vector to the left always yields another true vector. this paper considers the problem of recognizing whether a positive function f is regular, where f is given by min t(f) (the set of all minimal true vectors of f). we propose a simple linear time (i.e., o(n|min t(f)|)-time) algorithm for it. this improves upon the previous algorithm by j.s. provan and m.o. ball (1988) which requires o(n/sup 2/|min t(f)|) time. as a corollary, we also present an o(n(n+|min t(f)|))-time algorithm for the recognition problem of 2-monotonic functions",115,0.571428571
8711,Computers and IT,"The article examines the role of British Standard 7666 in the development of a national framework for geocoding land and property information in the United Kingdom. The author assesses how local authorities, and other agencies concerned with property and address datasets, are coping with the introduction of British Standard 7666, and examines the prospects and limitations of this development. British Standard 7666 has four parts, comprising specifications for street gazetteer; land and property gazetteer; addresses; and public rights of way. The organisation coordinating the introduction of British Standard 7666, Improvement and Development Agency (IDeA), is also overseeing the development and maintenance of a National Land and Property Gazetteer (NLPG) based on British Standard 7666. The introduction of the new addressing standard has mainly been prompted by Britain's effort to set up a national cadastral service to replace the obsolescent property registration system currently in place","the article examines the role of british standard 7666 in the development of a national framework for geocoding land and property information in the united kingdom. the author assesses how local authorities, and other agencies concerned with property and address datasets, are coping with the introduction of british standard 7666, and examines the prospects and limitations of this development. british standard 7666 has four parts, comprising specifications for street gazetteer; land and property gazetteer; addresses; and public rights of way. the organisation coordinating the introduction of british standard 7666, improvement and development agency (idea), is also overseeing the development and maintenance of a national land and property gazetteer (nlpg) based on british standard 7666. the introduction of the new addressing standard has mainly been prompted by britain's effort to set up a national cadastral service to replace the obsolescent property registration system currently in place",145,0.695652174
8712,Computers and IT,"A key issue for cadastral systems is the maintenance of their correctness. Correctness is defined to be the proper correspondence between the valid legal situation and the content of the cadastre. This correspondence is generally difficult to achieve, since the cadastre is not a complete representation of all aspects influencing the legal situation in reality. The goal of the paper is to develop a formal model comprising representations of the cadastre and of reality that allows the simulation and investigation of cases where this correspondence is potentially violated. For this purpose the model consists of two parts, the first part represents the valid legal situation and the second part represents the cadastre. This makes it feasible to mark the differences between reality and the cadastre. The marking together with the two parts of the model facilitate the discussion of issues in ""real-world"" cadastral systems where incorrectness occurs. In order to develop a formal model, the paper uses the transfer of ownership of a parcel between two persons as minimal case study. The foundation for the formalization is a modern version of the situation calculus. The focus moves from the analysis of the cadastre to the preparation of a conceptual and a formalized model and the implementation of a prototype","a key issue for cadastral system be the maintenance of their correctness . correctness be define to be the proper correspondence between the valid legal situation and the content of the cadastre . this correspondence be generally difficult to achieve , since the cadastre be not a complete representation of all aspect influence the legal situation in reality . the goal of the paper be to develop a formal model comprise representation of the cadastre and of reality that allow the simulation and investigation of case where this correspondence be potentially violate . for this purpose the model consist of two part , the first part represent the valid legal situation and the second part represent the cadastre . this make it feasible to mark the difference between reality and the cadastre . the mark together with the two part of the model facilitate the discussion of issue in `` real-world '' cadastral system where incorrectness occur . in order to develop a formal model , the paper use the transfer of ownership of a parcel between two person as minimal case study . the foundation for the formalization be a modern version of the situation calculus . the focus move from the analysis of the cadastre to the preparation of a conceptual and a formalize model and the implementation of a prototype",208,0.8
8713,Computers and IT,"A new method for generating a spatially accurate, legally supportive and operationally efficient cadastral database of the urban cadastral reality is described. The definition and compilation of an accurate cadastral database (achieving a standard deviation smaller than 0. 1 m) is based on an analytical reconstruction of cadastral boundaries rather than on the conventional field reconstruction process. The new method is based on GPS control points and traverse networks for providing the framework; the old field books for defining the links between the various original ground features; and a geometrical and cadastral adjustment process as the conceptual basis. A pilot project that was carried out in order to examine and evaluate the new method is described","a new method for generating a spatially accurate, legally supportive and operationally efficient cadastral database of the urban cadastral reality is described. the definition and compilation of an accurate cadastral database (achieving a standard deviation smaller than 0.1 m) is based on an analytical reconstruction of cadastral boundaries rather than on the conventional field reconstruction process. the new method is based on gps control points and traverse networks for providing the framework; the old field books for defining the links between the various original ground features; and a geometrical and cadastral adjustment process as the conceptual basis. a pilot project that was carried out in order to examine and evaluate the new method is described",115,0.666666667
8714,Computers and IT,"The study investigates the design of a process for parcel boundary identification with cadastral map overlay using the principle of least squares. The objective of this research is to provide an objective tool for boundary identification survey. The proposed process includes an adjustment model, a weighting scheme, and other related operations. A numerical example is included","the study investigate the design of a process for parcel boundary identification with cadastral map overlay use the principle of least square . the objective of this research be to provide an objective tool for boundary identification survey . the proposed process include an adjustment model , a weighting scheme , and other related operation . a numerical example be include",56,0.545454545
8715,Computers and IT,"World-wide, much attention has been given to cadastral development. As a consequence of experiences made during recent decades, several authors have stated the need for research in the domain of cadastre and proposed methodologies to be used. The paper contributes to the acceptance of research methodologies needed for cadastral development, and thereby enhances theory in the cadastral domain. The paper reviews nine publications on cadastre and identifies the methodologies used. The review focuses on the institutional, social, political and economic aspects of cadastral development, rather than on the technical aspects. The main conclusion is that the methodologies used are largely those of the social sciences. That agrees with the notion that cadastre relates as much to people and institutions, as it relates to land, and that cadastral systems are shaped by social, political and economic conditions, as well as technology. Since the geodetic survey profession has been the keeper of the cadastre, geodetic surveyors will have to deal ever more with social science matters, a fact that universities will have to consider","world-wide , much attention have be give to cadastral development . as aconsequence of experience make during recent decade , several author have state the need for research in the domain of cadastre and propose methodology to be use . the paper contribute to the acceptance of research methodology need for cadastral development , and thereby enhance theory in the cadastral domain . the paper review nine publication on cadastre and identify the methodology use . the review focus on the institutional , social , political and economic aspect of cadastral development , rather than on the technical aspect . the main conclusion be that the methodology use be largely those of the social science . that agree with the notion that cadastre relate as much to people and institution , as it relate to land , and that cadastral system be shape by social , political and economic condition , as well as technology . since the geodetic survey profession have be the keeper of the cadastre , geodetic surveyor will have to deal ever more with social science matter , a fact that university will have to consider",171,0.636363636
8716,Computers and IT,"Although the paper recognises the merits of the evolution of cadastral systems towards an increased capability over time, it promotes a radical introduction or overhaul of existing cadastral systems. It encourages the development of a capability to cope with some key drivers of major change. These have been identified as globalisation, the advent of fully automated cadastral environments, improved decentralised methods of governance and greatly improved service delivery of future cadastral systems to a wide range of users. The paper promotes the registration of title supported by government guarantee as an effective means for rapidly introducing cadastral systems to facilitate globally competitive land markets in developing countries. In developing automated environments for cadastral systems, the need to completely re-engineer and redesign cadastral systems to meet basic cadastral principles and responsiveness to individual user needs is promoted. In this environment, highly decentralised cadastral operations and administration combined with light regulatory control are advocated as a future governance strategy. With regard to the level of services to users, an emphasis on recognising and serving the future needs of users is seen as essential. International and national professional and user organisations involved in land administration are seen as an important vehicle for developing strategies and providing evaluation to guide the over-arching development of cadastral systems around the world","although the paper recognize the merit of the evolution of cadastral systemstoward an increase capability over time , it promote a radical introduction or overhaul of exist cadastral system . it encourage the development of a capability to cope with some key driver of major change . these have be identify as globalisation , the advent of fully automate cadastral environment , improve decentralise method of governance and greatly improved service delivery of future cadastral system to a wide range of user . the paper promote the registration of title support by government guarantee as an effective mean for rapidly introduce cadastral system to facilitate globally competitive land market in develop country . in develop automate environment for cadastral system , the need to completely re-engineer and redesign cadastral system to meet basic cadastral principle and responsiveness to individual user need be promote . in this environment , highly decentralise cadastral operation and administration combine with light regulatory control be advocate as a future governance strategy . with regard to the level of service to user , an emphasis on recognize and serve the future need of user be see as essential . international and national professional and user organization involve in land administration be see as an important vehicle for develop strategy and provide evaluation to guide the over-arching development of cadastral system around the world",214,0.777777778
8717,Computers and IT,"Although the establishment of a land administration system is enough of a challenge as it is, the task of keeping the system up to date with developments in society is even more challenging. Initial adjudication and cadastral mapping basically record land tenure as it exists at a given moment, i. e. the static situation. The paper aims to analyse the developments that might occur in a society with respect to tenure, value and use of land. These developments constitute a dynamic component of land administration. As land administration systems have to serve society on a long-term basis and normally have a long-term return on investment, the author recommends taking into account both the static and dynamic component when designing land administration systems","although the establishment of a land administration system be enough of a challenge as it be , the task of keep the system up to date with development in society be even more challenging . initial adjudication and cadastral mapping basically record land tenure as it exist at a give moment , i.e. the static situation . the paper aim to analyze the development that might occur in a society with respect to tenure , value and use of land . these development constitute a dynamic component of land administration . as land administration system have to serve society on a long-term basis and normally have a long-term return on investment , the author recommend take into account both the static and dynamic component when design land administration system",121,0.636363636
8718,Computers and IT,"We analyze the (3, 2, 1)-Shell Sort algorithm under the usual random permutation model","we analyze the (3, 2, 1)-shell sort algorithm under the usual randompermutation model",13,0.3
8719,Computers and IT,"The movement from client-server computing to multi-tier computing has created a potpourri of so-called middleware systems, including application servers, workflow products, EAI systems, ETL systems and federated data systems. We argue that the explosion in middleware has created a myriad of poorly integrated systems with overlapping functionality. The world would be well served by considerable consolidation, and we present some of the ways this might happen. Some of the points covered in the article have been previously explored by P. Bernstein (1996)","the movement from client-server computing to multi-tier computing has created apotpourri of so-called middleware systems, including application servers, workflow products, eai systems, etl systems and federated data systems. we argue that the explosion in middleware has created a myriad of poorly integrated systems with overlapping functionality. the world would be well served by considerable consolidation, and we present some of the ways this might happen. some of the points covered in the article have been previously explored by p. bernstein (1996)",81,0.7
8720,Computers and IT,"When an XML document conforms to a given type (e. g. a DTD or an XML schema type) it is called a valid document. Checking if a given XML document is valid is called the validation problem, and is typically performed by a parser (hence, validating parser), more precisely it is performed right after parsing, by the same program module. In practice however, XML documents are often generated dynamically, by some program: checking whether all XML documents generated by the program are valid WRT a given type is called the typechecking problem. While a validation analyzes an XML document, a type checker analyzes a program, and the problem's difficulty is a function of the language in which that program is expressed. The XML typechecking problem has been investigated recently and the XQuery Working Group adopted some of these techniques for typechecking XQuery. All these techniques, however, have limitations which need to be understood and further explored and investigated. We define the XML typechecking problem, and present current approaches to typechecking, discussing their limitations","when an xml document conforms to a given type (e.g. a dtd or an xml schematype) it is called a valid document. checking if a given xml document is valid is called the validation problem, and is typically performed by a parser (hence, validating parser), more precisely it is performed right after parsing, by the same program module. in practice however, xml documents are often generated dynamically, by some program: checking whether all xml documents generated by the program are valid wrt a given type is called the typechecking problem. while a validation analyzes an xml document, a type checker analyzes a program, and the problem's difficulty is a function of the language in which that program is expressed. the xml typechecking problem has been investigated recently and the xquery working group adopted some of these techniques for typechecking xquery. all these techniques, however, have limitations which need to be understood and further explored and investigated. we define the xml typechecking problem, and present current approaches to typechecking, discussing their limitations",171,0.75
8721,Computers and IT,"Semantic B2B integration architectures must enable enterprises to communicate standards-based B2B events like purchase orders with any potential trading partner. This requires not only back end application integration capabilities to integrate with e. g. enterprise resource planning (ERP) systems as the company-internal source and destination of B2B events, but also a capability to implement every necessary B2B protocol like electronic data interchange (EDI), RosettaNet as well as more generic capabilities like Web services (WS). This paper shows the placement and functionality of B2B engines in semantic B2B integration architectures that implement a generic framework for modeling and executing any B2B protocol. A detailed discussion shows how a B2B engine can provide the necessary abstractions to implement any standard-based B2B protocol or any trading partner specific specialization","semantic b2b integration architectures must enable enterprises to communicatestandards-based b2b events like purchase orders with any potential trading partner. this requires not only back end application integration capabilities to integrate with e.g. enterprise resource planning (erp) systems as the company-internal source and destination of b2b events, but also a capability to implement every necessary b2b protocol like electronic data interchange (edi), rosettanet as well as more generic capabilities like web services (ws). this paper shows the placement and functionality of b2b engines in semantic b2b integration architectures that implement a generic framework for modeling and executing any b2b protocol. a detailed discussion shows how a b2b engine can provide the necessary abstractions to implement any standard-based b2b protocol or any trading partner specific specialization",124,0.7
8722,Computers and IT,"The need for supply chain integration (SCI) methodologies has been increasing as a consequence of the globalization of production and sales, and the advancement of enabling information technologies. In this paper, we describe our experience with implementing and modeling SCIs. We present the integration architecture and the software components of our prototype implementation. We then discuss a variety of information sharing methodologies. Then, within the framework of a multi-echelon supply chain process model spanning multiple organizations, we summarize research on the benefits of intraorganizational knowledge sharing, and we discuss performance scalability","the need for supply chain integration (sci) methodologies has been increasingas a consequence of the globalization of production and sales, and the advancement of enabling information technologies. in this paper, we describe our experience with implementing and modeling scis. we present the integration architecture and the software components of our prototype implementation. we then discuss a variety of information sharing methodologies. then, within the framework of a multi-echelon supply chain process model spanning multiple organizations, we summarize research on the benefits of intraorganizational knowledge sharing, and we discuss performance scalability",90,0.833333333
8723,Computers and IT,"In this paper, we introduce a multi-agent system architecture and an implemented prototype for a software component marketplace. We emphasize the ontological perspective by discussing ontology modeling for the component marketplace, UML extensions for ontology modeling, and the idea of ontology transfer which makes the multi-agent system adapt itself to dynamically changing ontologies","in this paper , we introduce a multi-agent system architecture and an implement prototype for a software component marketplace . we emphasize the ontological perspective by discuss ontology modeling for the component marketplace , uml extension for ontology modeling , and the idea of ontology transfer which make the multi-agent system adapt itself to dynamically change ontology",53,0.857142857
8724,Computers and IT,"Business-to-business electronic commerce (B2B EC) opens up new possibilities for trade. For example, new business partners from around the globe can be found, their offers can be compared, even complex negotiations can be conducted electronically, and a contract can be drawn up and fulfilled via an electronic marketplace. However, sophisticated data management is required to provide such facilities. In this paper, the results of a multi-national project on creating a business-to-business electronic marketplace for small and medium-sized enterprises are presented. Tools for information discovery, protocol-based negotiations, and monitored contract enactment are provided and based on a business data repository. The repository integrates heterogeneous business data with business communication. Specific problems such as multilingual nature, data ownership, and traceability of contracts and related negotiations are addressed and it is shown that the present approach provides efficient business data management for B2B EC","business-to-business electronic commerce (b2b ec) opens up new possibilitiesfor trade. for example, new business partners from around the globe can be found, their offers can be compared, even complex negotiations can be conducted electronically, and a contract can be drawn up and fulfilled via an electronic marketplace. however, sophisticated data management is required to provide such facilities. in this paper, the results of a multi-national project on creating a business-to-business electronic marketplace for small and medium-sized enterprises are presented. tools for information discovery, protocol-based negotiations, and monitored contract enactment are provided and based on a business data repository. the repository integrates heterogeneous business data with business communication. specific problems such as multilingual nature, data ownership, and traceability of contracts and related negotiations are addressed and it is shown that the present approach provides efficient business data management for b2b ec",140,0.785714286
8725,Computers and IT,"Solving queries to support e-commerce transactions can involve retrieving and integrating information from multiple information resources. Often, users don't care which resources are used to answer their query. In such situations, the ideal solution would be to hide from the user the details of the resources involved in solving a particular query. An example would be providing seamless access to a set of heterogeneous electronic product catalogues. There are many problems that must be addressed before such a solution can be provided. In this paper, we discuss a number of these problems, indicate how we have addressed these and go on to describe the proof-of-concept demonstration system we have developed","solve query to support e-commerce transaction can involve retrieve andintegrat information from multiple information resource . often , user do n't care which resource be use to answer their query . in such situation , the ideal solution would be to hide from the user the detail of the resource involve in solve a particular query . an example would be provide seamless access to a set of heterogeneous electronic product catalogue . there be many problem that must be address before such a solution can be provide . in this paper , we discuss a number of these problem , indicate how we have address these and go on to describe the proof-of-concept demonstration system we have develop",109,0.5
8726,Computers and IT,"In order to support dynamic setup of business processes among independent organizations, a formal standard schema for describing the business processes is basically required. The ebXML framework provides such a specification schema called BPSS (Business Process Specification Schema) which is available in two standalone representations: a UML version, and an XML version. The former, however, is not intended for the direct creation of business process specifications, but for defining specification elements and their relationships required for creating an ebXML-compliant business process specification. For this reason, it is very important to support conceptual modeling that is well organized and directly matched with major modeling concepts. This paper deals with how to represent and manage B2B business processes using UML-compliant diagrams. The major challenge is to organize UML diagrams in a natural way that is well suited to the business process meta-model and then to transform the diagrams into an XML version. This paper demonstrates the usefulness of conceptually modeling business processes by prototyping a business process editor tool called ebDesigner","in order to support dynamic setup of business processes among independent organizations, a formal standard schema for describing the business processes is basically required. the ebxml framework provides such a specification schema called bpss (business process specification schema) which is available in two standalone representations: a uml version, and an xml version. the former, however, is not intended for the direct creation of business process specifications, but for defining specification elements and their relationships required for creating an ebxml-compliant business process specification. for this reason, it is very important to support conceptual modeling that is well organized and directly matched with major modeling concepts. this paper deals with how to represent and manage b2b business processes using uml-compliant diagrams. the major challenge is to organize uml diagrams in a natural way that is well suited to the business process meta-model and then to transform the diagrams into an xml version. this paper demonstrates the usefulness of conceptually modeling business processes by prototyping a business process editor tool called ebdesigner",169,0.8
8727,Computers and IT,Putting electronic business on a sound foundation-model theoretically as well as technologically-is a central challenge for research as well as commercial development. This paper concentrates on the discovery and negotiation phase of concluding an agreement based on a contract. We present a methodology for moving seamlessly from a many-to-many relationship in the discovery phase to a one-to-one relationship in the contract negotiation phase. Making the content of contracts persistent is achieved by reconstructing contract templates by means of mereologic (logic of the whole-part relation). Possibly nested sub-structures of the contract template are taken as a basis for negotiation in a dialogical way. For the negotiation itself the contract templates are extended by implications (logical) and sequences (topical),putting electronic business on a sound foundation-model theoretically as wellas technologically-is a central challenge for research as well as commercial development. this paper concentrates on the discovery and negotiation phase of concluding an agreement based on a contract. we present a methodology for moving seamlessly from a many-to-many relationship in the discovery phase to a one-to-one relationship in the contract negotiation phase. making the content of contracts persistent is achieved by reconstructing contract templates by means of mereologic (logic of the whole-part relation). possibly nested sub-structures of the contract template are taken as a basis for negotiation in a dialogical way. for the negotiation itself the contract templates are extended by implications (logical) and sequences (topical),116,0.909090909
8728,Computers and IT,"Service based architectures are a powerful approach to meet the fast evolution of business rules and the corresponding software. An active functionality service that detects events and involves the appropriate business rules is a critical component of such a service-based middleware architecture. In this paper we present an active functionality service that is capable of detecting events in heterogeneous environments, it uses an integral ontology-based approach for the semantic interpretation of heterogeneous events and data, and provides notifications through a publish/subscribe notification mechanism. The power of this approach is illustrated with the help of an auction application and through the personalization of car and driver portals in Internet-enabled vehicles","service base architecture be a powerful approach to meet the fast evolutionof business rule and the corresponding software . an active functionality service that detect event and involve the appropriate business rule be a critical component of such a service-based middleware architecture . in this paper we present an active functionality service that be capable of detect event in heterogeneous environment , it use an integral ontology-based approach for the semantic interpretation of heterogeneous event and data , and provide notification through a publish/subscribe notification mechanism . the power of this approach be illustrate with the help of an auction application and through the personalization of car and driver portal in internet-enabled vehicle",108,0.642857143
8729,Computers and IT,"The minimum k-assignment of an m*n matrix X is the minimum sum of k entries of X, no two of which belong to the same row or column. Coppersmith and Sorkin conjectured that if X is generated by choosing each entry independently from the exponential distribution with mean 1, then the expected value of its minimum k-assignment is given by an explicit formula, which has been proven only in a few cases. In this paper we describe our efforts to prove the Coppersmith-Sorkin conjecture by considering the more general situation where the entries x/sub ij/ of X are chosen independently from different distributions. In particular, we require that x/sub ij/ be chosen from the exponential distribution with mean 1/r/sub i/c/sub j/. We conjecture an explicit formula for the expected value of the minimum k-assignment of such X and give evidence for this formula","the minimum k-assignment of an m * n matrix x be the minimum sum of k entry ofx , no two of which belong to the same row or column . coppersmith and sorkin conjecture that if x be generate by choose each entry independently from the exponential distribution with mean 1 , then the expect value of its minimum k-assignment be give by an explicit formula , which have be prove only in a few case . in this paper we describe our effort to prove the coppersmith-sorkin conjecture by consider the more general situation where the entry x/sub ij / of x be choose independently from different distribution . in particular , we require that x/sub ij / be choose from the exponential distribution with mean 1/r/sub i/c/sub j / . we conjecture an explicit formula for the expect value of the minimum k-assignment of such x and give evidence for this formula",142,0.6
8730,Computers and IT,"During 2001, the Enterprise Engineering Laboratory at George Mason University was contracted by the Boeing Company to develop an eHub capability for aerospace suppliers in Taiwan. In a laboratory environment, the core technology was designed, developed, and tested, and now a large first-tier aerospace supplier in Taiwan is commercializing the technology. The project objective was to provide layered network and application services for transporting XML-based business transaction flows across multi-tier, heterogeneous data processing environments. This paper documents the business scenario, the eHub application, and the network transport mechanisms that were used to build the n-tier hub. In contrast to most eHubs, this solution takes the point of view of suppliers, pushing data in accordance with supplier requirements; hence, enhancing the probability of supplier adoption. The unique contribution of this project is the development of an eHub that meets the needs of small and medium enterprises (SMEs) and first-tier suppliers","during 2001, the enterprise engineering laboratory at george mason universitywas contracted by the boeing company to develop an ehub capability for aerospace suppliers in taiwan. in a laboratory environment, the core technology was designed, developed, and tested, and now a large first-tier aerospace supplier in taiwan is commercializing the technology. the project objective was to provide layered network and application services for transporting xml-based business transaction flows across multi-tier, heterogeneous data processing environments. this paper documents the business scenario, the ehub application, and the network transport mechanisms that were used to build the n-tier hub. in contrast to most ehubs, this solution takes the point of view of suppliers, pushing data in accordance with supplier requirements; hence, enhancing the probability of supplier adoption. the unique contribution of this project is the development of an ehub that meets the needs of small and medium enterprises (smes) and first-tier suppliers",148,0.727272727
8731,Computers and IT,"Personalization generally refers to making a Web site more responsive to the unique and individual needs of each user. We argue that for personalization to work effectively, detailed and interoperable user profiles should be globally available for authorized sites, and these profiles should dynamically reflect changes in user interests. Creating user profiles from user click-stream data seems to be an effective way of generating detailed and dynamic user profiles. However, a user profile generated in this way is available only on the computer where the user accesses his browser, and is inaccessible when the same user works on a different computer. On the other hand, integration of the Internet with telecommunication networks has made it possible for the users to connect to the Web with a variety of mobile devices as well as desktops. This requires that user profiles should be available to any desktop or mobile device on the Internet that users choose to work with. In this paper, we address these problems through the concept of ""trusted authority"". A user agent at the client side that captures the user click stream, dynamically generates a navigational history 'log' file in Extensible Markup Language (XML). This log file is then used to produce 'user profiles' in a resource description framework (RDF). A user's right to privacy is provided through the Platform for Privacy Preferences (P3P) standard. User profiles are uploaded to the trusted authority and served next time the user connects to the Web","personalization generally refers to making a web site more responsive to theunique and individual needs of each user. we argue that for personalization to work effectively, detailed and interoperable user profiles should be globally available for authorized sites, and these profiles should dynamically reflect changes in user interests. creating user profiles from user click-stream data seems to be an effective way of generating detailed and dynamic user profiles. however, a user profile generated in this way is available only on the computer where the user accesses his browser, and is inaccessible when the same user works on a different computer. on the other hand, integration of the internet with telecommunication networks has made it possible for the users to connect to the web with a variety of mobile devices as well as desktops. this requires that user profiles should be available to any desktop or mobile device on the internet that users choose to work with. in this paper, we address these problems through the concept of ""trusted authority"". a user agent at the client side that captures the user click stream, dynamically generates a navigational history 'log' file in extensible markup language (xml). this log file is then used to produce 'user profiles' in a resource description framework (rdf). a user's right to privacy is provided through the platform for privacy preferences (p3p) standard. user profiles are uploaded to the trusted authority and served next time the user connects to the web",243,0.625
8732,Computers and IT,"This paper studies five strategies for storing XML documents including one that leaves documents in the file system, three that use a relational database system, and one that uses an object manager. We implement and evaluate each approach using a number of XQuery queries. A number of interesting insights are gained from these experiments and a summary of the advantages and disadvantages of the approaches is presented","this paper study five strategy for store xml document include one thatleav document in the file system , three that use a relational database system , and one that use an object manager . we implement and evaluate each approach use a number of xquery query . a number of interesting insight be gain from these experiment and a summary of the advantage and disadvantage of the approach be present",66,0.666666667
8733,Computers and IT,"Silberer's company, CompLete, is making a specialty of workflow process analysis","silberer 's company , complete , be make a specialty of workflow processanalysi",10,0.333333333
8734,Computers and IT,Software Technology wants to turn 23 years of reselling experience in the legal business into an asset in the accounting market,software technology want to turn 23 year of resell experience in the legalbusines into an asset in the accounting market,20,0.666666667
8735,Computers and IT,"Middle-market accounting software vendors are taking to the open road, by way of souped-up distribution suites that can track product as it wends its way from warehouse floor to customer site. Integration provides efficiencies, and cost savings","middle-market accounting software vendor be take to the open road , by wayof souped-up distribution suite that can track product as it wend its way from warehouse floor to customer site . integration provide efficiency , and cost saving",36,0.333333333
8736,Computers and IT,"The move from CD to the Web looks unstoppable. Besides counting how many thousands of electronic tax forms they offer, vendors are rapidly moving those documents to the Web","the move from cd to the web look unstoppable . besides count how manythousand of electronic tax form they offer , vendor be rapidly move those document to the web",28,0.222222222
8737,Computers and IT,CPA providers of financial planning services are providing clients with a unified view of their investments,cpa provider of financial planning service be provide client with aunified view of their investment,15,0.5
8738,Computers and IT,"There's a lot of life in accounting practice management software, a valuable category that has been subject to much change in the last few years. Web-based time tracking grows in popularity. Looks at CCH ProSystem fx Practice, CMS Open Solutions 6, Creative Solutions Practice, Time Matters, CPASoftware Visual Practice Management, and Abak","there ' a lot of life in accounting practice management software , a valuablecategory that have be subject to much change in the last few year . web-based time track grow in popularity . look at cch prosystem fx practice , cm open solution 6 , creative solution practice , time matter , cpasoftware visual practice management , and abak",51,0.875
8739,Computers and IT,"Many NP-hard languages can be ""decided"" in subexponential time if the definition of ""decide"" is relaxed only slightly. Rubinfeld and Sudan introduced the notion of property testers, probabilistic algorithms that can decide, with high probability, if a function has a certain property or if it is far from any function having this property. Goldreich, Goldwasser, and Ron constructed property testers with constant query complexity for dense instances of a large class of graph problems. Since many graph problems can be viewed as special cases of the Constraint Satisfaction Problem on Boolean domains, it is natural to try to construct property testers for more general cases of the Constraint Satisfaction Problem. In this paper, we give explicit constructions of property testers using a constant number of queries for dense instances of Constraint Satisfaction Problems where the constraints have constant arity and the variables assume values in some domain of finite size","many np-hard language can be `` decide '' in subexponential time if thedefinition of `` decide '' be relax only slightly . rubinfeld and sudan introduce the notion of property tester , probabilistic algorithm that can decide , with high probability , if a function have a certain property or if it be far from any function have this property . goldreich , goldwasser , and ron construct property tester with constant query complexity for dense instance of a large class of graph problem . since many graph problem can be view as special case of the constraint satisfaction problem on boolean domain , it be natural to try to construct property tester for more general case of the constraint satisfaction problem . in this paper , we give explicit construction of property tester use a constant number of query for dense instance of constraint satisfaction problem where the constraint have constant arity and the variable assume value in some domain of finite size",149,0.9
8740,Computers and IT,"With the market for accounting software nearing saturation, vendors are training resellers in the subtleties of the cross-sell. The rewards can be great. The key is knowing when to focus, and when to partner","with the market for accounting software near saturation , vendor aretrain reseller in the subtlety of the cross-sell . the reward can be great . the key be know when to focus , and when to partner",33,0.333333333
8741,Computers and IT,"Technology is providing a positive impact on delivery mechanisms employed in distance education at the university level. Some institutions are incorporating distance education as a way to extend the classroom. Other institutions are investigating new delivery mechanisms, which support a revised perspective on education. These latter institutions are revising their processes for interacting with students, and taking a more ""learner centered"" approach to the delivery of education. This article discusses the impact of technology on the delivery mechanisms employed in distance education. A framework is proposed here, which presents a description of alternative modes of generic delivery mechanisms. It is suggested that those institutions, which adopt a delivery mechanism employing an asynchronous mode, can gain the most benefit from technology. This approach seems to represent the only truly innovative use of technology in distance education. The approach creates a student-oriented environment while maintaining high levels of interaction, both of which are factors that contribute to student satisfaction with their overall educational experience","technology be provide a positive impact on delivery mechanism employ in distance education at the university level . some institution be incorporate distance education as a way to extend the classroom . other institution be investigate new delivery mechanism , which support a revise perspective on education . these latter institution be revise their process for interact with student , and take a more `` learn center '' approach to the delivery of education . this article discuss the impact of technology on the delivery mechanism employ in distance education . a framework be propose here , which present a description of alternative mode of generic delivery mechanism . it be suggest that those institution , which adopt a delivery mechanism employ an asynchronous mode , can gain the most benefit from technology . this approach seem to represent the only truly innovative use of technology in distance education . the approach create a student-oriented environment while maintain high level of interaction , both of which be factor that contribute to student satisfaction with their overall educational experience",162,0.428571429
8742,Computers and IT,"With increasing globalization of business, the management of IT in international organizations is faced with the complex task of dealing with the difference between local and international IT needs. This study evaluates, and compares, the level of IT maturity and the critical success factors (CSFs) in selected geographic regions, namely, Norway, Australia/New Zealand, North America, Europe, Asia/Pacific, and India. The results show that significant differences in the IT management needs in these geographic regions exist, and that the IT management operating in these regions must balance the multiple critical success factors for achieving an optimal local-global mix for business success","with increasing globalization of business, the management of it ininternational organizations is faced with the complex task of dealing with the difference between local and international it needs. this study evaluates, and compares, the level of it maturity and the critical success factors (csfs) in selected geographic regions, namely, norway, australia/new zealand, north america, europe, asia/pacific, and india. the results show that significant differences in the it management needs in these geographic regions exist, and that the it management operating in these regions must balance the multiple critical success factors for achieving an optimal local-global mix for business success",99,0.8
8743,Computers and IT,"Many organizations, regardless of size, engage in at least one, and often many information system projects each year. Many of these projects consume massive amounts of resources, and may cost as little as a few thousand dollars to ten, and even hundreds of millions of dollars. Needless to say, the investment of time and resources into these ventures are of significant concern to chief information officers (CIOs), executives staff members, project managers, and others in leadership positions. This paper describes the results of a survey performed between Australia and the United States regarding factors leading to IS project failure. The findings suggest that, among other things, end user involvement and executive management leadership are key indicators influencing IS project failure","many organizations, regardless of size, engage in at least one, and often manyinformation system projects each year. many of these projects consume massive amounts of resources, and may cost as little as a few thousand dollars to ten, and even hundreds of millions of dollars. needless to say, the investment of time and resources into these ventures are of significant concern to chief information officers (cios), executives staff members, project managers, and others in leadership positions. this paper describes the results of a survey performed between australia and the united states regarding factors leading to is project failure. the findings suggest that, among other things, end user involvement and executive management leadership are key indicators influencing is project failure",119,0.8
8744,Computers and IT,"The study examined the perceptions of information systems (IS) developers from Japan and the United States regarding the strategies that are considered most important for successful implementation of an IS. The results of principal component analysis revealed that the IS strategies could be reduced to five components: (1) characteristics of the team members, (2) characteristics of the project leader, (3) management/user input, (4) proper technology, and (5) communication. The results indicated that there was a significant difference in the perceptions of Japanese and US developers with respect to the importance of the five components. Japanese developers perceived the project leader as the most crucial component for determining the success of an IS project. Team member characteristics was viewed as the least important by Japanese developers. On the other hand, developers from the US viewed communications as the most critical component. Project leader characteristics were perceived to be the least important by US developers. The results were discussed in terms of cultural differences","the study examined the perceptions of information systems (is) developers from japan and the united states regarding the strategies that are considered most important for successful implementation of an is. the results of principal component analysis revealed that the is strategies could be reduced to five components: (1) characteristics of the team members, (2) characteristics of the project leader, (3) management/user input, (4) proper technology, and (5) communication. the results indicated that there was a significant difference in the perceptions of japanese and us developers with respect to the importance of the five components. japanese developers perceived the project leader as the most crucial component for determining the success of an is project. team member characteristics was viewed as the least important by japanese developers. on the other hand, developers from the us viewed communications as the most critical component. project leader characteristics were perceived to be the least important by us developers. the results were discussed in terms of cultural differences",162,0.666666667
8745,Computers and IT,"The IS/IT function has recently emerged from the peripheral aspects of the finance department to the centre of critical organisational change. There is an increasing dependency on its activities as systems extend beyond supporting the internal efficiency of the organisation to augmenting global performance. The growth of wide and local networks has resulted in communication possibilities that were not possible a few years ago. E-commerce challenges the achievements of the IS/IT function and is very prominent in the globalisation of modern organisations. The complexity and diversity of electronic exchange is also well documented (Hackney et al. , 2000). This has a number of impacts on the development and implementation of IS/IT solutions for organisations involved in international trade. It is a conjecture that the IS/IT function is critically important for the alignment of the business to meet the demands of global competition, through building internal marketing strategies and creating knowledge based communities. There is clear evidence that IS/IT can lead to improved business performance and potentially for sustained competitive advantage. This is obviously true through the advent of new and emerging technologies such as the Internet","the is/it function has recently emerged from the peripheral aspects of thefinance department to the centre of critical organisational change. there is an increasing dependency on its activities as systems extend beyond supporting the internal efficiency of the organisation to augmenting global performance. the growth of wide and local networks has resulted in communication possibilities that were not possible a few years ago. e-commerce challenges the achievements of the is/it function and is very prominent in the globalisation of modern organisations. the complexity and diversity of electronic exchange is also well documented (hackney et al., 2000). this has a number of impacts on the development and implementation of is/it solutions for organisations involved in international trade. it is a conjecture that the is/it function is critically important for the alignment of the business to meet the demands of global competition, through building internal marketing strategies and creating knowledge based communities. there is clear evidence that is/it can lead to improved business performance and potentially for sustained competitive advantage. this is obviously true through the advent of new and emerging technologies such as the internet",184,0.5
8746,Computers and IT,Studies the reliability with sensor failures of the asymptotic tracking problem for linear time invariant systems using the factorization approach. The plant is two-output and the compensator is two-degree-of-freedom. Necessary and sufficient conditions are presented for the general problem and a simple solution is given for problems with stable plants,study the reliability with sensor failure of the asymptotic tracking problemfor linear time invariant system use the factorization approach . the plant be two-output and the compensator be two-degree-of-freedom . necessary and sufficient condition be present for the general problem and a simple solution be give for problem with stable plant,49,0.75
8747,Computers and IT,"For discrete-time scalar systems, we propose an approach for designing feedback controllers of fixed order to minimize an upper bound on the peak magnitude of the tracking error to a given command input. The work makes use of linear programming to design over a class of closed-loop systems proposed for the rejection of non-zero initial conditions and bounded disturbances. We incorporate performance robustness in the form of a guaranteed upper bound on the peak magnitude of the tracking error under plant coprime factor uncertainty","for discrete-time scalar system , we propose an approach for design feedbackcontroller of fix order to minimize an upper bind on the peak magnitude of the tracking error to a give command input . the work make use of linear programming to design over a class of closed-loop system propose for the rejection of non-zero initial condition and bound disturbance . we incorporate performance robustness in the form of a guarantee upper bind on the peak magnitude of the tracking error under plant coprime factor uncertainty",83,0.545454545
8748,Computers and IT,"Estimation of a single-input single-output block-oriented model is studied. The model consists of a linear block embedded between two static nonlinear gains. Hence, it is called an N-L-N Hammerstein-Wiener model. First, the model structure is motivated and the disturbance model is discussed. The paper then concentrates on parameter estimation. A relaxation iteration scheme is proposed by making use of a model structure in which the error is bilinear-in-parameters. This leads to a simple algorithm which minimizes the original loss function. The convergence and consistency of the algorithm are studied. In order to reduce the variance error, the obtained linear model is further reduced using frequency weighted model reduction. A simulation study is used to illustrate the method","estimation of a single-input single-output block-oriented model be study . themodel consist of a linear block embed between two static nonlinear gain . hence , it be call an n-l-n hammerstein-wiener model . first , the model structure be motivate and the disturbance model be discuss . the paper then concentrate on parameter estimation . a relaxation iteration scheme be propose by make use of a model structure in which the error be bilinear-in-parameter . this lead to a simple algorithm which minimize the original loss function . the convergence and consistency of the algorithm be study . in order to reduce the variance error , the obtain linear model be further reduce use frequency weighted model reduction . a simulation study be use to illustrate the method",116,0.857142857
8749,Computers and IT,Stability and L/sub 2/ gain properties of linear parameter-varying systems are obtained under assumed bounds on either the maximum or average value of the parameter rate,stability and l/sub 2 / gain property of linear parameter-varying system areobtain under assumed bound on either the maximum or average value of the parameter rate,25,0.666666667
8750,Computers and IT,"We consider the following long-range percolation model: an undirected graph with the node set {0, 1, . . . , N}/sup d/, has edges (x, y) selected with probability approximately= beta /||x - y||/sup s/ if ||x - y|| 1, and with probability 1 if ||x - y|| = 1, for some parameters beta , s 0. This model was introduced by who obtained bounds on the diameter of this graph for the one-dimensional case d = 1 and for various values of s, but left cases s = 1, 2 open. We show that, with high probability, the diameter of this graph is Theta (log N/log log N) when s = d, and, for some constants 0 2d. We also provide a simple proof that the diameter is at most log/sup O(1)/ N with high probability, when d s 2d, established previously in Benjamini and Berger (2001)","we consider the following long-range percolation model: an undirected graphwith the node set {0, 1, . . . , n}/sup d/, has edges (x, y) selected with probability approximately= beta /||x - y||/sup s/ if ||x - y|| > 1, and with probability 1 if ||x - y|| = 1, for some parameters beta , s > 0. this model was introduced by who obtained bounds on the diameter of this graph for the one-dimensional case d = 1 and for various values of s, but left cases s = 1, 2 open. we show that, with high probability, the diameter of this graph is theta (log n/log log n) when s = d, and, for some constants 0  2d. we also provide a simple proof that the diameter is at most log/sup o(1)/ n with high probability, when d < s < 2d, established previously in benjamini and berger (2001)",152,0.571428571
8751,Computers and IT,"Probabilistic robustness analysis and synthesis for nonlinear systems with uncertain parameters are presented. Monte Carlo simulation is used to estimate the likelihood of system instability and violation of performance requirements subject to variations of the probabilistic system parameters. Stochastic robust control synthesis searches the controller design parameter space to minimize a cost that is a function of the probabilities that design criteria will not be satisfied. The robust control design approach is illustrated by a simple nonlinear example. A modified feedback linearization control is chosen as controller structure, and the design parameters are searched by a genetic algorithm to achieve the tradeoff between stability and performance robustness","probabilistic robustness analysis and synthesis for nonlinear system withuncertain parameter be present . monte carlo simulation be use to estimate the likelihood of system instability and violation of performance requirement subject to variation of the probabilistic system parameter . stochastic robust control synthesis search the controller design parameter space to minimize a cost that be a function of the probability that design criterion will not be satisfy . the robust control design approach be illustrate by a simple nonlinear example . a modify feedback linearization control be choose as controller structure , and the design parameter be search by a genetic algorithm to achieve the tradeoff between stability and performance robustness",106,0.615384615
8752,Computers and IT,MUSCADET is a knowledge-based theorem prover based on natural deduction. It has participated in CADE Automated theorem proving System Competitions. The results show its complementarity with regard to resolution-based provers. This paper presents some of its crucial methods and gives some examples of MUSCADET proofs from the last competition (CASC-JC in IJCAR 2001),muscadet is a knowledge-based theorem prover based on natural deduction. it hasparticipated in cade automated theorem proving system competitions. the results show its complementarity with regard to resolution-based provers. this paper presents some of its crucial methods and gives some examples of muscadet proofs from the last competition (casc-jc in ijcar 2001),52,0.833333333
8753,Computers and IT,"The first-order theorem prover SCOTT has been through a series of versions over some ten years. The successive provers, while retaining the same underlying technology, have used radically different algorithms and shown wide differences of behaviour. The development process has depended heavily on experiments with problems from the TPTP library and has been sharpened by participation in CASC each year since 1997. We outline some of the difficulties inherent in designing and refining a theorem prover as complex as SCOTT, and explain our experimental methodology. While SCOTT is not one of the systems which have been highly optimised for CASC, it does help to illustrate the influence of both CASC and the TPTP library on contemporary theorem proving research","the first-order theorem prover scott have be through a series of version oversome ten year . the successive prover , while retain the same underlie technology , have use radically different algorithm and show wide difference of behavior . the development process have depend heavily on experiment with problem from the tptp library and have be sharpen by participation in casc each year since 1997 . we outline some of the difficulty inherent in design and refine a theorem prover as complex as scott , and explain our experimental methodology . while scott be not one of the system which have be highly optimise for casc , it do help to illustrate the influence of both casc and the tptp library on contemporary theorem prove research",118,0.625
8754,Computers and IT,"The architecture of the WALDMEISTER prover for unit equational deduction is based on a strict separation of active and passive facts. After an inspection of the system's proof procedure, the representation of each of the central data structures is outlined, namely indexing for the active facts, compression for the passive facts, successor sets for the hypotheses, and minimal recording of inference steps for the proof object. In order to cope with large search spaces, specialized redundancy criteria are employed, and the empirically gained control knowledge is integrated to ease the use of the system. The paper concludes with a quantitative comparison of the WALDMEISTER versions over the years, and a view of the future prospects","the architecture of the waldmeister prover for unit equational deduction isbased on a strict separation of active and passive fact . after an inspection of the system 's proof procedure , the representation of each of the central data structure be outline , namely indexing for the active fact , compression for the passive fact , successor set for the hypothesis , and minimal recording of inference step for the proof object . in order to cope with large search space , specialize redundancy criterion be employ , and the empirically gain control knowledge be integrate to ease the use of the system . the paper conclude with a quantitative comparison of the waldmeister version over the year , and a view of the future prospect",114,0.785714286
8755,Computers and IT,"We describe the superposition-based theorem prover E. E is a sound and complete prover for clausal first order logic with equality. Important properties of the prover include strong redundancy elimination criteria, the DISCOUNT loop proof procedure, a very flexible interface for specifying search control heuristics, and an efficient inference engine. We also discuss the strengths and weaknesses of the system","we describe the superposition-based theorem prover e. e be a sound and completeprover for clausal first order logic with equality . important property of the prover include strong redundancy elimination criterion , the discount loop proof procedure , a very flexible interface for specify search control heuristic , and an efficient inference engine . we also discuss the strength and weakness of the system",59,0.533333333
8756,Computers and IT,"We describe VAMPIRE: a high-performance theorem prover for first-order logic. As our description is mostly targeted to the developers of such systems and specialists in automated reasoning, it focuses on the design of the system and some key implementation features. We also analyze the performance of the prover at CASC-JC","we describe vampire : a high-performance theorem prover for first-order logic . as our description be mostly target to the developer of such system and specialist in automated reasoning , it focus on the design of the system and some key implementation feature . we also analyze the performance of the prover at casc-jc",49,0.571428571
8757,Computers and IT,"Researchers who make theoretical advances also need some way to demonstrate that an advance really does have general, overall positive consequences for system performance. For this it is necessary to evaluate the system on a set of problems that is sufficiently large and diverse to be somehow representative of the intended application area as a whole. It is only a small step from system evaluation to a communal system competition. The CADE ATP System Competition (CASC) has been run annually since 1996. Any competition is difficult to design and organize in the first instance, and to then run over the years. In order to obtain the full benefits of a competition, a thoroughly organized event, with an unambiguous and motivated design, is necessary. For some issues relevant to the CASC design, inevitable constraints have emerged. For other issues there have been several choices, and decisions have had to be made. This paper describes the evolution of CASC, paying particular attention to its design, design changes, and organization","researchers who make theoretical advances also need some way to demonstratethat an advance really does have general, overall positive consequences for system performance. for this it is necessary to evaluate the system on a set of problems that is sufficiently large and diverse to be somehow representative of the intended application area as a whole. it is only a small step from system evaluation to a communal system competition. the cade atp system competition (casc) has been run annually since 1996. any competition is difficult to design and organize in the first instance, and to then run over the years. in order to obtain the full benefits of a competition, a thoroughly organized event, with an unambiguous and motivated design, is necessary. for some issues relevant to the casc design, inevitable constraints have emerged. for other issues there have been several choices, and decisions have had to be made. this paper describes the evolution of casc, paying particular attention to its design, design changes, and organization",166,0.555555556
8758,Computers and IT,"This paper shows strong completeness of the system L for lattice valued logic given by S. Titani (1999), in which she formulates a lattice-valued set theory by introducing the logical implication which represents the order relation on the lattice. Syntax and semantics concerned are described and strong completeness is proved","this paper shows strong completeness of the system l for lattice valued logicgiven by s. titani (1999), in which she formulates a lattice-valued set theory by introducing the logical implication which represents the order relation on the lattice. syntax and semantics concerned are described and strong completeness is proved",49,0.833333333
8759,Computers and IT,"If A contained in omega , nor=2, and the function max({x/sub 1/, . . . , x/sub n/} intersection A) is partial recursive, it is easily seen that A is recursive. In this paper, we weaken this hypothesis in various ways (and similarly for ""min"" in place of ""max"") and investigate what effect this has on the complexity of A. We discover a sharp contrast between retraceable and co-retraceable sets, and we characterize sets which are the union of a recursive set and a co-r. e. , retraceable set. Most of our proofs are noneffective. Several open questions are raised","if a contained in omega , n>or=2, and the function max({x/sub 1/,...,x/subn/} intersection a) is partial recursive, it is easily seen that a is recursive. in this paper, we weaken this hypothesis in various ways (and similarly for ""min"" in place of ""max"") and investigate what effect this has on the complexity of a. we discover a sharp contrast between retraceable and co-retraceable sets, and we characterize sets which are the union of a recursive set and a co-r.e., retraceable set. most of our proofs are noneffective. several open questions are raised",92,0.6
8760,Computers and IT,"We say that a computably enumerable (c. e. ) degree b is a Lachlan nonsplitting base (LNB), if there is a computably enumerable degree a such that ab, and for any c. e. degrees w, vor=a, if aor=wVvV b then either aor=wV b or aor=vV b. In this paper we investigate the relationship between bounding and nonbounding of Lachlan nonsplitting bases and the high/low hierarchy. We prove that there is a non-Low/sub 2/ c. e. degree which bounds no Lachlan nonsplitting base","we say that a computably enumerable (c.e.) degree b is a lachlan nonsplittingbase (lnb), if there is a computably enumerable degree a such that a>b, and for any c.e. degrees w, v<or=a, if a<or=wvvv b then either a<or=wv b or a<or=vv b. in this paper we investigate the relationship between bounding and nonbounding of lachlan nonsplitting bases and the high/low hierarchy. we prove that there is a non-low/sub 2/ c.e. degree which bounds no lachlan nonsplitting base",77,0.333333333
8761,Computers and IT,"This paper deals with the application of receding horizon methods to hover and forward flight models of an experimental tethered flying wing developed at Caltech. The dynamics of the system are representative of a vertical landing and take off aircraft, such as a Harrier around hover, or a thrust-vectored aircraft such as F18-HARV or X-31 in forward flight. The adopted control methodology is a hybrid of receding horizon techniques and control Lyapunov function (CLF)-based ideas. First, a CLF is generated using quasi-LPV methods and then, by using the CLF as the terminal cost in the receding horizon optimization, stability is guaranteed. The main advantage of this approach is that stability can be guaranteed without imposing constraints in the on-line optimization, allowing the problem to be solved in a more efficient manner. Models of the experimental set-up are obtained for the hover and forward flight modes. Numerical simulations for different time horizons are presented to illustrate the effectiveness of the discussed methods. Specifically, it is shown that a mere upper bound on the cost-to-go is not an appropriate choice for a terminal cost, when the horizon length is short. Simulation results are presented using experimentally verified model parameters","this paper deals with the application of receding horizon methods to hover andforward flight models of an experimental tethered flying wing developed at caltech. the dynamics of the system are representative of a vertical landing and take off aircraft, such as a harrier around hover, or a thrust-vectored aircraft such as f18-harv or x-31 in forward flight. the adopted control methodology is a hybrid of receding horizon techniques and control lyapunov function (clf)-based ideas. first, a clf is generated using quasi-lpv methods and then, by using the clf as the terminal cost in the receding horizon optimization, stability is guaranteed. the main advantage of this approach is that stability can be guaranteed without imposing constraints in the on-line optimization, allowing the problem to be solved in a more efficient manner. models of the experimental set-up are obtained for the hover and forward flight modes. numerical simulations for different time horizons are presented to illustrate the effectiveness of the discussed methods. specifically, it is shown that a mere upper bound on the cost-to-go is not an appropriate choice for a terminal cost, when the horizon length is short. simulation results are presented using experimentally verified model parameters",196,0.421052632
8762,Computers and IT,"A hybrid and practical solution for relay evaluation is presented. Two main issues are taken into account: power system simulation and relay simulation, both of which consist of different stages. System simulation is carried out by means of EMTP and is complemented by additional features, such as filtering for location and determination of fault parameters that allow comparing simulated and actual fault records to improve and guarantee a correct system simulation. Relay simulation includes filtering algorithms, all the relaying units, and the decision logic. Playing simulated or real faults over the actual relay and comparing simulated and real responses can check for correct relay simulation","a hybrid and practical solution for relay evaluation be present . two mainissue be take into account : power system simulation and relay simulation , both of which consist of different stage . system simulation be carry out by mean of emtp and be complement by additional feature , such as filter for location and determination of fault parameter that allow compare simulated and actual fault record to improve and guarantee a correct system simulation . relay simulation include filter algorithm , all the relay unit , and the decision logic . play simulated or real fault over the actual relay and compare simulated and real response can check for correct relay simulation",104,0.666666667
8763,Computers and IT,"Positive systems possessing first integrals are considered. These systems frequently occur in applications. The paper is devoted to two stabilization problems. The first is concerned with the design of feedbacks to stabilize a given level set. Secondly, it is shown that the same feedback allows us to globally stabilize an equilibrium point if it is asymptotically stable with respect to initial conditions in its level set. Two examples are provided and the results are compared with those in the literature","positive system possess first integral be consider . these systemsfrequently occur in application . the paper be devote to two stabilization problem . the first be concern with the design of feedback to stabilize a give level set . secondly , it be show that the same feedback allow us to globally stabilize an equilibrium point if it be asymptotically stable with respect to initial condition in its level set . two example be provide and the result be compare with those in the literature",79,0.428571429
8764,Computers and IT,"Describes a systematic procedure for designing speed and rotor flux norm tracking H/sub infinity /. controllers with unknown load torque disturbances for current-fed induction motors. A new effective design tool is developed to allow selection of the control gains so as to adjust the disturbances' rejection capability of the controllers in the face of the bandwidth requirements of the closed-loop system. Application of the proposed design procedure is demonstrated in a case study, and the results of numerical simulations illustrate the satisfactory performance achievable even in presence of rotor resistance uncertainty","describe a systematic procedure for design speed and rotor flux norm track h/sub infinity / . controller with unknown load torque disturbance for current-fed induction motor . a new effective design tool be develop to allow selection of the control gain so as to adjust the disturbance ' rejection capability of the controller in the face of the bandwidth requirement of the closed-loop system . application of the propose design procedure be demonstrate in a case study , and the result of numerical simulation illustrate the satisfactory performance achievable even in presence of rotor resistance uncertainty",91,0.454545455
8765,Computers and IT,"Analysis of exclusively kinetic two-link underactuated mechanical systems is undertaken. It is first shown that such systems are not full-state feedback linearizable around any equilibrium point. Also, the equilibrium points for which the system is small-time locally controllable (STLC) is at most a one-dimensional submanifold. A concept less restrictive than STLC, termed the small-time local output controllability (STLOC) is introduced, the satisfaction of which guarantees that a chosen configuration output can be controlled at its desired value. It is shown that the class of systems considered is STLOC, if the inertial coupling between the input and output is nonzero. Also, in such a case, the system is nonminimum phase. An example section illustrates all the results presented","analysis of exclusively kinetic two-link underactuated mechanical systems isundertaken. it is first shown that such systems are not full-state feedback linearizable around any equilibrium point. also, the equilibrium points for which the system is small-time locally controllable (stlc) is at most a one-dimensional submanifold. a concept less restrictive than stlc, termed the small-time local output controllability (stloc) is introduced, the satisfaction of which guarantees that a chosen configuration output can be controlled at its desired value. it is shown that the class of systems considered is stloc, if the inertial coupling between the input and output is nonzero. also, in such a case, the system is nonminimum phase. an example section illustrates all the results presented",116,0.833333333
8766,Computers and IT,Presents theory for stability analysis and design for a class of observer-based feedback control systems. Relaxation of the controllability and observability conditions imposed in the Yakubovich-Kalman-Popov lemma can be made for a class of nonlinear systems described by a linear time-invariant system with a feedback-connected cone-bounded nonlinear element. It is shown how a circle-criterion approach can be used to design an observer-based state feedback control which yields a closed-loop system with specified robustness characteristics. The approach is relevant for design with preservation of stability when a cone-bounded nonlinearity is introduced in the feedback loop. Important applications are to be found in nonlinear control with high robustness requirements,present theory for stability analysis and design for a class of observer-basedfeedback control system . relaxation of the controllability and observability condition impose in the yakubovich-kalman-popov lemma can be make for a class of nonlinear system describe by a linear time-invariant system with a feedback-connected cone-bounded nonlinear element . it be show how a circle-criterion approach can be use to design an observer-based state feedback control which yield a closed-loop system with specify robustness characteristic . the approach be relevant for design with preservation of stability when a cone-bounded nonlinearity be introduce in the feedback loop . important application be to be find in nonlinear control with high robustness requirement,106,0.833333333
8767,Computers and IT,"The theory of non-commutative rings is introduced to provide a basis for the study of nonlinear control systems with time delays. The left Ore ring of non-commutative polynomials defined over the field of a meromorphic function is suggested as the framework for such a study. This approach is then generalized to a broader class of nonlinear systems with delays that are called generalized Roesser systems. Finally, the theory is applied to analyze nonlinear time-delay systems. A weak observability is defined and characterized, generalizing the well-known linear result. Properties of closed submodules are then developed to obtain a result on the accessibility of such systems","the theory of non-commutative ring be introduce to provide a basis for the study of nonlinear control system with time delay . the left ore ring of non-commutative polynomial define over the field of a meromorphic function be suggest as the framework for such a study . this approach be then generalize to a broad class of nonlinear system with delay that be call generalized roesser system . finally , the theory be apply to analyze nonlinear time-delay system . a weak observability be define and characterize , generalize the well-known linear result . property of closed submodule be then develop to obtain a result on the accessibility of such system",104,0.666666667
8768,Computers and IT,"We consider the finite sample properties of least-squares system identification, and derive non-asymptotic confidence ellipsoids for the estimate. The shape of the confidence ellipsoids is similar to the shape of the ellipsoids derived using asymptotic theory, but unlike asymptotic theory, they are valid for a finite number of data points. The probability that the estimate belongs to a certain ellipsoid has a natural dependence on the volume of the ellipsoid, the data generating mechanism, the model order and the number of data points available","we consider the finite sample property of least-square systemidentification , and derive non-asymptotic confidence ellipsoid for the estimate . the shape of the confidence ellipsoid be similar to the shape of the ellipsoid derive use asymptotic theory , but unlike asymptotic theory , they be valid for a finite number of data point . the probability that the estimate belong to a certain ellipsoid have a natural dependence on the volume of the ellipsoid , the data generate mechanism , the model order and the number of data point available",83,0.625
8769,Computers and IT,"The notion of doubly invariant (DI) equilibrium is introduced. The concept extends controlled and robustly controlled invariance notions to the context of two-person dynamic games. Each player tries to keep the state in a region of state space independently of the actions of the rival player. The paper gives existence conditions, criteria and algorithms for the determination of DI equilibria of linear dynamic games in discrete time. Two examples illustrate the results. The first one is in the area of fault-tolerant controller synthesis. The second is an application to macroeconomics","the notion of doubly invariant (di) equilibrium is introduced. the conceptextends controlled and robustly controlled invariance notions to the context of two-person dynamic games. each player tries to keep the state in a region of state space independently of the actions of the rival player. the paper gives existence conditions, criteria and algorithms for the determination of di equilibria of linear dynamic games in discrete time. two examples illustrate the results. the first one is in the area of fault-tolerant controller synthesis. the second is an application to macroeconomics",89,0.625
8770,Computers and IT,"We examine certain variance properties of model reduction. The focus is on L/sub 2/ model reduction, but some general results are also presented. These general results can be used to analyze various other model reduction schemes. The models we study are finite impulse response (FIR) and output error (OE) models. We compare the variance of two estimated models. The first one is estimated directly from data and the other one is computed by reducing a high order model, by L/sub 2/ model reduction. In the FIR case we show that it is never better to estimate the model directly from data, compared to estimating it via L/sub 2/ model reduction of a high order FIR model. For OE models we show that the reduced model has the same variance as the directly estimated one if the reduced model class used contains the true system","we examine certain variance properties of model reduction. the focus is onl/sub 2/ model reduction, but some general results are also presented. these general results can be used to analyze various other model reduction schemes. the models we study are finite impulse response (fir) and output error (oe) models. we compare the variance of two estimated models. the first one is estimated directly from data and the other one is computed by reducing a high order model, by l/sub 2/ model reduction. in the fir case we show that it is never better to estimate the model directly from data, compared to estimating it via l/sub 2/ model reduction of a high order fir model. for oe models we show that the reduced model has the same variance as the directly estimated one if the reduced model class used contains the true system",143,0.166666667
8771,Computers and IT,"A continuous feedback control approach for real-time scheduling of discrete events is presented motivated by the need for control theoretic techniques to analyze and design such systems in distributed manufacturing applications. These continuous feedback control systems exhibit highly nonlinear and discontinuous dynamics. Specifically, when the production demand in the manufacturing system exceeds the available resource capacity then the control system ""chatters"" and exhibits sliding modes. This sliding mode behavior is advantageously used in the scheduling application by allowing the system to visit different schedules within an infinitesimal region near the sliding surface. In the paper, an analytical model is developed to characterize the sliding mode dynamics. This model is then used to design controllers in the sliding mode domain to improve the effectiveness of the control system to ""search"" for schedules with good performance. Computational results indicate that the continuous feedback control approach can provide near-optimal schedules and that it is computationally efficient compared to existing scheduling techniques","a continuous feedback control approach for real-time scheduling of discrete event be present motivate by the need for control theoretic technique to analyze and design such system in distribute manufacturing application . these continuous feedback control system exhibit highly nonlinear and discontinuous dynamic . specifically , when the production demand in the manufacturing system exceed the available resource capacity then the control system `` chatter '' and exhibit slide mode . this slide mode behavior be advantageously use in the scheduling application by allow the system to visit different schedule within an infinitesimal region near the slide surface . in the paper , an analytical model be develop to characterize the slide mode dynamic . this model be then use to design controller in the slide mode domain to improve the effectiveness of the control system to `` search '' for schedule with good performance . computational result indicate that the continuous feedback control approach can provide near-optimal schedule and that it be computationally efficient compare to exist scheduling technique",158,0.777777778
8772,Computers and IT,A continuous-time gain-scheduled controller must be transformed to a corresponding discrete-time controller for sampled-data implementation. We show that certain linearization properties of a continuous-time gain scheduled controller are inherited by its sampled-data implementation. We also show that a similar relationship exists for multi-rate gain scheduled controllers arising in flight control applications,a continuous-time gain-scheduled controller must be transform to acorrespond discrete-time controller for sampled-data implementation . we show that certain linearization property of a continuous-time gain schedule controller be inherit by its sampled-data implementation . we also show that a similar relationship exist for multi-rate gain schedule controller arise in flight control application,50,1
8773,Computers and IT,"The problem of estimating perturbation bounds of finite trajectories is considered. The trajectory is assumed to be generated by a linear system with uncertainty characterized in terms of integral quadratic constraints. It is shown that such perturbation bounds can be obtained as the solution to a nonconvex quadratic optimization problem, which can be addressed using Lagrange relaxation. The result can be used in robustness analysis of hybrid systems and switched dynamical systems","the problem of estimate perturbation bound of finite trajectory isconsider . the trajectory be assume to be generate by a linear system with uncertainty characterize in term of integral quadratic constraint . it be show that such perturbation bound can be obtain as the solution to a nonconvex quadratic optimization problem , which can be address use lagrange relaxation . the result can be use in robustness analysis of hybrid system and switch dynamical system",71,0.818181818
8774,Computers and IT,"Studies the trajectory and force tracking control problem of mobile manipulators subject to holonomic and nonholonomic constraints with unknown inertia parameters. Adaptive controllers are proposed based on a suitable reduced dynamic model, the defined reference signals and the mixed tracking errors. The proposed controllers not only ensure the entire state of the system to asymptotically converge to the desired trajectory but also ensure the constraint force to asymptotically converge to the desired force. A detailed numerical example is presented to illustrate the developed methods","study the trajectory and force tracking control problem of mobile manipulator subject to holonomic and nonholonomic constraint with unknown inertia parameter . adaptive controller be propose base on a suitable reduced dynamic model , the define reference signal and the mixed tracking error . the propose controller not only ensure the entire state of the system to asymptotically converge to the desire trajectory but also ensure the constraint force to asymptotically converge to the desire force . a detailed numerical example be present to illustrate the develop method",84,0.416666667
8775,Computers and IT,"The essential issues of time complexity and probing signal selection are studied for persistent identification of linear time-invariant systems in a closed-loop setting. By establishing both upper and lower bounds on identification accuracy as functions of the length of observation, size of unmodeled dynamics, and stochastic disturbances, we demonstrate the inherent impact of unmodeled dynamics on identification accuracy, reduction of time complexity by stochastic averaging on disturbances, and probing capability of full rank periodic signals for closed-loop persistent identification. These findings indicate that the mixed formulation, in which deterministic uncertainty of system dynamics is blended with random disturbances, is beneficial to reduction of identification complexity","the essential issue of time complexity and probe signal selection be study for persistent identification of linear time-invariant system in a closed-loop setting . by establish both upper and low bound on identification accuracy as function of the length of observation , size of unmodeled dynamic , and stochastic disturbance , we demonstrate the inherent impact of unmodeled dynamic on identification accuracy , reduction of time complexity by stochastic averaging on disturbance , and probe capability of full rank periodic signal for closed-loop persistent identification . these finding indicate that the mixed formulation , in which deterministic uncertainty of system dynamic be blended with random disturbance , be beneficial to reduction of identification complexity",105,0.9
8776,Computers and IT,"Deals with probabilistic model set validation. It is assumed that the dynamics of a multi-input multi-output (MIMO) plant is described by a model set with unstructured uncertainties, and identification experiments are performed in closed loop. A necessary and sufficient condition has been derived for the consistency of the model set with both the stabilizing controller and closed-loop frequency domain experimental data (FDED). In this condition, only the Euclidean norm of a complex vector is involved, and this complex vector depends linearly on both the disturbances and the measurement errors. Based on this condition, an analytic formula has been derived for the sample unfalsified probability (SUP) of the model set. Some of the asymptotic statistical properties of the SUP have also been briefly discussed. A numerical example is included to illustrate the efficiency of the suggested method in model set quality evaluation","deals with probabilistic model set validation. it is assumed that the dynamicsof a multi-input multi-output (mimo) plant is described by a model set with unstructured uncertainties, and identification experiments are performed in closed loop. a necessary and sufficient condition has been derived for the consistency of the model set with both the stabilizing controller and closed-loop frequency domain experimental data (fded). in this condition, only the euclidean norm of a complex vector is involved, and this complex vector depends linearly on both the disturbances and the measurement errors. based on this condition, an analytic formula has been derived for the sample unfalsified probability (sup) of the model set. some of the asymptotic statistical properties of the sup have also been briefly discussed. a numerical example is included to illustrate the efficiency of the suggested method in model set quality evaluation",140,0.357142857
8777,Computers and IT,We propose a systematic switching control design method for a class of nonlinear discrete time hybrid systems. The novelty of the adopted approach is in the fact that unlike conventional control the control burden is shifted to a logical level thus creating the need for the development of new analysis/design methods,we propose a systematic switching control design method for a class ofnonlinear discrete time hybrid system . the novelty of the adopt approach be in the fact that unlike conventional control the control burden be shift to a logical level thus create the need for the development of new analysis/design method,50,0.25
8778,Computers and IT,"An algorithm combining neural networks with the fundamental parameters equations (NNFP) is proposed for making corrections for non-linear matrix effects in x-ray fluorescence analysis. In the algorithm, neural networks were applied to relate the concentrations of components to both the measured intensities and the relative theoretical intensities calculated by the fundamental parameter equations. The NNFP algorithm is compared with the classical theoretical correction models, including the fundamental parameters approach, the Lachance-Traill model, a hyperbolic function model and the COLA algorithm. For an alloy system with 15 measured elements, in most cases, the prediction errors of the NNFP algorithm are lower than those of the fundamental parameters approach, the Lachance-Traill model, the hyperbolic function model and the COLA algorithm separately. If there are the serious matrix effects, such as matrix effects among Cr, Fe and Ni, the NNFP algorithm generally decreased predictive errors as compared with the classical models, except for the case of Cr by the fundamental parameters approach. The main reason why the NNFP algorithm has generally a better predictive ability than the classical theoretical correction models might be that neural networks can better calibrate the non-linear matrix effects in a complex multivariate system","an algorithm combining neural networks with the fundamental parametersequations (nnfp) is proposed for making corrections for non-linear matrix effects in x-ray fluorescence analysis. in the algorithm, neural networks were applied to relate the concentrations of components to both the measured intensities and the relative theoretical intensities calculated by the fundamental parameter equations. the nnfp algorithm is compared with the classical theoretical correction models, including the fundamental parameters approach, the lachance-traill model, a hyperbolic function model and the cola algorithm. for an alloy system with 15 measured elements, in most cases, the prediction errors of the nnfp algorithm are lower than those of the fundamental parameters approach, the lachance-traill model, the hyperbolic function model and the cola algorithm separately. if there are the serious matrix effects, such as matrix effects among cr, fe and ni, the nnfp algorithm generally decreased predictive errors as compared with the classical models, except for the case of cr by the fundamental parameters approach. the main reason why the nnfp algorithm has generally a better predictive ability than the classical theoretical correction models might be that neural networks can better calibrate the non-linear matrix effects in a complex multivariate system",194,0.882352941
8779,Computers and IT,"We investigate the dynamics of a simple model of a wheelset that supports one end of a railway freight wagon by springs with linear characteristics and dry friction dampers. The wagon runs on an ideal, straight and level track with constant speed. The lateral dynamics in dependence on the speed is examined. We have included stick-slip and hysteresis in our model of the dry friction and assume that Coulomb's law holds during the slip phase. It is found that the action of dry friction completely changes the bifurcation diagram, and that the longitudinal component of the dry friction damping forces destabilizes the wagon","we investigate the dynamic of a simple model of a wheelset that support oneend of a railway freight wagon by spring with linear characteristic and dry friction damper . the wagon run on an ideal , straight and level track with constant speed . the lateral dynamic in dependence on the speed be examine . we have include stick-slip and hysteresis in our model of the dry friction and assume that coulomb 's law hold during the slip phase . it be find that the action of dry friction completely change the bifurcation diagram , and that the longitudinal component of the dry friction damp force destabilize the wagon",102,0.8
8780,Computers and IT,"When modelling vehicles for the vehicle dynamic simulation, special attention must be paid to the modelling of tyre forces and -torques, according to their dominant influence on the results. This task is not only about sufficiently exact representation of the effective forces but also about user-friendly and practical relevant applicability, especially when the experimental tyre-input-data is incomplete or missing. This text firstly describes the basics of the vehicle dynamic tyre model, conceived to be a physically based, semi-empirical model for application in connection with multi-body-systems (MBS). On the basis of tyres for a passenger car and a heavy truck the simulated steady state tyre characteristics are shown together and compared with the underlying experimental values. The possibility to link the tyre model TMeasy to any MBS-program is described, as far as it supports the 'Standard Tyre Interface'. As an example, the simulated and experimental data of a heavy truck doing a standardized driving manoeuvre are compared","when modelling vehicles for the vehicle dynamic simulation, special attention must be paid to the modelling of tyre forces and -torques, according to their dominant influence on the results. this task is not only about sufficiently exact representation of the effective forces but also about user-friendly and practical relevant applicability, especially when the experimental tyre-input-data is incomplete or missing. this text firstly describes the basics of the vehicle dynamic tyre model, conceived to be a physically based, semi-empirical model for application in connection with multi-body-systems (mbs). on the basis of tyres for a passenger car and a heavy truck the simulated steady state tyre characteristics are shown together and compared with the underlying experimental values. the possibility to link the tyre model tmeasy to any mbs-program is described, as far as it supports the 'standard tyre interface'. as an example, the simulated and experimental data of a heavy truck doing a standardized driving manoeuvre are compared",156,0.615384615
8781,Computers and IT,"A stroke dependent damper is designed for the front axle suspension of a truck. The damper supplies extra damping for inward deflections rising above 4 cm. In this way the damper should reduce extreme suspension deflections without deteriorating the comfort of the truck. But the question is which stroke dependent damping curve yields the best compromise between suspension deflection working space and comfort. Therefore an optimization problem is defined to minimize the maximum inward suspension deflection subject to constraints on the chassis acceleration for three typical road undulations. The optimization problem is solved using sequential linear programming (SLP) and multibody dynamics simulation software. Several optimization runs have been carried out for a small two degree of freedom vehicle model and a large full-scale model of the truck semi-trailer combination. The results show that the stroke dependent damping can reduce large deflections at incidental road disturbances, but that the optimum stroke dependent damping curve is related to the acceleration bound. By means of vehicle model simulation and numerical optimization we have been able to quantify this trade-off between suspension deflection working space and truck comfort","a stroke dependent damper is designed for the front axle suspension of a truck. the damper supplies extra damping for inward deflections rising above 4 cm. in this way the damper should reduce extreme suspension deflections without deteriorating the comfort of the truck. but the question is which stroke dependent damping curve yields the best compromise between suspension deflection working space and comfort. therefore an optimization problem is defined to minimize the maximum inward suspension deflection subject to constraints on the chassis acceleration for three typical road undulations. the optimization problem is solved using sequential linear programming (slp) and multibody dynamics simulation software. several optimization runs have been carried out for a small two degree of freedom vehicle model and a large full-scale model of the truck semi-trailer combination. the results show that the stroke dependent damping can reduce large deflections at incidental road disturbances, but that the optimum stroke dependent damping curve is related to the acceleration bound. by means of vehicle model simulation and numerical optimization we have been able to quantify this trade-off between suspension deflection working space and truck comfort",184,0.875
8782,Computers and IT,"This paper presents a neural network based face detection system. Our objective is to design a system that can detect human faces in visual scenes at high searching speed and accuracy. We used a neural network with a simple structure but trained using face and non-face samples preprocessed by several methods (position normalization, histogram equalization, etc. ) to attain high accuracy, then pruned the size of the neural network so that it could run faster and reduced the total search area of a target visual scene using the skin color detector. Skin color detection assumes that faces reside only in skin color regions. The system design is made up of two parts: the face detecting system that detects the faces, and the searching speed improving system. Speed improvement is achieved by reducing the face locator network size using the structural learning with knowledge and by reducing the face search area using the skin color detection system. Faster training of the neural networks was also achieved using variable step sizes","this paper presents a neural network based face detection system. our objectiveis to design a system that can detect human faces in visual scenes at high searching speed and accuracy. we used a neural network with a simple structure but trained using face and non-face samples preprocessed by several methods (position normalization, histogram equalization, etc.) to attain high accuracy, then pruned the size of the neural network so that it could run faster and reduced the total search area of a target visual scene using the skin color detector. skin color detection assumes that faces reside only in skin color regions. the system design is made up of two parts: the face detecting system that detects the faces, and the searching speed improving system. speed improvement is achieved by reducing the face locator network size using the structural learning with knowledge and by reducing the face search area using the skin color detection system. faster training of the neural networks was also achieved using variable step sizes",167,0.333333333
8783,Computers and IT,"We consider identification of a certain class of discrete-time nonlinear systems known as linear parameter varying system. We assume that inputs, outputs and the scheduling parameters are directly measured, and a form of the functional dependence of the system coefficients on the parameters is known. We show how this identification problem can be reduced to a linear regression, and provide compact formulae for the corresponding least mean square and recursive least-squares algorithms. We derive conditions on persistency of excitation in terms of the inputs and scheduling parameter trajectories when the functional dependence is of polynomial type. These conditions have a natural polynomial interpolation interpretation, and do not require the scheduling parameter trajectories to vary slowly. This method is illustrated with a simulation example using two different parameter trajectories","we consider identification of a certain class of discrete-time nonlinearsystem know as linear parameter vary system . we assume that input , output and the scheduling parameter be directly measure , and a form of the functional dependence of the system coefficient on the parameter be know . we show how this identification problem can be reduce to a linear regression , and provide compact formula for the corresponding least mean square and recursive least-square algorithm . we derive condition on persistency of excitation in term of the input and scheduling parameter trajectory when the functional dependence be of polynomial type . these condition have a natural polynomial interpolation interpretation , and do not require the scheduling parameter trajectory to vary slowly . this method be illustrate with a simulation example use two different parameter trajectory",127,0.642857143
8784,Computers and IT,"It has been found that discontinuity plays a crucial role in natural evolutions (Lin 1998). In this presentation, we will generalize the idea of integration and differentiation, we developed in calculus, to the study of time series in the hope that the problem of outliers and discontinuities can be resolved more successfully than simply deleting the outliers and avoiding discontinuities from the overall data analysis. In general, appearances of outliers tend to mean existence of discontinuities, explosive growth or decline in the evolution. At the same time, our approach can be employed to partially overcome the problem of not having enough data values in any available time series. At the end, we will look at some real-life problems of prediction in order to see the power of this new approach","it has been found that discontinuity plays a crucial role in natural evolutions(lin 1998). in this presentation, we will generalize the idea of integration and differentiation, we developed in calculus, to the study of time series in the hope that the problem of outliers and discontinuities can be resolved more successfully than simply deleting the outliers and avoiding discontinuities from the overall data analysis. in general, appearances of outliers tend to mean existence of discontinuities, explosive growth or decline in the evolution. at the same time, our approach can be employed to partially overcome the problem of not having enough data values in any available time series. at the end, we will look at some real-life problems of prediction in order to see the power of this new approach",129,1
8785,Computers and IT,"Deals with the problem of robust L/sub 2/ disturbance attenuation for nonlinear systems with input dynamical uncertainty. The input dynamical uncertainty is restricted to be minimum-phase and relative degree zero. A sufficient condition. is given such that the nonlinear system satisfies the L/sub 2/ gain performance and input-to-state stable property. Using this condition, a design approach is given for a smooth state feedback control law that solves the robust L/sub 2/ disturbance attenuation problem, and the approach is extended to a more general case where the nominal system has higher relative degree. Finally, a numerical example is given to demonstrate the proposed approach","deal with the problem of robust l/sub 2 / disturbance attenuation for nonlinear system with input dynamical uncertainty . the input dynamical uncertainty be restrict to be minimum-phase and relative degree zero . a sufficient condition . be give such that the nonlinear system satisfy the l/sub 2 / gain performance and input-to-state stable property . use this condition , a design approach be give for a smooth state feedback control law that solve the robust l/sub 2 / disturbance attenuation problem , and the approach be extend to a more general case where the nominal system have high relative degree . finally , a numerical example be give to demonstrate the proposed approach",103,0.9
8786,Computers and IT,"FC++ is a library for programming functionally in C++. Compared to other C++ functional programming libraries, FC++ is distinguished by its powerful type system which allows the manipulation of parametrically polymorphic functions (e. g. , passing them as arguments to other functions and returning them as results). In this paper, we show how FC++ can be used in common object-oriented programming tasks. We demonstrate FC++ implementations of several common design patterns (Adapter, Builder, Command, and more). Compared to conventional C++ implementations of these patterns, our implementations are either simpler (in that fewer classes/dependencies are needed), more efficient, or more type-safe (thanks to parametric polymorphism and type inference)","fc++ is a library for programming functionally in c++. compared to other c++functional programming libraries, fc++ is distinguished by its powerful type system which allows the manipulation of parametrically polymorphic functions (e.g., passing them as arguments to other functions and returning them as results). in this paper, we show how fc++ can be used in common object-oriented programming tasks. we demonstrate fc++ implementations of several common design patterns (adapter, builder, command, and more). compared to conventional c++ implementations of these patterns, our implementations are either simpler (in that fewer classes/dependencies are needed), more efficient, or more type-safe (thanks to parametric polymorphism and type inference)",104,0.666666667
8787,Computers and IT,"Many software applications require co-operative work support, including collaborative editing, group awareness, versioning, messaging and automated notification and co-ordination agents. Most approaches hard-code such facilities into applications, with fixed functionality and limited ability to reuse groupware implementations. We describe our recent work in seamlessly adding such capabilities to component-based applications via a set of collaborative work-supporting plug-in software components. We describe a variety of applications of this technique, along with descriptions of the novel architecture, user interface adaptation and implementation techniques for the collaborative work-supporting components that we have developed. We report on our experiences to date with this method of supporting collaborative work enhancement of component-based systems, and discuss the advantages of our approach over conventional techniques","many software application require co-operative work support , includingcollaborative editing , group awareness , version , message and automate notification and co-ordination agent . most approach hard-code such facility into application , with fix functionality and limit ability to reuse groupware implementation . we describe our recent work in seamlessly add such capability to component-based application via a set of collaborative work-supporting plug-in software component . we describe a variety of application of this technique , along with description of the novel architecture , user interface adaptation and implementation technique for the collaborative work-supporting component that we have develop . we report on our experience to date with this method of support collaborative work enhancement of component-based system , and discuss the advantage of our approach over conventional technique",117,0.9
8788,Computers and IT,"The BLISS programming language was invented by William A. Wulf and others at Carnegie-Mellon University in 1969, originally for the DEC PDP-10. BLISS-10 caught the interest of Ronald F. Brender of DEC (Digital Equipment Corporation). After several years of collaboration, including the creation of BLISS-11 for the PDP-11, BLISS was adopted as DEC's implementation language for use on its new line of VAX computers in 1975. DEC developed a completely new generation of BLISSs for the VAX, PDP-10 and PDP-11, which became widely used at DEC during the 1970s and 1980s. With the creation of the Alpha architecture in the early 1990s, BLISS was extended again, in both 32- and 64-bit flavors. BLISS support for the Intel IA-32 architecture was introduced in 1995 and IA-64 support is now in progress. BLISS has a number of unusual characteristics: it is typeless, requires use of an explicit contents of operator (written as a period or 'dot'), takes an algorithmic approach to data structure definition, has no goto, is an expression language, and has an unusually rich compile-time language. This paper reviews the evolution and use of BLISS over its three decade lifetime. Emphasis is on how the language evolved to facilitate portable programming while retaining its initial highly machine-specific character. Finally, the success of its characteristics are assessed","the bliss programming language was invented by william a. wulf and others atcarnegie-mellon university in 1969, originally for the dec pdp-10. bliss-10 caught the interest of ronald f. brender of dec (digital equipment corporation). after several years of collaboration, including the creation of bliss-11 for the pdp-11, bliss was adopted as dec's implementation language for use on its new line of vax computers in 1975. dec developed a completely new generation of blisss for the vax, pdp-10 and pdp-11, which became widely used at dec during the 1970s and 1980s. with the creation of the alpha architecture in the early 1990s, bliss was extended again, in both 32- and 64-bit flavors. bliss support for the intel ia-32 architecture was introduced in 1995 and ia-64 support is now in progress. bliss has a number of unusual characteristics: it is typeless, requires use of an explicit contents of operator (written as a period or 'dot'), takes an algorithmic approach to data structure definition, has no goto, is an expression language, and has an unusually rich compile-time language. this paper reviews the evolution and use of bliss over its three decade lifetime. emphasis is on how the language evolved to facilitate portable programming while retaining its initial highly machine-specific character. finally, the success of its characteristics are assessed",215,0.666666667
8789,Computers and IT,"PLT Scheme (DrScheme and MzScheme) supports the Component Object Model (COM) standard with two pieces of software. The first piece is MzCOM, a COM class that makes a Scheme evaluator available to COM clients. With MzCOM, programmers can embed Scheme code in programs written in mainstream languages such as C++ or Visual BASIC. Some applications can also be used as MzCOM clients. The other piece of component-support software is MysterX, which makes COM classes available to PLT Scheme programs. When needed, MysterX uses a programmable Web browser to display COM objects. We describe the technical issues encountered in building these two systems and sketch some applications","plt scheme (drscheme and mzscheme) supports the component object model (com)standard with two pieces of software. the first piece is mzcom, a com class that makes a scheme evaluator available to com clients. with mzcom, programmers can embed scheme code in programs written in mainstream languages such as c++ or visual basic. some applications can also be used as mzcom clients. the other piece of component-support software is mysterx, which makes com classes available to plt scheme programs. when needed, mysterx uses a programmable web browser to display com objects. we describe the technical issues encountered in building these two systems and sketch some applications",105,0.8
8790,Computers and IT,A new approach to MFC calibration links the physical parameters of nitrogen to the physical characteristics of various process gases. This precludes the conventional need for surrogate gases. What results is a physics-based tuning algorithm and enhanced digital control system that enables rearranging and gas change of digital MFCs. The end result should be better process control through more accurate gas flow. The new method also decreases the number of MFC spare parts required to back up a fab,a new approach to mfc calibration link the physical parameter of nitrogen tothe physical characteristic of various process gas . this preclude the conventional need for surrogate gas . what result be a physics-based tuning algorithm and enhanced digital control system that enable rearranging and gas change of digital mfc . the end result should be better process control through more accurate gas flow . the new method also decrease the number of mfc spare part require to back up a fab,78,0.5
8791,Computers and IT,"The migration to tighter geometries and more complex process sequence integration schemes requires having the ability to compensate for upstream deviations from target specifications. Doing so ensures that-downstream process sequences operate on work-in-progress that is well within control. Because point-of-use visibility of work-in-progress quality has become of paramount concern in the industry's drive to reduce scrap and improve yield, controlling trench depth has assumed greater importance. An integrated, interferometric based, rate monitor for etch-to-depth and spacer etch applications has been developed for controlling this parameter. This article demonstrates that the integrated rate monitor, using polarization and digital signal processing, enhances control etch-to-depth processes and can also be implemented as a predictive endpoint in a wafer manufacturing environment for dual damascene trench etch and spacer etch applications","the migration to tight geometry and more complex process sequenceintegration scheme require have the ability to compensate for upstream deviation from target specification . do so ensure that-downstream process sequence operate on work-in-progres that be well within control . because point-of-use visibility of work-in-progress quality have become of paramount concern in the industry 's drive to reduce scrap and improve yield , control trench depth have assume great importance . an integrate , interferometric base , rate monitor for etch-to-depth and spacer etch application have be develop for control this parameter . this article demonstrate that the integrated rate monitor , use polarization and digital signal processing , enhance control etch-to-depth process and can also be implement as a predictive endpoint in a wafer manufacturing environment for dual damascene trench etch and spacer etch application",125,0.45
8792,Computers and IT,"Multiple comparison methods (MCMs) are used to investigate differences between pairs of population means or, more generally, between subsets of population means using sample data. Although several such methods are commonly available in statistical software packages, users may be poorly informed about the appropriate method(s) to use and/or the correct way to interpret the results. This paper classifies the MCMs and presents the important methods for each class. Both simulated and real data are used to compare the methods, and emphasis is placed on a correct application and interpretation. We include suggestions for choosing the best method. Mathematica programs developed by the authors are used to compare MCMs. By taking the advantage of Mathematica's notebook structure, all interested student can use these programs to explore the subject more deeply","multiple comparison methods (mcms) are used to investigate differences betweenpairs of population means or, more generally, between subsets of population means using sample data. although several such methods are commonly available in statistical software packages, users may be poorly informed about the appropriate method(s) to use and/or the correct way to interpret the results. this paper classifies the mcms and presents the important methods for each class. both simulated and real data are used to compare the methods, and emphasis is placed on a correct application and interpretation. we include suggestions for choosing the best method. mathematica programs developed by the authors are used to compare mcms. by taking the advantage of mathematica's notebook structure, all interested student can use these programs to explore the subject more deeply",128,0.142857143
8793,Computers and IT,A simple method to formulate an explicit expression for the roots of any analytic transcendental function is presented. The method is based on Cauchy's integral theorem and uses only basic concepts of complex integration. A convenient method for numerically evaluating the exact expression is presented. The application of both the formulation and evaluation of the exact expression is illustrated for several classical root finding problems,a simple method to formulate an explicit expression for the root of anyanalytic transcendental function be present . the method be base on cauchy 's integral theorem and use only basic concept of complex integration . a convenient method for numerically evaluate the exact expression be present . the application of both the formulation and evaluation of the exact expression be illustrate for several classical root finding problem,64,0.25
8794,Computers and IT,"The authors evaluated magnetic resonance (MR) imaging with high spectral and spatial resolutions (HSSR) of water and fat in breasts of healthy volunteers (n=6) and women with suspicious lesions (n=6). Fat suppression, edge delineation, and image texture were improved on MR images derived from HSSR data compared with those on conventional MR images. HSSR MR imaging data acquired before and after contrast medium injection showed spectrally inhomogeneous changes in the water resonances in small voxels that were not detectable with conventional MR imaging","the authors evaluated magnetic resonance (mr) imaging with high spectral and spatial resolutions (hssr) of water and fat in breasts of healthy volunteers (n=6) and women with suspicious lesions (n=6). fat suppression, edge delineation, and image texture were improved on mr images derived from hssr data compared with those on conventional mr images. hssr mr imaging data acquired before and after contrast medium injection showed spectrally inhomogeneous changes in the water resonances in small voxels that were not detectable with conventional mr imaging",83,0.615384615
8795,Computers and IT,A new methodology is proposed for the design of trajectory tracking controllers for autonomous vehicles. The design technique builds on gain scheduling control theory. An application is made to the design of a trajectory tracking controller for a prototype autonomous underwater vehicle (AUV). The effectiveness and advantages of the new control laws derived are illustrated in simulation using a full set of non-linear equations of motion of the vehicle,a new methodology is proposed for the design of trajectory tracking controllers for autonomous vehicles. the design technique builds on gain scheduling control theory. an application is made to the design of a trajectory tracking controller for a prototype autonomous underwater vehicle (auv). the effectiveness and advantages of the new control laws derived are illustrated in simulation using a full set of non-linear equations of motion of the vehicle,69,0.666666667
8796,Computers and IT,"Evaluates the effectiveness of a computerized classification method as an aid to radiologists reviewing clinical mammograms for which the diagnoses were unknown to both the radiologists and the computer. Six mammographers and six community radiologists participated in an observer study. These 12 radiologists interpreted, with and without the computer aid, 110 cases that were unknown to both the 12 radiologist observers and the trained computer classification scheme. The radiologists' performances in differentiating between benign and malignant masses without and with the computer aid were evaluated with receiver operating characteristic (ROC) analysis. Two-tailed P values were calculated for the Student t test to indicate the statistical significance of the differences in performances with and without the computer aid. When the computer aid was used, the average performance of the 12 radiologists improved, as indicated by an increase in the area under the ROC curve (A/sub z/) from 0. 93 to 0. 96 (P. 001), by an increase in partial area under the ROC curve (/sub 0. 9/0A'/sub z/) from 0. 56 to 0. 72 (P. 001), and by an increase in sensitivity from 94% to 98% (P=. 022). No statistically significant difference in specificity was found between readings with and those without computer aid ( Delta +-0. 014; P=. 46; 95% Cl: -0. 054, 0. 026), where Delta is difference in specificity. When we analyzed results from the mammographers and community radiologists as separate groups, a larger improvement was demonstrated for the community radiologists. Computer-aided diagnosis can potentially help radiologists improve their diagnostic accuracy in the task of differentiating between benign and malignant masses seen on mammograms","evaluates the effectiveness of a computerized classification method as an aid to radiologists reviewing clinical mammograms for which the diagnoses were unknown to both the radiologists and the computer. six mammographers and six community radiologists participated in an observer study. these 12 radiologists interpreted, with and without the computer aid, 110 cases that were unknown to both the 12 radiologist observers and the trained computer classification scheme. the radiologists' performances in differentiating between benign and malignant masses without and with the computer aid were evaluated with receiver operating characteristic (roc) analysis. two-tailed p values were calculated for the student t test to indicate the statistical significance of the differences in performances with and without the computer aid. when the computer aid was used, the average performance of the 12 radiologists improved, as indicated by an increase in the area under the roc curve (a/sub z/) from 0.93 to 0.96 (p<.001), by an increase in partial area under the roc curve (/sub 0.9/0a'/sub z/) from 0.56 to 0.72 (p<.001), and by an increase in sensitivity from 94% to 98% (p=.022). no statistically significant difference in specificity was found between readings with and those without computer aid ( delta +-0.014; p=.46; 95% cl: -0.054, 0.026), where delta is difference in specificity. when we analyzed results from the mammographers and community radiologists as separate groups, a larger improvement was demonstrated for the community radiologists. computer-aided diagnosis can potentially help radiologists improve their diagnostic accuracy in the task of differentiating between benign and malignant masses seen on mammograms",254,0.761904762
8797,Computers and IT,"Uses artificial intelligence methods to determine whether quantitative parameters describing the perfusion image can be synthesized to make a reasonable estimate of the pulmonary arterial (PA) pressure measured at angiography. Radionuclide perfusion images were obtained in 120 patients with normal chest radiographs who also underwent angiographic PA pressure measurement within 3 days of the radionuclide study. An artificial neural network (ANN) was constructed from several image parameters describing statistical and boundary characteristics of the perfusion images. With use of a leave-one-out cross-validation technique, this method was used to predict the PA systolic pressure in cases on which the ANN had not been trained. A Pearson correlation coefficient was determined between the predicted and measured PA systolic pressures. ANN predictions correlated with measured pulmonary systolic pressures (r=0. 846, P. 001). The accuracy of the predictions was not influenced by the presence of pulmonary embolism. None of the 51 patients with predicted PA pressures of less than 29 mm Hg had pulmonary hypertension at angiography. All 13 patients with predicted PA pressures greater than 48 mm Hg had pulmonary hypertension at angiography. Meaningful information regarding PA pressure can be derived from noninvasive radionuclide perfusion scanning. The use of image analysis in concert with artificial intelligence methods helps to reveal physiologic information not readily apparent at visual image inspection","uses artificial intelligence methods to determine whether quantitativeparameters describing the perfusion image can be synthesized to make a reasonable estimate of the pulmonary arterial (pa) pressure measured at angiography. radionuclide perfusion images were obtained in 120 patients with normal chest radiographs who also underwent angiographic pa pressure measurement within 3 days of the radionuclide study. an artificial neural network (ann) was constructed from several image parameters describing statistical and boundary characteristics of the perfusion images. with use of a leave-one-out cross-validation technique, this method was used to predict the pa systolic pressure in cases on which the ann had not been trained. a pearson correlation coefficient was determined between the predicted and measured pa systolic pressures. ann predictions correlated with measured pulmonary systolic pressures (r=0.846, p<.001). the accuracy of the predictions was not influenced by the presence of pulmonary embolism. none of the 51 patients with predicted pa pressures of less than 29 mm hg had pulmonary hypertension at angiography. all 13 patients with predicted pa pressures greater than 48 mm hg had pulmonary hypertension at angiography. meaningful information regarding pa pressure can be derived from noninvasive radionuclide perfusion scanning. the use of image analysis in concert with artificial intelligence methods helps to reveal physiologic information not readily apparent at visual image inspection",213,0.68
8798,Computers and IT,"A patient active simulator is proposed which is capable of reproducing values of the parameters of pulmonary mechanics of healthy newborns and preterm pathological infants. The implemented prototype is able to: (a) let the operator choose the respiratory pattern, times of apnea, episodes of cough, sobs, etc. , (b) continuously regulate and control the parameters characterizing the pulmonary system; and, finally, (c) reproduce the attempt of breathing of a preterm infant. Taking into account both the limitation due to the chosen application field and the preliminary autocalibration phase automatically carried out by the proposed device, accuracy and reliability on the order of 1% is estimated. The previously indicated value has to be considered satisfactory in light of the field of application and the small values of the simulated parameters. Finally, the achieved metrological characteristics allow the described neonatal simulator to be adopted as a reference device to test performances of neonatal ventilators and, more specifically, to measure the time elapsed between the occurrence of a potentially dangerous condition to the patient and the activation of the corresponding alarm of the tested ventilator","a patient active simulator is proposed which is capable of reproducing values of the parameters of pulmonary mechanics of healthy newborns and preterm pathological infants. the implemented prototype is able to: (a) let the operator choose the respiratory pattern, times of apnea, episodes of cough, sobs, etc., (b) continuously regulate and control the parameters characterizing the pulmonary system; and, finally, (c) reproduce the attempt of breathing of a preterm infant. taking into account both the limitation due to the chosen application field and the preliminary autocalibration phase automatically carried out by the proposed device, accuracy and reliability on the order of 1% is estimated. the previously indicated value has to be considered satisfactory in light of the field of application and the small values of the simulated parameters. finally, the achieved metrological characteristics allow the described neonatal simulator to be adopted as a reference device to test performances of neonatal ventilators and, more specifically, to measure the time elapsed between the occurrence of a potentially dangerous condition to the patient and the activation of the corresponding alarm of the tested ventilator",181,0.285714286
8799,Computers and IT,"This article presents the design and performance evaluation of a six-degree-of-freedom piezoelectrically actuated fine motion stage that will be used for three dimensional error compensation of a long-range translation mechanism. Development of a single element, piezoelectric linear displacement actuator capable of translations of 1. 67 mu m with 900 V potential across the electrodes and under a 27. 4 N axial load and 0. 5 mm lateral distortion is presented. Finite element methods have been developed and used to evaluate resonant frequencies of the stage platform and the complete assembly with and without a platform payload. In general, an error of approximately 10. 0% between the finite element results and the experimentally measured values were observed. The complete fine motion stage provided approximately +or-0. 93 mu m of translation and +or-38. 0 mu rad of rotation in all three planes of motion using an excitation range of 1000 V. An impulse response indicating a fundamental mode resonance at 162 Hz was measured with a 0. 650 kg payload rigidly mounted to the top of the stage","this article present the design and performance evaluation of asix-degree-of-freedom piezoelectrically actuate fine motion stage that will be use for three dimensional error compensation of a long-range translation mechanism . development of a single element , piezoelectric linear displacement actuator capable of translation of 1.67 mu m with 900 v potential across the electrode and under a 27.4 n axial load and 0.5 mm lateral distortion be present . finite element method have be develop and use to evaluate resonant frequency of the stage platform and the complete assembly with and without a platform payload . in general , an error of approximately 10.0 % between the finite element result and the experimentally measure value be observe . the complete fine motion stage provide approximately + or-0 .93 mu m of translation and + or-38 .0 mu rad of rotation in all three plane of motion use an excitation range of 1000 v. an impulse response indicate a fundamental mode resonance at 162 hz be measure with a 0.650 kg payload rigidly mount to the top of the stage",168,0.684210526
8800,Computers and IT,"Interferometric gravitational wave detectors require a large degree of vibration isolation. For this purpose, a multilayer stack constructed of rubber and metal blocks is suitable, because it provides isolation in all degrees of freedom at once. In TAMA300, a 300 m interferometer in Japan, long-term dimensional stability and compatibility with an ultrahigh vacuum environment of about 10/sup -6/ Pa are also required. To keep the interferometer at its operating point despite ground strain and thermal drift of the isolation system, a thermal actuator was introduced. To prevent the high outgassing rate of the rubber from spoiling the vacuum, the rubber blocks were enclosed by gas-tight bellows. Using these techniques, we have successfully developed a three-layer stack which has a vibration isolation ratio of more than 10/sup 3/ at 300 Hz with control of drift and enough vacuum compatibility","interferometric gravitational wave detector require a large degree of vibration isolation . for this purpose , a multilay stack construct of rubber and metal block be suitable , because it provide isolation in all degree of freedom at once . in tama300 , a 300 m interferometer in japan , long-term dimensional stability and compatibility with an ultrahigh vacuum environment of about 10/sup -6 / pa be also require . to keep the interferometer at its operating point despite ground strain and thermal drift of the isolation system , a thermal actuator be introduce . to prevent the high outgassing rate of the rubber from spoil the vacuum , the rubber block be enclose by gas-tight bellow . use these technique , we have successfully develop a three-lay stack which have a vibration isolation ratio of more than 10/sup 3 / at 300 hz with control of drift and enough vacuum compatibility",138,0.777777778
8801,Computers and IT,A four channel scaler for counting applications has been designed and built using a standard high transfer rate parallel computer interface bus parallel data card. The counter section is based on standard complex programmable logic device integrated circuits. With a 200 MHz Pentium based host PC a sustained counting and data transfer with channel widths as short as 200 ns for a single channel is realized. The use of the multichannel scaler is demonstrated in dynamic light scattering experiments. The recorded traces are analyzed with wavelet and other statistical techniques to obtain transient changes in the properties of the scattered light,a four channel scaler for count application have be design and build use a standard high transfer rate parallel computer interface bus parallel data card . the counter section be base on standard complex programmable logic device integrate circuit . with a 200 mhz pentium base host pc a sustain counting and data transfer with channel width as short as 200 n for a single channel be realize . the use of the multichannel scaler be demonstrate in dynamic light scattering experiment . the recorded trace be analyze with wavelet and other statistical technique to obtain transient change in the property of the scattered light,101,0.6
8802,Computers and IT,A theoretical-information description of self-organization processes during stimulated transitions between stationary states of open nonextensive systems is presented. S/sub q/- and I/sub q/-theorems on changes of the entropy and Tsallis difference information measures in the process of evolution in the space of control parameters are proved. The entropy and the Tsallis difference information are derived and their new extreme properties are discussed,a theoretical-information description of self-organization process during stimulate transition between stationary state of open nonextensive system be present . s/sub q / - and i/sub q / - theorem on change of the entropy and tsallis difference information measure in the process of evolution in the space of control parameter be prove . the entropy and the tsalli difference information be derive and their new extreme property be discuss,62,0.777777778
8803,Computers and IT,"A novel flow control protocol is presented for availability bit rate (ABR) service in asynchronous transfer mode (ATM) networks. This scheme features periodic explicit rate feedback that enables precise allocation of link bandwidth and buffer space on a hop-by-hop basis to guarantee maximum throughput, minimum cell loss, and high resource efficiency. With the inclusion of resource management cell synchronization and consolidation algorithms, this protocol is capable of controlling point-to-multipoint ABR services within a unified framework. The authors illustrate the modeling of single ABR connection, the interaction between multiple ABR connections, and the constraints applicable to flow control decisions. A loss-free flow control mechanism is presented for high-speed ABR connections using a fluid traffic model. Supporting algorithms and ATM signaling procedures are specified, in company with linear system modeling, numerical analysis, and simulation results, which demonstrate its performance and cost benefits in high-speed backbone networking scenarios","a novel flow control protocol is presented for availability bit rate (abr)service in asynchronous transfer mode (atm) networks. this scheme features periodic explicit rate feedback that enables precise allocation of link bandwidth and buffer space on a hop-by-hop basis to guarantee maximum throughput, minimum cell loss, and high resource efficiency. with the inclusion of resource management cell synchronization and consolidation algorithms, this protocol is capable of controlling point-to-multipoint abr services within a unified framework. the authors illustrate the modeling of single abr connection, the interaction between multiple abr connections, and the constraints applicable to flow control decisions. a loss-free flow control mechanism is presented for high-speed abr connections using a fluid traffic model. supporting algorithms and atm signaling procedures are specified, in company with linear system modeling, numerical analysis, and simulation results, which demonstrate its performance and cost benefits in high-speed backbone networking scenarios",144,0.565217391
8804,Computers and IT,"This article presents a formal framework and outlines a method that autonomous agents can use to negotiate the semantics of their communication language at run-time. Such an ability is needed in open multi-agent systems so that agents can ensure they understand the implications of the utterances that are being made and so that they can tailor the meaning of the primitives to best fit their prevailing circumstances. To this end, the semantic space framework provides a systematic means of classifying the primitives along multiple relevant dimensions. This classification can then be used by the agents to structure their negotiation (or semantic fixing) process so that they converge to the mutually agreeable semantics that are necessary for coherent social interactions","this article presents a formal framework and outlines a method that autonomousagents can use to negotiate the semantics of their communication language at run-time. such an ability is needed in open multi-agent systems so that agents can ensure they understand the implications of the utterances that are being made and so that they can tailor the meaning of the primitives to best fit their prevailing circumstances. to this end, the semantic space framework provides a systematic means of classifying the primitives along multiple relevant dimensions. this classification can then be used by the agents to structure their negotiation (or semantic fixing) process so that they converge to the mutually agreeable semantics that are necessary for coherent social interactions",118,0
8805,Computers and IT,"Conversation protocols are used to achieve certain goals or to bring about certain states in the world. Therefore, one may identify the landmarks or the states that must be brought about during the goal-directed execution of a protocol. Accordingly, the landmarks, characterized by propositions that are true in the state represented by that landmark, are the most important aspect of a protocol. Families of conversation protocols can be expressed formally as partially ordered landmarks after the landmarks necessary to achieve a goal have been identified. Concrete protocols represented as joint action expressions can, then, be derived from the partially ordered landmarks and executed directly by joint intention interpreters. This approach of applying Joint Intention theory to protocols also supports flexibility in the actions used to get to landmarks, shortcutting protocol execution, automatic exception handling, and correctness criterion for protocols and protocol compositions","conversation protocol be use to achieve certain goal or to bring aboutcertain state in the world . therefore , one may identify the landmark or the state that must be bring about during the goal-directed execution of a protocol . accordingly , the landmark , characterize by proposition that be true in the state represent by that landmark , be the most important aspect of a protocol . family of conversation protocol can be express formally as partially order landmark after the landmark necessary to achieve a goal have be identify . concrete protocol represent as joint action expression can , then , be derive from the partially order landmark and execute directly by joint intention interpreter . this approach of apply joint intention theory to protocol also support flexibility in the action use to get to landmark , shortcutt protocol execution , automatic exception handling , and correctness criterion for protocol and protocol composition",141,0
8806,Computers and IT,"This paper describes application of parameter-dependent control design methods to a turbofan engine. Parameter-dependent systems are linear systems, whose state-space descriptions are known functions of time-varying parameters. The time variation of each of the parameters is not known in advance, but is assumed to be measurable in real-time. Three linear, parameter-varying (LPV) approaches to control design are discussed. The first method is based on linear fractional transformations which relies on the small gain theorem for bounds on performance and robustness. The other methods make use of either a single (SQLF) or parameter-dependent (PDQLF) quadratic Lyapunov function to bound the achievable level of performance. The latter two techniques are used to synthesize controllers for a high-performance turbofan engine. A LPV model of the turbofan engine is constructed from Jacobian linearizations at fixed power codes for control design. The control problem is formulated as a model matching problem in the H/sub infinity / and LPV framework. The objective is decoupled command response of the closed-loop system to pressure and rotor speed requests. The performance of linear, H/sub infinity / point designs are compared with the SQLF and PDQLF controllers. Nonlinear simulations indicate that the controller synthesized using the SQLF approach is slightly more conservative than the PDQLF controller. Nonlinear simulations with the SQLF and PDQLF controllers show very robust designs that achieve all desired performance objectives","this paper describes application of parameter-dependent control design methodsto a turbofan engine. parameter-dependent systems are linear systems, whose state-space descriptions are known functions of time-varying parameters. the time variation of each of the parameters is not known in advance, but is assumed to be measurable in real-time. three linear, parameter-varying (lpv) approaches to control design are discussed. the first method is based on linear fractional transformations which relies on the small gain theorem for bounds on performance and robustness. the other methods make use of either a single (sqlf) or parameter-dependent (pdqlf) quadratic lyapunov function to bound the achievable level of performance. the latter two techniques are used to synthesize controllers for a high-performance turbofan engine. a lpv model of the turbofan engine is constructed from jacobian linearizations at fixed power codes for control design. the control problem is formulated as a model matching problem in the h/sub infinity / and lpv framework. the objective is decoupled command response of the closed-loop system to pressure and rotor speed requests. the performance of linear, h/sub infinity / point designs are compared with the sqlf and pdqlf controllers. nonlinear simulations indicate that the controller synthesized using the sqlf approach is slightly more conservative than the pdqlf controller. nonlinear simulations with the sqlf and pdqlf controllers show very robust designs that achieve all desired performance objectives",223,0
8807,Computers and IT,"Conversations are sequences of messages exchanged among interacting agents. For conversations to be meaningful, agents ought to follow commonly known specifications limiting the types of messages that can be exchanged at any point in the conversation. These specifications are usually implemented using conversation policies (which are rules of inference) or conversation protocols (which are predefined conversation templates). In this article we present a semantic model for specifying conversations using conversation policies. This model is based on the principles that the negotiation and uptake of shared social commitments entail the adoption of obligations to action, which indicate the actions that agents have agreed to perform. In the same way, obligations are retracted based on the negotiation to discharge their corresponding shared social commitments. Based on these principles, conversations are specified as interaction specifications that model the ideal sequencing of agent participations negotiating the execution of actions in a joint activity. These specifications not only specify the adoption and discharge of shared commitments and obligations during an activity, but also indicate the commitments and obligations that are required (as preconditions) or that outlive a joint activity (as postconditions). We model the Contract Net Protocol as an example of the specification of conversations in a joint activity","conversations are sequences of messages exchanged among interacting agents. forconversations to be meaningful, agents ought to follow commonly known specifications limiting the types of messages that can be exchanged at any point in the conversation. these specifications are usually implemented using conversation policies (which are rules of inference) or conversation protocols (which are predefined conversation templates). in this article we present a semantic model for specifying conversations using conversation policies. this model is based on the principles that the negotiation and uptake of shared social commitments entail the adoption of obligations to action, which indicate the actions that agents have agreed to perform. in the same way, obligations are retracted based on the negotiation to discharge their corresponding shared social commitments. based on these principles, conversations are specified as interaction specifications that model the ideal sequencing of agent participations negotiating the execution of actions in a joint activity. these specifications not only specify the adoption and discharge of shared commitments and obligations during an activity, but also indicate the commitments and obligations that are required (as preconditions) or that outlive a joint activity (as postconditions). we model the contract net protocol as an example of the specification of conversations in a joint activity",203,0.444444444
8808,Computers and IT,"In this article we present the design of an ACL for a dynamic system of agents. The ACL includes a set of conversation performatives extended with operations to register, create, and terminate agents. The main design goal at the agent-level is to provide only knowledge-level primitives that are well integrated with the dynamic nature of the system. This goal has been achieved by defining an anonymous interaction protocol which enables agents to request and supply knowledge without considering symbol-level issues concerning management of agent names, routing, and agent reachability. This anonymous interaction protocol exploits a distributed facilitator schema which is hidden at the agent-level and provides mechanisms for registering capabilities of agents and delivering requests according to the competence of agents. We present a formal specification of the ACL and of the underlying architecture, exploiting an algebra of actors, and illustrate it with the help of a graphical notation. This approach provides the basis for discussing dynamic primitives in ACL and for studying properties of dynamic multi agent systems, for example concerning the behavior of agents and the correctness of their conversation policies","in this article we present the design of an acl for a dynamic system of agent . the acl include a set of conversation performative extend with operation to register , create , and terminate agent . the main design goal at the agent-level be to provide only knowledge-level primitive that be well integrate with the dynamic nature of the system . this goal have be achieve by define an anonymous interaction protocol which enable agent to request and supply knowledge without consider symbol-level issue concern management of agent name , rout , and agent reachability . this anonymous interaction protocol exploit a distribute facilitator schema which be hide at the agent-level and provide mechanism for register capability of agent and deliver request accord to the competence of agent . we present a formal specification of the acl and of the underlie architecture , exploit an algebra of actor , and illustrate it with the help of a graphical notation . this approach provide the basis for discuss dynamic primitive in acl and for study property of dynamic multi agent system , for example concern the behavior of agent and the correctness of their conversation policy",182,0.888888889
8809,Computers and IT,"Agent technology is an exciting and important new way to create complex software systems. Agents blend many of the traditional properties of AI programs - knowledge-level reasoning, flexibility, proactiveness, goal-directedness, and so forth - with insights gained from distributed software engineering, machine learning, negotiation and teamwork theory, and the social sciences. An important part of the agent approach is the principle that agents (like humans) can function more effectively in groups that are characterized by cooperation and division of labor. Agent programs are designed to autonomously collaborate with each other in order to satisfy both their internal goals and the shared external demands generated by virtue of their participation in agent societies. This type of collaboration depends on a sophisticated system of inter-agent communication. The assumption that inter-agent communication is best handled through the explicit use of an agent communication language (ACL) underlies each of the articles in this special issue. In this introductory article, we will supply a brief background and introduction to the main topics in agent communication","agent technology is an exciting and important new way to create complexsoftware systems. agents blend many of the traditional properties of ai programs - knowledge-level reasoning, flexibility, proactiveness, goal-directedness, and so forth - with insights gained from distributed software engineering, machine learning, negotiation and teamwork theory, and the social sciences. an important part of the agent approach is the principle that agents (like humans) can function more effectively in groups that are characterized by cooperation and division of labor. agent programs are designed to autonomously collaborate with each other in order to satisfy both their internal goals and the shared external demands generated by virtue of their participation in agent societies. this type of collaboration depends on a sophisticated system of inter-agent communication. the assumption that inter-agent communication is best handled through the explicit use of an agent communication language (acl) underlies each of the articles in this special issue. in this introductory article, we will supply a brief background and introduction to the main topics in agent communication",169,0.615384615
8810,Computers and IT,"Modifications on the dynamic matrix control (DMC) algorithm are presented to deal with transfer functions with varying parameters in order to control a high temperature-short time pasteurisation process. To control processes with first order with pure time delay models whose parameters present an exogenous variable dependence, a new method of free response calculation, using multiple model information, is developed. Two methods, to cope with those nonlinear models that allow a generalised Hammerstein model description, are proposed. The proposed methods have been tested, both in simulation and in real cases, in comparison with PID and DMC classic controllers, showing important improvements on reference tracking and disturbance rejection","modifications on the dynamic matrix control (dmc) algorithm are presented todeal with transfer functions with varying parameters in order to control a high temperature-short time pasteurisation process. to control processes with first order with pure time delay models whose parameters present an exogenous variable dependence, a new method of free response calculation, using multiple model information, is developed. two methods, to cope with those nonlinear models that allow a generalised hammerstein model description, are proposed. the proposed methods have been tested, both in simulation and in real cases, in comparison with pid and dmc classic controllers, showing important improvements on reference tracking and disturbance rejection",105,0.692307692
8811,Computers and IT,"A solution to a variational calculus problem is studied under the conditions of integrant convexity. The existence theorem is proved. As an example, a function is given, which satisfies all the conditions of the theorem but is not convex","a solution to a variational calculus problem be study under the condition ofintegrant convexity . the existence theorem be prove . as an example , a function be give , which satisfy all the condition of the theorem but be not convex",38,0.75
8812,Computers and IT,The interpolation accuracy of polynomial operators in a Hilbert space with a measure is estimated when nodal values of these operators are given approximately,the interpolation accuracy of polynomial operator in a hilbert space with a measure be estimate when nodal value of these operator be give approximately,24,0.5
8813,Computers and IT,An optimization approach to the simulation of ecological and economical structural-type functions is proposed. A methodology for construction of such functions is created in an explicit analytical form,an optimization approach to the simulation of ecological and economicalstructural-type function be propose . a methodology for construction of such function be create in an explicit analytical form,27,0.5
8814,Computers and IT,"Control for a semi-Markovian inventory system is considered. Under general assumptions on system functioning, conditions for existence of an optimal nonrandomized Markovian strategy are found. It is shown that under some additional assumptions on storing conditions for the inventory, the optimal strategy has a threshold (s, S)-frame","control for a semi-markovian inventory system is considered. under generalassumptions on system functioning, conditions for existence of an optimal nonrandomized markovian strategy are found. it is shown that under some additional assumptions on storing conditions for the inventory, the optimal strategy has a threshold (s, s)-frame",46,1
8815,Computers and IT,A class of Euclidean combinatorial optimization problems is selected that can be solved by the dynamic programming method. The problem of allocation of servicing enterprises is solved as an example,a class of euclidean combinatorial optimization problem be select that can be solve by the dynamic programming method . the problem of allocation of service enterprise be solve as an example,30,1
8816,Computers and IT,Optimal time of switching between several portfolios of securities are found for the purpose of profit maximization. Two methods of their determination are considered. The cases with three and n portfolios are studied in detail,optimal time of switch between several portfolio of security be foundfor the purpose of profit maximization . two method of their determination be consider . the case with three and n portfolio be study in detail,34,1
8817,Computers and IT,"This study examines the use of written sources, and personal interviews and informal conversations with individuals from New Jersey's religious, political, and educational community to identify African American women writers in New Jersey and their intellectual output. The focus on recognizing the community as an oral repository of history and then tapping these oral sources for collection development and acquisition purposes is supported by empirical and qualitative evidence. Findings indicate that written sources are so limited that information professionals must rely on oral sources to uncover local writers and their publications","this study examine the use of write source , and personal interview and informal conversation with individual from new jersey 's religious , political , and educational community to identify african american woman writer in new jersey and their intellectual output . the focus on recognize the community as an oral repository of history and then tap these oral source for collection development and acquisition purpose be support by empirical and qualitative evidence . finding indicate that write source be so limited that information professional must rely on oral source to uncover local writer and their publication",91,0.8
8818,Computers and IT,"The problem of adaptive decentralized stabilization for a class of linear time-invarying large-scale systems with nonlinear interconnectivity and uncertainties is discussed. The bounds of uncertainties are assumed to be unknown. For such uncertain dynamic systems, an adaptive decentralized controller is presented. The resulting closed-loop systems are asymptotically stable in theory. Moreover, an adaptive decentralized control scheme is given. The scheme ensures the closed-loop systems exponentially practically stable and can be used in practical engineering. Finally, simulations show that the control scheme is effective","the problem of adaptive decentralize stabilization for a class of linear time-invarying large-scale system with nonlinear interconnectivity and uncertainty be discuss . the bound of uncertainty be assume to be unknown . for such uncertain dynamic system , an adaptive decentralize controller be present . the result closed-loop system be asymptotically stable in theory . moreover , an adaptive decentralize control scheme be give . the scheme ensure the closed-loop system exponentially practically stable and can be use in practical engineering . finally , simulation show that the control scheme be effective",83,0.6
